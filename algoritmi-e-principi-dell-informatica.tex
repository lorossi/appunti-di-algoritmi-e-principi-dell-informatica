\documentclass[italian, 10pt]{article}
\usepackage{notestemplate}

% document specific commands
\input{shortfor.tex}

% math operators
\DeclareMathOperator{\succop}{succ}
\DeclareMathOperator{\Succop}{Succ}
\DeclareMathOperator{\lastop}{last}
\DeclareMathOperator{\firstop}{first}

% various text
\newcommand{\vero}{\textcolor{ForestGreen}{\texttt{vero}}\xspace} % green check
\newcommand{\falso}{\textcolor{BrickRed}{\texttt{falso}}\xspace} % red
\newcommand{\blank}{\texttt{b}} % similar to ƀ
\newcommand{\controlla}[1]{\textcolor{BrickRed}{!#1! \texttt{CONTROLLA}}\xspace} % red cross

% don't ask
\usepackage{mathrsfs}
\newcommand{\flower}{\ding{97}\xspace}

\begin{document}

\makecover{Algoritmi e Principi dell'Informatica}{AA 2021/2022}

\section*{Introduzione}

Questi appunti si riferiscono al corso di \textit{Algoritmi e principi dell'Informatica}, tenuto nell'anno accademico \textit{2021/2022} dal professor \textit{Marco Martinenghi}.
La versione più aggiornata può essere trovata sulla repo di GitHub: \href{https://github.com/lorossi/appunti-di-algoritmi-e-principi-dell-informatica}{github.com/lorossi/appunti-di-algoritmi-e-principi-dell-informatica}.
\bigskip

Il contenuto di questo documento è tratto parzialmente dalle slide mostrate in aula e dai libri indicati nel manifesto degli studi, (\textit{Informatica Teorica. Mandrioli, Spoletini} e \textit{Introduzione agli algoritmi e strutture dati. Cormen, Leiserson, Rivest, Stein}).
Spesso, le illustrazioni e le tabelle saranno uguali \textit{(a meno di palesi errori da parte mia)}.

\bigskip
Alcuni argomenti, paragrafi e sezioni potrebbero talora seguire le lezioni, talora seguire i libri perché mi sono preso la libertà di riordinare il corso a piacimento, in modo che seguisse il mio schema mentale.

\bigskip
Il lettore si senta libero di avvisarmi qualora trovasse un errore!
Vi ringrazio in anticipo.

\signature

\clearpage

\section{Linguaggi Formali}

Con \textbf{linguaggi formali} si intende un'insieme di \textbf{stringhe} costruito su un \textbf{alfabeto}, secondo uno specifico insieme di regole.

\textit{Formalmente}:
\begin{itemize}
  \item \textbf{Alfabeto}, o \textit{vocabolario}
        \begin{itemize}
          \item Insieme \textbf{finito} di simboli di base
        \end{itemize}
  \item \textbf{Stringa} su un alfabeto \(A\)
        \begin{itemize}
          \item Sequenza \textbf{finita} di simboli dell'alfabeto \(A\)
                \begin{itemize}[label=\(\rightarrow\)]
                  \item c'è un ordine tra gli elementi
                  \item non c'è un limite superiore alla lunghezza
                \end{itemize}
          \item Sono consentite le \textbf{ripetizioni}
        \end{itemize}
\end{itemize}

\subsection{Stringhe}

Una \textbf{stringa}, è caratterizzata dalla sua \textbf{lunghezza}:

\begin{itemize}
  \item Equivale al numero di simboli che contiene
  \item La lunghezza della generica stringa \(x\) si indica con \(|x|\)
\end{itemize}

La stringa \textbf{vuota} è una stringa:

\begin{itemize}
  \item Indicata come \(\epsilon\)
  \item Per \textbf{definizione}, \(|\epsilon| = 0\)
  \item Un insieme formato dalla stringa vuota non corrisponde all'insieme vuoto
        \begin{itemize}[label=\(\rightarrow\)]
          \item \textit{in simboli:} \(\{\epsilon\} \neq \emptyset\)
        \end{itemize}
\end{itemize}

La stringa vuota è definita su qualsiasi alfabeto.

\subsubsection{Confronto di stringhe}

Date due stringhe

\begin{gather*}
  x = x_1 x_2 \ldots x_n \\
  y = y_1 y_2 \ldots y_m
\end{gather*}

esse si dicono \textbf{uguali} se e solo se sono valide le seguenti due proposizioni:

\begin{enumerate}
  \item Le due stringhe hanno la stessa \textbf{lunghezza} \[ |x| = |y| \Leftrightarrow n = m \]
  \item Gli elementi in posizioni corrispondenti sono \textbf{uguali} \[ x_i = y_i \ \forall \, 1 \leq i \leq n \]
\end{enumerate}

\subsubsection{Concatenazione di stringhe}

Date due stringhe \(x\) e \(y\), la loro \textbf{concatenazione} (detta anche \textit{prodotto}) è una stringa \(xy\) (\textit{oppure} \(x \cdot y \)) dove \(x\) è seguita da \(y\).

\bigskip
\textbf{Proprietà} della concatenazione:

\begin{enumerate}
  \item Una stringa \(x\) concatenata con \(\epsilon\) è ancora \(x\) \textit{(non viene alterata)}
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(\{\epsilon\} \cdot x = x\)
        \end{itemize}
  \item La concatenazione è \textbf{associativa} e \textbf{non commutativa}
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(a \cdot (x \cdot y) = (a \cdot x) \cdot y\)
          \item \(x \cdot y \neq x \cdot y\)
        \end{itemize}
  \item Le ripetizioni di un carattere all'interno di una stringa vengono abbreviate tramite elevazione a potenza
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(xx \rightarrow x^2\)
          \item \(yyy \rightarrow y^3\)
          \item \(yyyyxx \rightarrow y^4 x^2\)
        \end{itemize}
\end{enumerate}

\subsubsection{Sottostringhe}

Una stringa \(x\) è una \textbf{sottostringa} (detta anche \textit{fattore}) di una stringa \(s\) se esistono due stringhe \(y, z\)

\begin{gather*}
  y = y_1 y_2 \ldots y_m \\
  z = z_1 z_2 \ldots z_m
\end{gather*}

tali che:

\[ s = y x z \]

\bigskip
\textbf{Proprietà} delle sottostringhe:

\begin{itemize}
  \item sia \(y\) che \(z\) possono essere \(\epsilon\)
        \begin{itemize}
          \item se \(y = \epsilon \Rightarrow x\) è detta \textbf{prefisso} e \(s\) inizia con i caratteri di \(x\)
          \item se \(z = \epsilon \Rightarrow x\) è detta \textbf{suffisso} e \(s\) finisce con i caratteri di \(x\)
        \end{itemize}
  \item Se \(y = \epsilon, z = \epsilon\) allora \(x\) è \textit{uguale} a \(s\)
\end{itemize}

\subsection{Stella di Kleene}
\label{sec:stella-di-Kleene}

La \textbf{stella di Kleene} è un operatore unario che si applica a un insieme di simboli o a un insieme di stringhe.
Si indica con il simbolo \(\ast\) e si pronuncia \inlinequote{A star}.

Se \(A\) è un alfabeto, allora \(A^\ast\) è l'insieme di tutte le stringhe su simboli di \(A\), inclusa la stringa vuota \(\epsilon\) \textit{a patto che essa faccia parte dell'alfabeto}.
Non è imposto un limite superiore alla lunghezza delle stringhe prodotte.

\subsubsection{Definizione formale della Stella di Kleene}

È possibile definire la stella di Kleene tramite una trattazione più algebrica:

\begin{itemize}
  \item Un \textbf{semigruppo} è una coppia \(\langle S, \circ \rangle\) in cui:
        \begin{itemize}
          \item \(S\) è un \textbf{insieme} ed è \textbf{chiuso} rispetto a \(\circ\)
          \item \(\circ\) è un'operazione \textbf{associativa} su \(S\)
                \begin{itemize}[label=\(\rightarrow\)]
                  \item le operazioni possono essere associate a piacere
                  \item è \textit{distributiva} e \textit{commutativa}
                \end{itemize}
        \end{itemize}
  \item Un \textbf{monoide} è un semigruppo tale per cui:
        \begin{itemize}
          \item \( \exists \, u \, \forall \, x \, | \, x \circ u = u \circ x = x \)
          \item \(u\) è \textit{l'elemento neutro} rispetto all'operazione \(\circ\)
        \end{itemize}
  \item Un \textbf{gruppo} è un monoide che tale per cui:
        \begin{itemize}
          \item \( \forall \, x \, \exists \, x^{-1} \, | \, x \circ x^{-1} = x^{-1} \circ x = u\)
          \item \(x^{-1}\) è \textit{l'elemento inverso} di \(x\) rispetto all'operazione \(\circ\)
        \end{itemize}
\end{itemize}

Tra i 3 insiemi è valida la relazione:

\[ semigruppo \subseteq monoide \subseteq gruppo \]

\bigskip
A valle di queste definizioni, è possibile inoltre affermare che:

\begin{itemize}
  \item Dato un semigruppo \(\langle S, \circ \rangle\) e un sottoinsieme \(X\) di \(S\):
        \begin{itemize}
          \item \(X^+\) \textit{(detto \textquote{più di Kleene})} denota il sottoinsieme di \(S\) generato da \(X\), cioè tutte le sequenza della forma
                \[ x_1 \circ \ldots \circ x_n \quad x_i \in X, n \geq 1 \]
          \item Per un monoide \(\langle S, \circ \rangle\) con unità \(u\):
                \begin{itemize}
                  \item \(X^\ast = X^+ \cup \{u\}\)
                  \item \(X^\ast\) è detto il \textbf{monoide libero} generato da \(X\)
                \end{itemize}

        \end{itemize}
\end{itemize}

\subsection{Linguaggi}

È detto \textbf{linguaggio} un qualsiasi insieme di stringhe definite su un alfabeto.
Sono linguaggi:

\begin{itemize}
  \item Italiano, Inglese, Francese, \ldots
  \item \texttt{C}, \texttt{Java}, \texttt{Pascal}, \ldots
  \item Linguaggi grafici, Musica, Multimedia, \ldots
\end{itemize}

\textit{Inoltre:}

\begin{itemize}
  \item Una lingua come l'Italiano è \textit{infinita}, perché è possibile scrivere frasi di lunghezza infinita
  \item Analogamente, un linguaggio come il \texttt{C} è un insieme \textit{potenzialmente infinito} poiché l'insieme di programmi corretti è infinito
\end{itemize}

\textit{Formalmente}, un linguaggio \(L\) definito su un alfabeto \(A\) è un \textit{sottoinsieme} di \(A^\ast\).

I linguaggi formali, contrariamente a quanto possa sembrare, \textit{non sono solo rappresentazioni matematiche astratte}.
Essi infatti sono metodi utili a \textit{rappresentare} o \textit{comunicare} una informazione, quindi non solo stringhe senza significato.
Usando le operazioni descritte nella Sezione~\ref{sec:operazioni-linguaggi}, in base ai vari contesti, è possibile interpretare un linguaggio in modi consoni.
Anche i \textit{calcoli} possono essere rappresentati tramite linguaggi formali.

Esistono molti tipi di linguaggi, tra i quali si riconoscono soprattutto:

\begin{itemize}
  \item Linguaggi \textit{naturali}
  \item Linguaggi \textit{di programmazione}
  \item Linguaggi \textit{logici}
\end{itemize}

Il significato di queste definizioni verrà affrontato in seguito.

\subsubsection{Operazioni sui linguaggi}
\label{sec:operazioni-linguaggi}

Poiché un linguaggio non è altro che un insieme di stringhe, su di loro si applicano le operazioni insiemistiche.
Esse sono:

\begin{enumerate}
  \item \textbf{Unione} - \(\cup\)
  \item \textbf{Intersezione} - \(\cap\)
  \item \textbf{Differenza} - \(\backslash\) \textit{oppure} \(-\)
  \item \textbf{Complemento} - \(L^c\)
  \item \textbf{Concatenazione} - \(\cdot\)
  \item \textbf{Potenza n-esima} - \(L^n\)
  \item \textbf{Chiusura di Kleene} - \(L^\ast\)
\end{enumerate}

Le operazioni sui linguaggi creano nuove classi di linguaggi.
Esempi e le loro rispettive proprietà sono descritte nelle Sezioni seguenti (\ref{par:unione-di-linguaggi}~-~\ref{par:potenze-di-linguaggi}).

\paragraph{Unione}
\label{par:unione-di-linguaggi}

Siano \(L_1, L_2\) due linguaggi:

\[ L_1 \cup L_2 = \{w \, | \, w \in L_1 \lor w \in L_2 \} \]

\bigskip
\textit{Esempio}: siano \(L_1, L_2\) due linguaggi:

\[ L_1 = \{ \epsilon, a, b, c, bc, ca\} \qquad L_2 = \{ ba, bb, bc, ca, cb, cc\} \]

La loro unione sarà:

\[ L_1 \cup L_2 = \{\epsilon, a, b,c , ba, bb, bc, ca, cb, cc\} \]

\paragraph{Intersezione}

Siano \(L_1, L_2\) due linguaggi:

\[ L_1 \cup L_2 = \{w \, | \, w \in L_1 \land w \in L_2 \} \]

\bigskip
\textit{Esempio}: siano \(L_1, L_2\) due linguaggi:

\[ L_1 = \{ \epsilon, a, b, c, bc, ca\} \qquad L_2 = \{ ba, bb, bc, ca, cb, cc\} \]

La loro intersezione sarà:

\[ L_1 \cap L_2 = \{bc, ca\} \]

\paragraph{Differenza}

Siano \(L_1, L_2\) due linguaggi:

\[ L_1 \, \backslash \, L_2 = L_1 - L_2 = \left\{ w \, | \, w \in L_1, w \notin L_2 \right\} \]

È un'operazione che viene generalmente usata quando \(L_2 \subseteq L_1\).

\bigskip
\textit{Esempio:} siano \(L_1, L_2\) due linguaggi:

\[L_1 = \{ ba, bb, bc, ca, cb, cc\} \qquad L_2 = \{ bc, ca \} \]

La loro differenza sarà:

\[ L_1 \backslash L_2 = \{ba, bb, cb, cc\} \]

\paragraph{Complemento}

Sia \(L_1\) un linguaggio:

\[ \neg L_1 = \left\{ w \, | \, w \notin L_1 \right\} \]

Sia \(L\) un linguaggio definito su un alfabeto \(A\).
Allora la sua operazione di complementazione sarà definita come:

\[ L^c = A^\ast \backslash L \]

\paragraph{Concatenazione}

Siano \(L_1, L_2\) due linguaggi:

\[ L_1 \cdot L_2 = \{ w z \, | \, w \in L_1, z \in L_2 \} \]

\begin{itemize}
  \item L'operazione non è commutativa: \( L_1 \cdot L_2 \neq L_2 \cdot L_1 \)
  \item Si fa scorrere ogni elemento di \(L_1\) associandolo a ogni elemento di \(L_2\)
  \item La concatenazione di un linguaggio con un linguaggio vuoto dà origine al linguaggio stesso, mentre la concatenazione di un linguaggio con un insieme vuoto dà un insieme vuoto:
        \[ L \cdot \{\epsilon\} = L \quad L \cdot \emptyset = \emptyset \]
  \item Il numero delle stringhe in \(L_1 \cdot L_2\) sarà pari al prodotto del numero di stringhe in ciascun linguaggio:
        \[ | L_1 \cdot L_2 | = |L_1| \cdot |L_2| \]

\end{itemize}

\bigskip
\textit{Esempio:} siano \(L_1, L_2\) due linguaggi:

\[ L_1 = \{ \epsilon, a, b, c, bc, ca\} \qquad L_2 = \{ ba, bb, bc, ca, cb, cc \} \]

La loro concatenazione sarà:

\begin{align*}
  L_1 \cdot L_2 = \{ \  & ba, bb, bc, ca, cb, cc, aba, abb, abc, aca, acb, acc, bba, bbb, bbc, bca,    \\
                        & bcb, bcc, cba, cbb, cbc, cca, ccb, ccc, bcba, bcbb, bcbc, bcca, bccb, bccc , \\
                        & caba, cabb, cabc, caca, cacb, cacc \ \}
\end{align*}

\paragraph{Potenze}
\label{par:potenze-di-linguaggi}

Sia \(L\) un linguaggio. La sua potenza \(L^n\) sarà ottenuta \textit{concatenando} \(L\) con se stesso per \(n\) volte.

\[ \displaystyle L^n = \underbrace{L \cdot L \cdot \ldots \cdot L}_{n \text{ volte}} \]

\bigskip
\textit{Definizione induttiva:}
\begin{enumerate}
  \item \(L^0 = \{\epsilon\}\) \label{enum:potenze-caso-base}
  \item \(L^i = L^{i-1} \cdot L\) \label{enum:potenze-passo-induttivo}
\end{enumerate}

Si noti che il punto~\ref{enum:potenze-caso-base} prende il nome di \textbf{caso base}, mentre il punto~\ref{enum:potenze-passo-induttivo} si chiama \textbf{passo induttivo}.

Il \textit{passo induttivo} implica che almeno una parte del problema sia stato risolto (tramite \textbf{ipotesi induttiva}).
In questo caso, l'\textit{ipotesi induttiva} è data dalla relazione che riguarda \(L^{i-1}\).

\bigskip
\textit{Esempi:}
\begin{itemize}
  \item \(L^2 = L \cdot L\)
  \item \(L^3 = L \cdot L \cdot L\)
  \item \(L^4 = L \cdot L \cdot L \cdot L\)
\end{itemize}

\paragraph{Chiusura di Kleene}
\label{sec:proprieta-Kleene}

Sia \(L\) un linguaggio.
Allora l'operatore \textit{stella di Kleene} su \(L\) è definito come:

\[ L^\ast = \bigcup_{n=0}^{\infty} L^n \]

\bigskip
Analogamente, è possibile definire l'operatore \textit{più di Kleene}:

\[ L^+ = \bigcup_{n=1}^{\infty} L^n \]

\bigskip
\textit{Proprietà:}
\begin{enumerate}
  \item \(L^\ast = L^+ \cup L^0 = L^+ \cup \{\epsilon\}\)
  \item \(L^+ = L \cdot L^\ast\)
  \item \(L^+ = L^\ast \Leftrightarrow \epsilon \in L\)
\end{enumerate}

Poiché la concatenazione \textbf{non è commutativa}, la chiusura di Kleene \textbf{non è riflessiva}.

\clearpage

\section{Automi a stati finiti - \FSA}
\label{sec:automi-a-stati-finiti}

Un \textbf{automa a stati finiti} (o \FSA, dall'Inglese \textit{Finite State Automaton}) è il più semplice modello di astrazione.

Esso rappresenta un sistema che ammette un \textit{insieme finito} di stati (e di conseguenza un numero limitato di configurazioni).
In seguito ad un determinato ingresso (a sua volta formato da un insieme finito di valori), potrà avvenire una transizione tra due stati distinti.

Gli \FSA rappresentano il più semplice modello di computazione: molti dispositivi possono essere modellati come tali, seppure con alcune limitazioni.

Per poter usare gli \FSA per riconoscere linguaggi, è importante identificare:

\begin{enumerate}
  \item Le condizioni \textit{iniziali} del sistema
  \item Gli stati \textit{finali} del sistema
\end{enumerate}

Affinché sia possibile costruire un modello adatto, è necessario riconoscere gli elementi al suo interno:

\begin{itemize}
  \item Gli \textbf{stati}, tra i quali si identificano:
        \begin{itemize}
          \item Stato \textbf{iniziale}
          \item Stati \textbf{finali}
        \end{itemize}
  \item Le \textbf{transizioni}
  \item L'\textbf{ingresso}
\end{itemize}

\subsection{Stati, Transizioni e Ingressi}

Gli stati sono rappresentati come dei cerchi con all'interno la loro etichetta di riferimento.
La loro rappresentazione è illustrata in Figura~\ref{fig:stati-iniziale-finale-FSA}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, node distance=2cm, >=Triangle]
    \node [state, initial left, initial text=, minimum size=1.5cm](initial) {\footnotesize \texttt{iniziale}};
    \node [state, minimum size=1.5cm](generic)  [right=of initial] {\footnotesize \texttt{stato}};
    \node [state, minimum size=1.5cm, accepting] [right=of generic] {\footnotesize \texttt{finale}};
  \end{tikzpicture}
  \caption{Stati iniziali e finali di un \FSA}
  \label{fig:stati-iniziale-finale-FSA}
  \bigskip
\end{figure}

\bigskip
Un \FSA è definito su un alfabeto.
I simboli dell'alfabeto rappresentano l'ingresso del sistema.

\bigskip
Quando il sistema riceve un ingresso, cambia il proprio stato interno.
Il passaggio tra stati diversi avviene tramite \textbf{transizioni}.
Una \textbf{transizione} può avere un'etichetta che denomina l'azione che viene intrapresa nel momento della sua attivazione.

Una transizione è rappresentata mediante frecce, come in Figura~\ref{fig:transizioni-FSA}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, on grid, node distance=4cm, >=Triangle]
    \node [state, minimum size=1.5cm](on) {\footnotesize \texttt{ON}};
    \node [state, minimum size=1.5cm](off) [right=of on] {\footnotesize \texttt{OFF}};

    \path[->, thick]
    (on) edge [loop left] node {} ()
    (on) edge [] node {} (off);

  \end{tikzpicture}

  \caption{Transizioni in un \FSA}
  \label{fig:transizioni-FSA}
  \bigskip
\end{figure}

\subsection{Definizione formale \FSA}
\label{sec:definizione-formale-FSA}

Formalmente, un \FSA è una tupla di \(5\) elementi \(\langle Q, A, \delta, q_0, F \rangle\) dove:

\begin{itemize}
  \item \(Q\) è un insieme di \textbf{stati}, finito
  \item \(A\) è l'\textbf{alfabeto} di \textbf{ingresso}
  \item \(\delta\) è la \textbf{funzione} di \textbf{transizione}:
        \begin{itemize}
          \item \(\delta: Q \times A \rightarrow Q\)
          \item la funzione di transizione è detta \textbf{parziale} se non tutte le transizione da tutti i possibili stati per tutti i possibili elementi dell'alfabeto sono definite
          \item un \FSA con una funzione di transizione totale è detto \textbf{completo}
        \end{itemize}
  \item \(q_0 \in Q\) è lo \textbf{stato iniziale}
  \item \(F \subseteq Q\) è l'insieme di \textbf{stati finali}
\end{itemize}

Si noti che nonostante sia definire \textbf{un solo} stato iniziale, un \FSA ammette \textbf{più} stati finali.
Inoltre, uno stato può essere contemporaneamente iniziale e finale.

\subsubsection{\FSA con transizione totale}

Come già enunciato, una funzione di transizione completa implica che essa sia definita per tutti i possibili stati per ogni possibile elemento dell'alfabeto di ingresso.
Il \FSA in Figura~\ref{fig:esempio-funzione-transizione-totale-FSA} ha una funzione di transizione \textit{totale}, mentre quello in Figura~\ref{fig:esempio-funzione-transizione-parziale-FSA} presenta una funzione di transizione \textit{parziale}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{subfigure}[t]{0.62\textwidth}
    \centering
    \scalebox{0.9}{
      \begin{tikzpicture}[auto, on grid, node distance=4cm, >=Triangle]
        \node [state](on) {\texttt{ON}};
        \node [state](off) [right=of on] {\texttt{OFF}};

        \path[->, thick]
        (on) edge [loop left] node {\texttt{switch on}} ()
        (off) edge [loop right] node {\texttt{switch off}} ()
        (on) edge [bend left] node {\texttt{switch off}} (off)
        (off) edge [bend left] node {\texttt{switch on}} (on);
      \end{tikzpicture}
    }
    \caption{\FSA con funzione di transizione totale}
    \label{fig:esempio-funzione-transizione-totale-FSA}
  \end{subfigure}
  \begin{subfigure}[t]{0.37\textwidth}
    \centering
    \scalebox{0.9}{
      \begin{tikzpicture}[auto, on grid, node distance=4cm, >=Triangle]
        \node [state](on) {\texttt{ON}};
        \node [state](off) [right=of on] {\texttt{OFF}};

        \path[->, thick]
        (on) edge [bend left] node {\texttt{switch off}} (off)
        (off) edge [bend left] node {\texttt{switch on}} (on);
      \end{tikzpicture}
    }
    \caption{\FSA con funzione di transizione parziale}
    \label{fig:esempio-funzione-transizione-parziale-FSA}
  \end{subfigure}
  \bigskip
\end{figure}

Il \FSA in Figura~\ref{fig:esempio-funzione-transizione-parziale-FSA}, al contrario di quello in Figura~\ref{fig:esempio-funzione-transizione-totale-FSA}, non presenta le transizioni in corrispondenza dell'azione di \texttt{switch off} sullo stato \texttt{off} e \texttt{switch on} sullo stato \texttt{on}.

\subsubsection{Sequenza di mosse}
\label{sec:sequenza-di-mosse}

Una \textbf{sequenza di mosse} inizia da uno stato iniziale ed è di \textbf{accettazione} se raggiunge uno degli stati \textit{finali}.

\bigskip
\textit{Formalmente:}
\begin{itemize}
  \item \textbf{Sequenza di mosse}:
        \begin{itemize}
          \item \(\delta^\ast : Q \times A^\ast \rightarrow Q\)
          \item Il \textbf{dominio} è definito come il prodotto cartesiano tra stati e stella di Kleene di sequenze di simboli
          \item Il \textbf{codominio} coincide con l'insieme degli stati
        \end{itemize}
  \item \(\delta^\ast\) è definita induttivamente a partire da \(\delta\):
        \begin{enumerate}
          \item \(\delta^\ast(q, \epsilon) = q\)
          \item \(\delta^\ast(q, yi) = \delta\left(\delta^\ast(q, y), i\right)\)
        \end{enumerate}
  \item Stato \textbf{iniziale} \(q_0 \in Q\)
  \item Stati \textbf{finali} \( F \subseteq Q \)
\end{itemize}

\subsubsection{Condizione di accettazione di un \FSA}

Il linguaggio relativo al \FSA è costituito dalle stringhe \(x\) che appartengono a \(\delta^\ast\).
\textit{Formalmente}:

\[ \forall \, x \in L \Leftrightarrow \delta^\ast (q_0, x) \in F \]

I linguaggi riconosciuti dagli \FSA, come verrà analizzato più in dettaglio in seguito \textit{(per esempio nella Sezione~\ref{sec:gerarchia-di-Chomsky})}, prendono il nome di \textbf{regolari}.

\subsection{Trasduttori a stati finiti - \FST}
\label{sec:trasduttori-a-stat-finiti}

\textit{Idea}: usare gli automi come traduttori di linguaggi.
Nasce il \textbf{FST} \textit{(dall'Inglese finite state transducer)}, cioè un \FSA che lavora su due nastri.
È una specie di macchina traduttrice.

La rappresentazione più semplificata di un \textit{FST} è rappresentata in Figura~\ref{fig:diagramma-FST}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-1.tikz}
  \caption{Diagramma semplificato di un FST}
  \label{fig:diagramma-FST}
  \bigskip
\end{figure}

Un \FST è composto da:

\begin{enumerate}
  \item Una stringa di \textbf{ingresso} \(x\)
  \item Una stringa di \textbf{uscita} \(y\)
  \item Una funzione \(\tau: L_1 \rightarrow L_2 \) tale che \(y = \tau(x)\)
\end{enumerate}

\subsubsection{Definizione formale \FST}

Un trasduttore a stati finiti (\FST) è una tupla di \(7\) elementi \(\langle Q, A, \delta, q_0, F, O, \eta \rangle\):

\begin{itemize}
  \item \(Q\) è un insieme finito di \textbf{stati}
  \item \(A\) è l'\textbf{alfabeto} di \textbf{ingresso}
  \item \(\delta\) è la \textbf{funzione} di \textbf{transizione}:
        \begin{itemize}
          \item \(\delta: Q \times A \rightarrow Q\)
        \end{itemize}
  \item \(q_0 \in Q\) è lo \textbf{stato iniziale}
  \item \(F \subseteq Q\) è l'insieme di \textbf{stati finali}
  \item \(O\) è l'\textbf{alfabeto} di \textbf{uscita}
  \item \(\eta\) è la \textbf{funzione} di \textbf{uscita}:
        \begin{itemize}
          \item \(\eta: Q \times I \rightarrow O^\ast\)
          \item il \textbf{dominio} è il prodotto cartesiano degli stati per l'alfabeto di ingresso
          \item il \textbf{codominio} è la stella di Kleene dell'alfabeto di uscita
        \end{itemize}
\end{itemize}

La definizione di \(Q, A, \delta, q_0, F\) è analoga a quella avvenuta nella Sezione~\ref{sec:definizione-formale-FSA} riguardante gli \FSA.

La condizione di accettazione resta la stessa degli accettori.
Di conseguenza, \textbf{la traduzione avviene solo su stringhe accettate}.

\subsubsection{Traduzione di una stringa}

Analogamente a quanto già illustrato nella definizione della sequenza di mosse \(\delta\) (Sezione~\ref{sec:sequenza-di-mosse}), \(\eta^\ast\) verrà definito induttivamente.
Infatti, ricordando che \(\eta^\ast : Q \times I^\ast \rightarrow O^\ast \):

\begin{enumerate}
  \item \(\eta^\ast(q, \epsilon) = \epsilon\)
  \item \(\eta^\ast(q, y \cdot i) = \eta^\ast(q, y) \cdot \eta\left(\delta^\ast(q, y), i\right)\)
\end{enumerate}

sarà quindi valida la relazione:

\[\forall \, x, \tau(x) = \eta^\ast (q_0, x) \Leftrightarrow \delta^\ast(q_0, x) \in F\]

\subsubsection{Pumping lemma}
\label{sec:pumping-lemma}

Si consideri il \FSA in Figura~\ref{fig:pumping-lemma-FSA}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, on grid, node distance=2cm, >=Triangle]
    \node [state, initial, initial text = ](q0) {\(q_0\)};
    \node [state](q1) [right=of q0] {\(q_1\)};
    \node [state](q2) [right=of q1] {\(q_2\)};
    \node [state, accepting](q9) [right=of q2] {\(q_9\)};
    \node [state](q5) [below=of q0] {\(q_5\)};
    \node [state](q4) [right=of q5] {\(q_4\)};
    \node [state](q3) [right=of q4] {\(q_3\)};
    \node [state, accepting](q8) [right=of q3] {\(q_8\)};
    \node [state](q6) [below=of q4] {\(q_6\)};
    \node [state](q7) [right=of q6] {\(q_7\)};

    \path[->]
    (q0) edge [] node {b} (q1)
    (q2) edge [] node {b} (q9)
    (q1) edge [] node {b} (q7)
    (q9) edge [] node {b} (q7)
    (q9) edge [] node {a} (q8)
    (q4) edge [] node {a} (q5);

    \path[->, thick, draw=red, fill=red]
    (q1) edge [] node {a} (q2)
    (q2) edge [] node {a} (q3)
    (q3) edge [] node {b} (q6)
    (q6) edge [] node {a} (q4)
    (q4) edge [] node {b} (q1);

  \end{tikzpicture}
  \caption{Pumping lemma}
  \label{fig:pumping-lemma-FSA}
  \bigskip
\end{figure}

Se si ammette la possibilità che il ciclo \textcolor{red}{\(q_1 \rightarrow q_2 \rightarrow q_3 \rightarrow q_6 \rightarrow q_4 \rightarrow q_1 \rightarrow \ldots\)} venga attraversato una volta, allora è possibile che esso venga attraversato un numero indefinito \textit{(potenzialmente infinito)} di volte.

Più formalmente:

\begin{itemize}
  \item Se \(x \in L, |x| \geq |Q| \), allora esistono uno stato \(q \in Q\) a una stringa \(w \in I^+\) tali che:
        \begin{itemize}
          \item \(x = ywz\)
          \item \(\delta^\ast (q, w) = q \)
        \end{itemize}
  \item Perciò varrà anche \(\forall \, n \geq 0 \ y w^n z \in L\)
\end{itemize}

Questo fenomeno prende il nome di \textbf{pumping lemma}, e porta a due possibili \textit{conseguenze}:

\begin{enumerate}
  \item \(L = \emptyset\)
        \begin{itemize}
          \item \(\exists \, x \in L \Leftrightarrow \exists \, y \in L, \ |y| < |Q|\)
          \item È sufficiente rimuovere tutti i \textit{cicli} dall'\FSA che accetta \(x\)
        \end{itemize}
  \item \(L = \infty\)
        \begin{itemize}
          \item Il \FSA deve verificare in modo analogo se \(\exists \, x \in L, \ |Q| \leq |x| < 2 \cdot |Q| \)
          \item \textit{La lunghezza della stringa \(x\) è tale da compiere meno di due cicli}
        \end{itemize}
\end{enumerate}

\bigskip
Nella pratica, ciò porta alle seguenti \textit{implicazioni}:

\begin{itemize}
  \item Un linguaggio di programmazione che ammette \(0\) programmi corretti \textbf{non è rilevante}
  \item Analogamente, un linguaggio di programmazione che ammette \textit{un numero finito} di programmi corretti \textbf{non è rilevante}
  \item[\(\Rightarrow\)] Il \textit{pumping lemma}, \textbf{quindi, non unicamente è un aspetto negativo} poiché permette di creare linguaggi \textbf{infiniti}
\end{itemize}

\paragraph{Conseguenza negativa del \textit{pumping lemma}}
\label{par:conseguenza-negativa-pumping-lemma}

Si consideri il linguaggio
\[ L = \left\{ a^n b^n \, | \, n > 0 \right\} \]
e si supponga che sia riconosciuto da un qualsiasi \FSA.

\bigskip
Si consideri ora la stringa
\[ x = a^m b^m, \  m > |Q| \]
e si applichi il \textit{pumping lemma}.
Come conseguenza dello stesso, poiché la lunghezza della stringa è superiore al numero di stati, dovrebbe esistere una costante \(k\) per la quale, se \(x \in L, |x| \geq k\), la stessa \(x\) può essere scritta come \(ywz\) con \(1 \leq |w| \leq k, yw^iz \in L \, \forall \, i \geq 0\).

La dimostrazione avviene \textit{per assurdo}.
Scomponendo \(x\), si possono identificare \(3\) possibili forme di essa che verranno accettate:

\begin{itemize}
  \item \(x = ywz, w = a^r, r > 0\), quindi anche \(a^{s + k \cdot r} b^n \, \forall \, k \geq 0\) \textbf{dovrebbe essere accettato}
  \item \(x = ywz, w = b^r, r > 0\), quindi anche \(a^n b^{s + k \cdot r} \, \forall \, k \geq 0\) \textbf{dovrebbe essere accettato}
  \item \(x = ywz, w = a^r b^s, r > 0, s > 0\), quindi anche \(a^{n - r} (a^r b^s)^k b^{n-s} \, \forall \, k \geq 0\) \textbf{dovrebbe essere accettato}
\end{itemize}

Tutte e tre queste forma, tuttavia, violano la forma di \(x\) indicata nell'ipotesi.
Quindi si può affermare che esistono linguaggi non riconoscibili tramite \FSA.

Più precisamente, esistono dei \textit{linguaggi infiniti} che non possono essere riconosciuti dagli \FSA.
Questa affermazione può essere giustificata informalmente affermando che \inlinequote{gli \FSA non possono contare}.

La famiglia dei linguaggi che sono riconosciuti dagli \FSA prende il nome di \textbf{regolare}.

\paragraph{Provare che un linguaggio sia infinito}

Per poter provare che:

\begin{itemize}
  \item Il linguaggio riconosciuto dal \FSA sia infinito, bisogna provare le infinite stringhe che appartengono al linguaggio su cui esso è definito
  \item Il linguaggio riconosciuto dal \FSA sia vuoto, bisogna provare le infinite stringhe che appartengono al linguaggio su cui esso è definito
\end{itemize}

Non è possibile effettuare un test con un numero infinito di stringhe in un tempo finito.
Quindi, per effettuare una ricerca esaustiva e trovare una risposta a questa domanda, si provano le stringhe strettamente inferiori al numero di stati:

\begin{itemize}[label=\(\Rightarrow\)]
  \item Se una stringa \textbf{viene accettata}, allora \textbf{tutte} le stringhe verranno accettate
  \item Se una stringa \textbf{non viene accettata}, allora \textbf{nessuna} stringa verrà accettata
\end{itemize}

Il numero di queste stringhe, e quindi il numero di test da effettuare, è limitato.

\paragraph{Provare la presenza di un ciclo}

Se un automa accetta un certo linguaggio, allora una stringa di lunghezza superiore al numero di stati verrà accettata se e solo se è presente un ciclo all'interno del corrispondente \FSA.

\subsection{Operazioni sugli \FSA}

Prima di poter definire le operazioni sugli \FSA, è necessario definire il concetto di \textbf{chiusura}:

Chiusura in \textit{matematica}: un insieme \(S\) è \textbf{chiuso} rispetto a una operazione \(OP\) se, quando \(OP\) è applicata agli elementi di \(S\), il risultato è ancora un elemento di \(S\)

Leggermente diverso è il caso dei \textit{linguaggi}, che verrà esaminato più nel seguente paragrafo.

\subsubsection{Chiusura per i linguaggi}

Siano:

\begin{itemize}
  \item \(\mathcal{L} = \{L_i\}\) una \textit{famiglia} di \textbf{linguaggi}
        \begin{itemize}
          \item \(\mathcal{L}\) è chiuso rispetto all'operazione \(OP\) se e solo se, \(\forall \, L_1, L_2 \in \mathcal{L}, \ L_1 \ OP \ L_2 \in \mathcal{L} \)
        \end{itemize}
  \item \(\mathcal{R}\) la \textit{famiglia} dei \textbf{linguaggi regolari}
        \begin{itemize}
          \item \(\mathcal{R}\) è chiuso rispetto alle operazioni insiemistiche, alla concatenazione e all'operatore \(\ast\) \textit{(stella di Kleene)}
        \end{itemize}
\end{itemize}

\paragraph{Intersezione}
\label{par:intersezione-FSA}
% pagina 108 libro, slide 32 1-02.FSA

Siano \(A_1\) e \(A_2\) due generici \FSA, caratterizzati dalle loro espressioni

\begin{align*}
  A_1 & = \langle Q_1, I_1, \delta_1, q_{01}, F_1 \rangle \\
  A_2 & = \langle Q_2, I_2, \delta_2, q_{02}, F_2 \rangle
\end{align*}

Si supponga che \( I_2  = I_1 = I \) e che \(Q_1 \cap Q_2 = \emptyset \).
Entrambe le supposizioni non comportano alcuna perdita di generalità perché:

\begin{enumerate}
  \item Se \(I_1\) ed \(I_2\), fossero diversi, posto che la funzione di transizione non deve essere necessariamente uguale, i due atomi possono essere considerati definiti su \(I_1 \cup I_2\)
  \item Se \(Q_1\) e \(Q_2\) non fossero disgiunti, i nomi degli stati comuni possono essere banalmente cambiati
\end{enumerate}

\FSA che riconosce entrambi i linguaggi (e quindi \(I_1 \cap I_2\)) è costruito come:

\begin{gather*}
  C = \langle A_1, A_2 \rangle = \langle Q_1 \times Q_2, I, \delta, \langle q_{01}, q_{02} \rangle, F_1 \times F_2 \rangle \\
  \delta(\langle q_1, q_2 \rangle, i)  = \langle \delta_1 (q_1, i), \delta_2(q_2, i) \rangle
\end{gather*}

\bigskip
La dimostrazione che
\[ L \left( \langle A_1, A_2 \rangle \right) = L\left(A_1\right) \cap L\left(A_2\right) \]
può avvenire un ragionamento per induzione alla funzione \(\delta\).

\bigskip
Intuitivamente, l'\textit{esecuzione parallela} di due \FSA può essere simulata \textit{accoppiandoli} tramite prodotto cartesiano.
L'intersezione di due \FSA può essere vuota, non riconoscendo alcun linguaggio (ricordando che \(\emptyset \neq \{\epsilon\}\)).

\bigskip
\textit{Esempio:} siano \(A\) e \(B\) due \FSA rappresentati come segue:

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, on grid, node distance=2cm, >=Triangle]
    \node [state, initial, initial text = A](q0) {\(q_0\)};
    \node [state](q1) [right=of q0] {\(q_1\)};
    \node [state](q2) [right=of q1] {\(q_2\)};
    \node [state, accepting](q3) [right=of q2] {\(q_3\)};

    \node [state, initial, initial text = B](p0) [below=of q0] {\(p_0\)};
    \node [state](p1) [right=of p0] {\(p_1\)};
    \node [state](p2) [right=of p1] {\(p_2\)};
    \node [state, accepting](p3) [right=of p2] {\(p_3\)};

    \path[->, thick]
    (q0) edge [] node {} (q1)
    (q1) edge [] node {} (q2)
    (q2) edge [] node {} (q3);

    \path[->, thick]
    (p0) edge [] node {} (p1)
    (p1) edge [] node {} (p2)
    (p2) edge [] node {} (p3);

  \end{tikzpicture}
  \caption{Intersezione tra FSA}
  \label{fig:intersezione-FSA-1}
  \bigskip
\end{figure}

La loro \textit{intersezione} \(C = A \cap B\) può essere rappresentata come segue:

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, on grid, node distance=2cm, >=Triangle]
    \node [state, initial, initial text = \(C\)](q0) {\(q_0 p_0\)};
    \node [state](q1) [right=of q0] {\(q_1 p_1\)};
    \node [state](q2) [right=of q1] {\(q_2 p_2\)};
    \node [state, accepting](q3) [right=of q2] {\(q_3 p_3\)};

    \path[->, thick]
    (q0) edge [] node {} (q1)
    (q1) edge [] node {} (q2)
    (q2) edge [] node {} (q3);

  \end{tikzpicture}
  \caption{Intersezione tra FSA}
  \label{fig:intersezione-FSA-2}
  \bigskip
\end{figure}

\paragraph{Unione}
\label{par:FSA-unione}
% pagina 111 libro, slide 35 1-02.FSA
Analogamente a quanto avviene nell'intersezione (Sezione~\ref{par:intersezione-FSA}), con le stesse ipotesi, l'unione di due \FSA:

\begin{align*}
  A_1 & = \langle Q_1, I_1, \delta_1, q_{01}, F_1 \rangle \\
  A_2 & = \langle Q_2, I_2, \delta_2, q_{02}, F_2 \rangle
\end{align*}

La loro unione sarà data da:

\begin{gather*}
  C = \langle A_1, A_2 \rangle = \langle Q_1 \times Q_2, I, \delta, \langle q_{01}, q_{02} \rangle, F_1 \times Q_2 \cup Q_1 \times F_2 \rangle \\
  \delta(\langle q_1, q_2 \rangle, i) = \langle \delta_1 (q_1, i), \delta_2(q_2, i) \rangle
\end{gather*}

\paragraph{Complemento}
% pagina 109 libro, slide 36 1-02.FSA
Si consideri il \FSA:

\[ A = \langle Q, I, \delta, q_0, F \rangle \]

Per poterlo complementare, è prima necessario costruire un \FSA \(A^\prime\) aggiungendo un nuovo stato \(\overline{q}\) ad \(A\) in modo che la funzione di transizione di \(A^\prime\) conduca a \(\overline{q}\) ogni qualvolta che è indefinita in \(A\).
Si imponga inoltre che l'automa, una volta in \(\overline{q}\), ci rimanga per qualsiasi simbolo di ingresso.

In questo modo, la funzione di transizione di \(A\) (e quindi la sua corrispondente \(\delta^\ast)\) è totale.

\textit{Proprietà del complemento:}

\begin{itemize}
  \item Se l'intera stringa di ingresso viene scandita, allora per complementare il risultato basta \textit{scambiare il} \texttt{si} \textit{con il} \texttt{no}
  \item Se la fine della stringa non viene raggiunta, allora lo scambio di \(F\) con \(Q - F\) non funziona
  \item Nel caso degli \FSA il \inlinequote{trucco} consiste nel completare la funzione di transizione \(\delta\)
  \item In generale, \textit{non è possibile considerare equivalenti la risposta negativa a una domanda e la risposta positiva della domanda opposta}
\end{itemize}

Grazie a questa operazione, è possibile definire l'unione di due \FSA (Sezione~\ref{par:FSA-unione}) tramite le leggi di \textit{De Morgan}:

\[ L(A_1) \cup L(A_2) = \overline{\overline{L(A_1)} \cap \overline{L(A_2)}} = \left(L(A_1)^C \cup L(A_2)^C\right)^C \]

con il medesimo risultato.

\clearpage

\section{Pushdown automaton - \PDA}

Come mostrato prima nell'applicazione del \textit{pumping lemma} al linguaggio \( L = \left\{ a^n b^n \, | \, n > 0 \right\}\) (Sezione~\ref{par:conseguenza-negativa-pumping-lemma}), dimostra l'impossibilità di riconoscere alcuni linguaggi tramite \FSA, a causa della loro inabilità nel contare una quantità di simboli sconosciuta a priori.

A causa di questa mancanza, essi sono inadatti al riconoscimento di molti linguaggi di interesse pratico.
Per ovviare a questo limite, vengono introdotti i \PDA (in Italiano \textit{automi a pila} or \textit{AP}), il cui diagramma semplificato è mostrato in Figura~\ref{fig:diagramma-PDA}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-2.tikz}
  \caption{Diagramma semplificato di un PDA}
  \label{fig:diagramma-PDA}
  \bigskip
\end{figure}

La \textbf{pila} è una forma di memoria in cui:

\begin{itemize}
  \item I nuovi simboli sono \textbf{inseriti in cima}
  \item La pila viene \textbf{letta dalla cima}
  \item Un simbolo letto viene \textbf{estratto dalla cima}
        \begin{itemize}
          \item[\(\rightarrow\)] si attua una politica \texttt{FIFO}
        \end{itemize}
  \item L'ultimo elemento in basso della pila è occupato da un particolare simbolo \(Z_0\)
\end{itemize}

I \PDA differiscono dagli automi a stati finiti in due modi:

\begin{enumerate}
  \item Possono usare la cima della pila per decidere quale transizione effettuare
  \item Possono manipolare la pila durante una transizione
\end{enumerate}

\subsection{Definizione formale \PDA}
\label{sec:definizione-formale-PDA}

Formalmente, un \PDA é una tupla di \(7\) elementi \( \langle Q, A, \Gamma, \delta, q_0, Z_0, F \rangle \), dove:

\begin{itemize}
  \item \(Q\) è un \textbf{insieme di stati}, finito
  \item \(A\) è l'\textbf{alfabeto} di ingresso
  \item \(\Gamma\) è l'\textbf{alfabeto} di pila
  \item \(\delta\) è la funzione di \textbf{transizione}:
        \begin{itemize}
          \item \(\delta: Q \times \left(I \, \cup \, \{\epsilon\} \right) \times \Gamma \rightarrow Q \times \Gamma^\ast\)
          \item Il \textbf{dominio} è definito come il prodotto cartesiano tra stati, alfabeto di ingresso unito all'elemento nullo e alfabeto di pila
          \item Il \textbf{codominio} è definito come il prodotto cartesiano tra stati e \textit{stella di Kleene} dell'alfabeto di pila
        \end{itemize}
  \item \(Z_0 \in \Gamma\) è il \textbf{simbolo iniziale} (il primo in \textit{basso}) di pila
  \item \(q_0 \in Q\) è lo \textbf{stato iniziale}
  \item \(F \subseteq Q\) è l'insieme di \textbf{stati finali}, finito
\end{itemize}

Si noti che la definizione di \(Q, A, \delta, q_0, F\) sono analoghi a quanto illustrato in Sezione~\ref{sec:definizione-formale-FSA} riguardo gli \FSA.

\subsection{Mosse di un \PDA}

In base a:

\begin{itemize}
  \item Il simbolo \textbf{letto} dall'ingresso \textit{(opzionalmente)}
  \item Il simbolo \textbf{letto} dalla cima della pila
  \item Lo \textbf{stato} del dispositivo di controllo
\end{itemize}

il \PDA:

\begin{itemize}
  \item Cambia il proprio \textbf{stato}
  \item Sposta in avanti la \textbf{stato}
  \item Sostituisce il simbolo letto dalla \textbf{pila} con una stringa \(\alpha\) \textit{eventualmente vuota}
\end{itemize}

Sono anche ammesse delle delle mosse \textit{spontanee} (dette \(\epsilon\)-mosse) che avvengono anche senza che venga letto il simbolo dalla stringa, ignorandolo.
Per queste mosse la funzione di transizione è definita come:
\[ \delta(q, \epsilon, A) = \langle p, \alpha \rangle \]
Se una \(\epsilon\)-mossa è stata definita per una coppia \(\langle \overline{Q}, \overline{A} \rangle\) \textit{(stato e simbolo in cima alla pila)}, non è possibile definire un'altra mossa non-\(\epsilon\) per lo stesso simbolo \(\overline{A}\) dallo stesso stato \(\overline{Q}\).

\textbf{Gli \FSA non hanno \(\epsilon\)-mosse}, in quanto esse sono una caratteristica dei \PDA.
Essi non implicano che il carattere in cima alla stringa sia \(\epsilon\) ma che la stringa non viene letta.

\subsection{Accettazione di una stringa}

Alla fine della computazione, la stringa di ingresso \(x\) è \textit{riconosciuta} (o \textit{accettata}) se valgono entrambe le condizioni:

\begin{enumerate}
  \item Il \PDA la legge \textbf{completamente}
  \item Il \PDA si trova in uno stato di accettazione (\textbf{finale}) quando la fine di \(x\) è stata raggiunta
\end{enumerate}

Non ci sono condizioni sulla pila sia vuota affinché la stringa \(x\) sia accettata.
In particolare, \textbf{non è necessario che la pila sia vuota} alla fine della computazione \textit{(anche se è una condizione che si può verificare)}.

\subsection{Configurazione di un \PDA}
\label{sec:configurazione-PDA}
% slide 26 - 1-03PDA.pdf

Una \textbf{configurazione} è una generalizzazione della nozione di stato.
Essa mostra:

\begin{itemize}
  \item Lo \textbf{stato corrente} del dispositivo di controllo
  \item La \textbf{porzione di stringa} di ingresso a destra dalla testina
        \begin{itemize}
          \item indica la parte di essa ancora non letta
          \item ciò chè è già stato letto non è rilevante poiché è già stato consumato
        \end{itemize}
  \item Il \textbf{contenuto} della pila
\end{itemize}

Può essere vista come una \textit{istantanea} del \PDA, mostrando nel tempo il suo stato interno.

\bigskip
\textit{Formalmente}, la configurazione \(c\) è una tripla \(\langle q, x, \gamma \rangle\):

\begin{itemize}
  \item \(q \in Q\) è lo \textbf{stato corrente} del dispositivo di controllo
  \item \(x \in I^\ast\) è la \textbf{porzione non letta} della stringa di ingresso
  \item \(\gamma \in \Gamma^\ast\) è la \textbf{stringa di simboli} nella pila.
\end{itemize}

\subsection{Transizioni di un \PDA}
% slide 28 1-03PDA.pdf

Le \textbf{transizioni} tra configurazioni dipendono dalla funzione di transizione e si indicano con il simbolo \(\vdash\).
Una sequenza di transizioni è indicata col simbolo \(\vdash^\ast\).

Esse illustrano come commutare tra una \textit{istantanea} e la sua successiva.
Per un dato \PDA \(A\), la transizione \(\vdash_A\) nello spazio di tutte le possibili configurazioni di \(A\) è definita da
\[ c = \langle q, x, \gamma \rangle \, \vdash_A c^\prime  = \langle q^\prime , x^\prime , \gamma^\prime  \rangle \]
solo se vale una delle due condizioni:

\begin{enumerate}
  \item La funzione di transizione è definita per un simbolo di ingresso
        \begin{align*}
          x = ay           & \mapsto \ x{'} = y                    \\
          \gamma = A \beta & \mapsto \gamma^\prime  = \alpha \beta \\
          \delta(q, a, A)  & =  \langle q^\prime , \alpha \rangle
        \end{align*}
  \item La funzione di transizione è definita per una \(\epsilon\)-mossa
        \begin{align*}
          x                      & \mapsto x{'},                         \\
          \gamma       = A \beta & \mapsto \gamma^\prime  = \alpha \beta \\
          \delta(q, \epsilon, A) & = \langle q^\prime , \alpha \rangle
        \end{align*}
\end{enumerate}

\subsubsection{Nota sulle transizioni}

Si consideri un generico \PDA con le caratteristiche già indicate, sia \(\delta\) la sua funzione di transizione.

Si indica con il simbolo \(\perp\) il risultato di una transizione che
\textbf{non è definita} per i parametri indicati.

Allora:

\begin{itemize}
  \item Una \(\epsilon\)-mossa è una mossa \textbf{spontanea}:
        \begin{itemize}
          \item se \(\delta(q, \epsilon, A) \neq \bot \) \textit{(\(\delta\) non è indefinita)} e \(A\) è il simbolo in cima alla pila, la transizione \textbf{può sempre essere eseguita}
        \end{itemize}
  \item Se \(\delta(q, \epsilon, A) \neq \bot\), allora \(\forall \, i \in I, \delta(q, i, A) = \bot\)
        \begin{itemize}
          \item se questa proprietà non fosse soddisfatta, entrambe le transizioni \textbf{sarebbero consentite}
          \item questa condizione \textbf{garantisce il determinismo} poiché non sarebbe possibile definire unicamente quale delle più transizioni scegliere partendo dallo stato
          \item più avanti verrà mostrato il funzionamento degli automi in assenza di determinismo (Sezione~\ref{sec:non-determinismo})
        \end{itemize}
\end{itemize}

\subsubsection{Notazione grafica delle transizioni di un \PDA}

Il diagramma mostrato in Figura~\ref{fig:notazione-grafica-PDA-push} mostra una transizione di un \PDA da uno stato \(q_0\) a uno stato \(p_0\).

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, on grid, node distance=4cm, >=Triangle]
    \node [state, initial, initial text=](q) {\(q_0\)};
    \node [state](p) [right= of q] {\(p_0\)};

    \path [->, thick]
    (q) edge [] node {\(a, B \, | \, CB \)} (p);

  \end{tikzpicture}
  \caption{Notazione grafica dei \PDA, \textit{push}}
  \label{fig:notazione-grafica-PDA-push}
  \bigskip
\end{figure}

La notazione implica che se:

\begin{enumerate}
  \item Il \PDA si trova nello \textbf{stato} \(q_0\)
  \item Il \textbf{carattere} \(a\) viene letto dalla stringa di input
  \item Il \textbf{carattere} \(B\) è in cima alla pila
\end{enumerate}

allora succederanno i seguenti eventi:

\begin{enumerate}
  \item Il carattere \(C\) viene \textbf{aggiunto} in cima alla pila \textit{(operazione di push)}
  \item Lo stato attuale diventa \(p_0\)
\end{enumerate}

\bigskip

Il diagramma mostrato in Figura~\ref{fig:notazione-grafica-PDA-pop} mostra una transizione di un \PDA da uno stato \(q_1\) a uno stato \(p_1\).

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, on grid, node distance=4cm, >=Triangle]
    \node [state, initial, initial text=](q) {\(q_1\)};
    \node [state](p) [right= of q] {\(p_1\)};

    \path [->, thick]
    (q) edge [] node {\(a, B \, | \, \epsilon\)} (p);

  \end{tikzpicture}
  \caption{Notazione grafica dei \PDA, \textit{pop}}
  \label{fig:notazione-grafica-PDA-pop}
  \bigskip
\end{figure}

La notazione implica che se:

\begin{enumerate}
  \item Il \PDA si trova nello \textbf{stato} \(q_1\)
  \item Il \textbf{carattere} \(a\) viene letto dalla stringa di input
  \item Il \textbf{carattere} \(B\) è in cima alla pila
\end{enumerate}

allora succederanno i seguenti eventi:

\begin{enumerate}
  \item Il carattere \(B\) viene \textbf{rimosso} dalla pila \textit{(operazione di pop)}
  \item Lo stato attuale diventa \(p_1\)
\end{enumerate}

\subsection{Condizione di accettazione di un \PDA}

Sia \(\vdash^\ast\) la \textit{chiusura riflessiva} e \textit{transitiva} della relazione \(\vdash\).
La condizione di accettazione è:
\[ \forall \, x \in I^\ast \, | \, x \in L \Leftrightarrow c_0 = \langle q_0, x, Z_0 \rangle \vdash^\ast c_F = \langle q, \epsilon, \gamma \rangle, \ q \in F \]

in cui:
\begin{enumerate}
  \item La configurazione iniziale parte dallo \textbf{stato iniziale} con la \textbf{stringa ancora interamente da leggere} e la \textbf{pila vuota}
  \item Viene applicato un certo numero di \textbf{transizioni}
  \item La configurazione finale ha la \textbf{stringa completamente letta} e lo \textbf{stato attuale è nell'insieme degli stati finali} mentre non ci sono condizioni sullo stato della pila
\end{enumerate}

\textit{Informalmente}, una stringa è accettata da un \PDA se esiste un cammino coerente con \(x\) su di esso che va dallo stato iniziale allo stato finale.
La stringa deve essere letta in tutta la sua interezza.

\subsection{\PDA vs \FSA}

Alcuni linguaggi, a causa del \textit{pumping lemma}, non possono essere riconosciuti da un \FSA.
Essi tuttavia possono essere riconosciuti da un \PDA e prendono il nome di \textbf{linguaggi regolari}.

I \PDA sono quindi \textbf{più espressivi} degli \FSA.

Infatti, dato un \FSA \(A = \langle Q, I, \delta, q_0, F \rangle\) è immediato costruire un \PDA \(A^\prime  = \langle Q^\prime , i^\prime , \Gamma^\prime , q_0^\prime , Z_0^\prime , F^\prime  \rangle \) tale che \(L(A) = L(A^\prime )\).

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.7]{image-3.tikz}
  \caption{Linguaggi riconosciuti da \PDA e \FSA}
  \label{fig:PDA-vs-FSA}
  \bigskip
\end{figure}

\textit{Informalmente}, i \PDA, rispetto agli \FSA, hanno la capacità di invertire le stringhe di lunghezza indefinita e contare, proprio grazie alla loro memoria a pila.

\subsection{\PDA ciclici e aciclici}

A differenza degli \FSA, i \PDA potrebbero non fermarsi dopo un numero finito di mosse: sono possibili \textbf{cicli} di \(\epsilon\)-mosse.
Tuttavia, i \PDA ciclici non aggiungono potere espressivo alla classe dei \PDA e questa loro caratteristica potrà essere rimossa.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, on grid, node distance = 4cm, >=Triangle]
    \node [state, initial, initial text = \(a Z_0 \, \vert \, A Z_0\)](q) {\(q\)};

    \path [->, thick]
    (q) edge [loop above] node {\(\epsilon, A \, | \, AA \)} ();

  \end{tikzpicture}

  \caption{\PDA ciclico}
  \label{fig:PDA-ciclo}
  \bigskip
\end{figure}

Formalmente, si consideri un \PDA \(A\).
La notazione
\[ \langle q, x, \alpha \rangle \vdash^\ast_d \langle q^\prime , y, \beta \rangle\]
indica che:

\begin{enumerate}
  \item Il \PDA evolve dalla \textit{configurazione iniziale} \(\langle q, x, \alpha \rangle\) alla configurazione \(\langle q^\prime , y, \beta \rangle\) tramite operatore \(\vdash^\ast\)
  \item Per \(\beta = Z \beta^\prime\), \(\delta(q^\prime , \epsilon, Z) = \bot\) (è quindi \textit{indefinita})
        \begin{itemize}
          \item \(\vdash^\ast_d\) è una sequenza di mosse che porta a una configurazione da cui \textbf{non è possibile procedere} con una \(\epsilon\)-mossa
          \item per evolvere da questa configurazione è necessario leggere un simbolo in ingresso
        \end{itemize}
\end{enumerate}

Ciò detto, un \PDA è \textbf{aciclico} se e solo se
\[ \forall \, x \in I^\ast \ \exists \, (q, y) \, | \, \langle q_0, x, Z_0 \rangle \vdash^\ast_d  \langle q, \epsilon, \gamma \rangle \]
e che quindi:

\begin{enumerate}
  \item Legge sempre l'intera stringa di ingresso e, al termine di essa, si ferma
  \item Non può ripetere un ciclo indefinitamente con \(\epsilon\)-mosse
  \item Si ferma dopo un numero finito di mosse
\end{enumerate}

Ogni \PDA ciclico può (e deve) essere trasformato nel proprio equivalente aciclico.
Un \PDA che presenta cicli, infatti, potrebbe non raggiungere mai la fine della stringa e il suo corrispondente stato di accettazione, rimanendo per sempre in un ciclo di \(\epsilon\)-mosse.

\subsection{Operazioni sui \PDA}

Nelle successive Sezioni (\ref{sec:complemento-PDA}-\ref{sec:differenza-PDA}) ci si occuperà di trattare la chiusura dei linguaggi riconosciuti da \PDA rispetto a varie operazioni.

\subsubsection{Complemento}
\label{sec:complemento-PDA}

La classe dei linguaggi riconosciuti da \PDA \textbf{è chiusa} rispetto alla complementazione.
Il complemento può essere costruito:

\begin{itemize}
  \item Eliminando i cicli
  \item Aggiungendo stati di errore
  \item Scambiando stati finali con non finali
  \item Occupandosi di \(\epsilon\)-mosse che collegano stati finali e non finali:
        \begin{itemize}
          \item Un \PDA potrebbe imporre l'accettazione solo alla fine di una sequenza \textit{(finita)} di \(\epsilon\)-mosse
          \item Se così fosse, scambiare gli stati finali e iniziali non porterebbe a una complementazione corretta
        \end{itemize}
\end{itemize}

\subsubsection{Unione}
\label{sec:chiusura-PDA-unione}
La classe dei linguaggi riconosciuti dai \PDA \textbf{non è chiusa} rispetto all'unione.

\bigskip

\textit{Per esempio}, non esiste alcun \PDA che riconosca \(\left\{ a^n b^n \, | \, n \geq 1 \right\} \cup \left\{ a^n b^{2n} \, | \, n \geq 1 \right\}\).
Tuttavia:

\begin{itemize}
  \item \(A = \left\{ a^n b^n \, | \, n \geq 1 \right\}\) è riconoscibile via \PDA
  \item \(B = \left\{ a^n b^{2n} \, | \, n \geq 1 \right\}\) è riconoscibile via \PDA
\end{itemize}

\textit{Esempio intuitivo della dimostrazione:}

\begin{itemize}
  \item Se si procede riconoscendo una stringa di \(A\) ma trovando una stringa di \(B\), pur sapendo che rimangono \(n\) caratteri \(b\) da leggere si ha perso l'informazione sul valore stesso di \(n\)
  \item Analogamente, riconoscendo una stringa di \(B\) ma trovando una stringa di \(A\), pur sapendo che sono rimasti almeno \(n\) simboli nella pila non sarà possibile verificare se essi sono effettivamente \(n\)
\end{itemize}

\subsubsection{Intersezione}
\label{sec:intersezione-PDA}

La classe dei linguaggi riconosciuti da \PDA \textbf{non è chiusa} rispetto all'intersezione.

Applicando la legge di \textit{De Morgan} si verifica che:
\[ \cup B = \overline{\left(\overline{A} \cap \overline{B} \right)} =\left( A^C \cap B^C\right)^C \]
Poiché i linguaggi dei \PDA sono chiusi rispetto al complemento, se fossero chiusi rispetto all'intersezione dovrebbero essere chiusi anche rispetto all'unione, in contrario a quanto affermato nella Sezione~\ref{sec:differenza-PDA}

\subsubsection{Differenza}
\label{sec:differenza-PDA}

La classe dei linguaggi riconosciuti da \PDA \textbf{non è chiusa} rispetto alla differenza.

Applicando le leggi insiemistiche, si verifica che:
\[ A \cap B = A - \overline{B} = A - B^C \]
Poiché i linguaggi dei \PDA sono chiusi rispetto al complemento, se fossero chiusi rispetto alla differenza dovrebbero essere chiusi anche rispetto all'intersezione, in contrario a quanto affermato nella Sezione~\ref{sec:intersezione-PDA}

\subsection{Trasduttori a pila - \PDT}

\subsubsection{Definizione formale \PDT}

Un \textbf{trasduttore a pila} (\textit{pushdown transducer} o \PDT) è una tupla \(\langle Q, I, \Gamma. \delta, q_0, Z_0, F, O, \eta \rangle\), dove:

\begin{itemize}
  \item \(Q\) è un \textbf{insieme di stati}, finito
  \item \(I\) è l'\textbf{alfabeto di ingresso}
  \item \(\Gamma\) è l'\textbf{alfabeto di pila}
        \begin{itemize}
          \item \(Z_0 \in \Gamma\) è il simbolo iniziale (il primo in \textit{basso}) di pila
        \end{itemize}
  \item \(\delta\) è la \textbf{funzione di transizione}
  \item \(q_0 \in Q\) è lo \textbf{stato iniziale}
  \item \(F \subseteq Q\) è l'\textbf{insieme di stati finali}
  \item \(O\) è l'\textbf{alfabeto di uscita}
  \item \(\eta\) è la \textbf{funzione di uscita}:
        \begin{itemize}
          \item \(\eta: Q \times \left(I \cup \{ \epsilon \}\right) \times \Gamma \rightarrow O^\ast\)
          \item \textbf{dominio}: prodotto cartesiano dell'insieme di stati, alfabeto di ingresso (compreso \(\epsilon\)) e alfabeto di pila
          \item \textbf{codominio}: stella di Kleene dell'alfabeto di uscita
        \end{itemize}
\end{itemize}

Si noti che:

\begin{itemize}
  \item \(Q, I, \Gamma, \delta, q_0, Z_0, F\) sono definiti in modo analogo ai \PDA nella sezione~\ref{sec:definizione-formale-PDA}.
  \item \(\eta\) è definita solo dove è definita \(\delta\)
  \item La pila può essere necessaria perché richiesta dal linguaggio da riconoscere o perché richiesta dalla traduzione
\end{itemize}

L'illustrazione della transizione tra due stati con \(\delta(q, I, A) = \langle p, \alpha \rangle\) e \(\eta(q, I, A) = w\) è mostrata in Figura~\ref{fig:transizione-stati-PDT}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, on grid, node distance = 4cm, >=Triangle]
    \node [state](q) {\(q\)};
    \node [state](p) [right=of q] {\(p\)};

    \path[thick, ->]
    (q) edge [] node {\(i, A / \alpha, w\)} (p);

  \end{tikzpicture}

  \caption{Transizione tra due stati in un \PDT}
  \label{fig:transizione-stati-PDT}
  \bigskip
\end{figure}

I \PDT sono molto utilizzati per sopperire a due grosse lacune dei \FST: \textit{contare} i caratteri e \textit{invertire} le stringhe.

\subsubsection{Configurazione di un \PDT}

Una configurazione \(c\) di un \PDT è una tupla \(\langle q, x, \gamma, z \rangle\) in cui:

\begin{itemize}
  \item \(q \in Q\) è lo \textbf{stato corrente} del dispositivo di controllo
  \item \(x \in I^\ast\) è la \textbf{porzione non letta} della stringa d'ingresso
  \item \(\gamma \in \Gamma\) è la stringa di \textbf{simboli nella pila}
  \item \(z\) è la stringa già scritta sul \textbf{nastro di uscita}
\end{itemize}

\subsubsection{Transizioni di un \PDT}
% da slide 45 - 1-03PDA.pdf

Per un dato \PDT \(T\), la \textbf{relazione binaria di transizione} \(\vdash_T\) nello spazio di tutte le possibili configurazioni di \(A\) è definita da:
\[ \langle q, x, \gamma, z \rangle \vdash_a c^\prime = \langle q^\prime, x^\prime, \gamma^\prime, z^\prime \rangle, \quad z^\prime = z \overline{z} \]

se e solo se vale una delle due condizioni:

\begin{gather}
  x = ay, \ x^\prime = y, \ \gamma^\prime = A \beta, \ \gamma^\prime = \alpha \beta, \ \delta(q, a, A) = \langle q^\prime, \alpha \rangle, \ \overline{z} = \eta(q, a, A) \label{eq:transizione-PDT-0} \\
  x = x^\prime, \ \gamma = A \beta, \ \gamma^\prime = \alpha\beta, \ \delta(q, \epsilon, A) = \langle q^\prime, \alpha \rangle, \ \overline{z} = \eta(q, \epsilon, A) \label{eq:transizione-PDT-1}
\end{gather}

Ovverosia:

\begin{itemize}
  \item La condizione \ref{eq:transizione-PDT-0} descrive il passaggio da una configurazione all'altra quando viene letto un simbolo in ingresso
  \item La condizione \ref{eq:transizione-PDT-1} prende in considerazione il caso di \(\epsilon\)-mosse \textit{(quando la testina in ingresso rimane ferma)}
\end{itemize}

\subsubsection{Condizione di accettazione di un \PDT}
% da slide 46 - 1-03PDA.pdf

La condizione di accettazione di un \PDT può essere descritta come:
\[ \forall \, x \in I^\ast, \ x \in L \Leftrightarrow c_0 = \langle q_0, x, Z_0, \epsilon \rangle \vdash^\ast c_F = \langle q, \epsilon, \gamma, z \rangle, \ q \in F\]

La traduzione di \(x\) è definita se e solo se la stringa \(x\) è accettata

\clearpage

\section{Macchine di Turing - \TM}

Come visto in precedenza (Sezione~\ref{sec:chiusura-PDA-unione}), l'unione di alcuni linguaggi riconosciuti dai \PDA non può essere riconosciuta dai \PDA perché la classe di questi ultimi non è chiusa rispetto all'unione.

\bigskip

Si consideri il linguaggio \(L = \{a^n b^n c^n \, | \, n > 0\}\):

\begin{itemize}
  \item La pila può essere usata per contare le \(a\)
  \item I simboli sulla pila possono essere usati per controllare che il numero di \(b\) sia uguale al numero di \(a\)
        \begin{itemize}
          \item[\xmark] Il numero di \(c\), di conseguenza, \textbf{non può essere contato}
        \end{itemize}
\end{itemize}

Questa limitazione è dovuta alla \textbf{pila}.
Infatti essa è una memoria \textbf{distruttiva}, perché una volta che il simbolo viene letto, esso è distrutto.
Questa proprietà può essere dimostrata formalmente attraverso una generalizzazione del \textit{pumping lemma} (visto in Sezione~\ref{sec:pumping-lemma}).

Per risolvere questo problema, sono state introdotte le \textbf{Macchine di Turing} (in Inglese \textit{Turing Machine} o \TM), che fanno uso di \textbf{nastri di memoria}.
Essi, infatti, non sono di natura distruttiva e possono venire letti più volte.

\bigskip
Un diagramma semplificato di una \TM è mostrato in Figura~\ref{fig:diagramma-TM}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.8]{image-4.tikz}
  \caption{Diagramma semplificato di una TM}
  \label{fig:diagramma-TM}
  \bigskip
\end{figure}

\textit{Informalmente}, una \TM è costituita da:

\begin{itemize}
  \item \textbf{Stati} e \textbf{alfabeto} definiti come negli \FSA (Sezione~\ref{sec:definizione-formale-FSA})
  \item Un nastro di \textbf{ingresso}, con la testina \(T_I\)
  \item Un nastro di \textbf{uscita}, con la testina \(T_F\)
  \item Un \textbf{dispositivo di controllo}
  \item Un \textbf{alfabeto di memoria}
  \item \(k\) nastri di \textbf{memoria}:
        \begin{itemize}
          \item ogni nastro presenta una \textbf{testina di lettura}, \(T_0, \ldots, T_k\)
          \item rappresentati come \textbf{sequenze infinite}
          \item gli spazi \textbf{vuoti} sono occupati da simboli speciali detti \texttt{blank} (anche \blank, \(-\))
          \item inizialmente, ogni nastro di memoria è \textbf{vuoto}
          \item in ogni momento, i nastri contengono \textbf{un numero finito} di simboli non-\texttt{blank}
        \end{itemize}
\end{itemize}

Un passo di \textbf{computazione} di una \TM è basato su:

\begin{itemize}
  \item Un \textbf{simbolo letto} dal nastro di ingresso
  \item \(K\) simboli, uno per ogni nastro di \textbf{memoria}
  \item \textbf{Stato} del dispositivo di controllo
\end{itemize}

e può eseguire le seguenti operazioni \textbf{(mosse)}:

\begin{enumerate}
  \item \textbf{Cambio di stato} del dispositivo di controllo
  \item \textbf{Sovrascrittura} di un simbolo in nelle celle sottostanti alle testine dei nastri di memoria
  \item \textbf{Spostamento} delle \(k+1\) testine:
        \begin{itemize}[label=\(\rightarrow\)]
          \item i \(k\) nastri di memoria possono spostarsi a \textit{sinistra} o \textit{destra}
          \item il nastro di uscita può spostarsi solo a \textit{destra}
        \end{itemize}
  \item Nessuna operazione, fermandosi \textbf{definitivamente}
\end{enumerate}

La direzione di spostamento di ogni testina deve essere specificata esplicitamente, indicando:

\begin{itemize}
  \item[\(\rightarrow\)] \(R\) per \textit{destra di una posizione}
  \item[\(\leftarrow\)] \(L\) per \textit{sinistra di una posizione}
  \item[\(\downarrow\)] \(S\) per \textit{nessuno spostamento}
\end{itemize}

\subsection{Definizione formale \TM}
\label{sec:definizione-formale-TM}

Formalmente, una \TM con \(k\) nastri è una tupla di \(9\) elementi \(\langle Q, I, \Gamma, O, \delta, \eta, q_0, Z_0, F \rangle\) dove:

\begin{itemize}
  \item \(Q\) è un insieme di \textbf{stati}, finito
  \item \(I\) è l'\textbf{alfabeto} di \textbf{ingresso}
  \item \(O\) è l'\textbf{alfabeto} di \textbf{uscita}
  \item \(\Gamma\) è l'\textbf{alfabeto} di \textbf{memoria}
  \item \(\delta\) è la \textbf{funzione} di \textbf{transizione}:
        \begin{itemize}
          \item  \(\delta: \left(Q-F\right) \times I \times \Gamma^k \rightarrow Q \times \Gamma^k \times \left\{R, L, S \right\}^{k+1}\)
          \item \textbf{dominio}: prodotto cartesiano di tutti gli stati meno quelli finali, alfabeto di ingresso e alfabeto di memoria \textit{elevato al numero di nastri}
          \item \textbf{codominio}: prodotto cartesiano di tutti gli stati, dell'alfabeto di memoria elevato al numero di nastri e dei possibili movimenti dei nastri \textit{elevati a \(k+1\)}
                \begin{itemize}[label=\(\rightarrow\)]
                  \item \(k+1\) perché si riferisce ai \(k\) nastri di memoria e il nastro di uscita
                \end{itemize}
          \item può essere \textbf{parziale}, come avviene nei \FSA (Sezione~\ref{sec:definizione-formale-FSA})
          \item è definita in modo tale che non ci siano transizioni uscenti da uno stato finale
        \end{itemize}
  \item \(\eta\) è la \textbf{funzione} di \textbf{uscita}:
        \begin{itemize}
          \item \(\eta : (Q - F) \times I \times \Gamma^k \rightarrow O \times \{R, S\}\)
          \item \textbf{dominio}: coincide con quello della funzione \(\delta\)
          \item \textbf{codominio}: prodotto cartesiano dell'alfabeto di uscita con i possibili movimenti del nastro di uscita
          \item può essere \textbf{parziale}
        \end{itemize}
  \item \(q_0 \in Q\) è lo stato \textbf{iniziale}
  \item \(Z_0 \in \Gamma\) è il \textbf{simbolo iniziale} di \textbf{memoria}
  \item \(F \subseteq Q\) è l'\textbf{insieme di stati finali}
\end{itemize}

Se il valore di \(k\) non è indicato, la \TM viene semplicemente chiamata \TM \textit{multinastro (con un numero arbitrario di nastri)}.

La presenza di \(O\) e \(\eta\) non è obbligatoria.
Essi infatti sono definiti se è previsto un \textit{nastro di uscita}, come per esempio nelle \TM \textit{traduttrici} (Sezione~\ref{sec:TM-traduttrice}).

\bigskip
Si consideri una transizione in cui:

\begin{itemize}
  \item \(q_0, q_1\) sono due \textbf{stati}
  \item \(i\) è il \textbf{simbolo} di \textbf{ingresso}
  \item \(A_j\) è il \textbf{simbolo} letto dal \(j\)-esimo nastro di memoria
  \item \(A_j^\prime\) è il \textbf{simbolo} che rimpiazza \(A_j\)
  \item \(M_0\) è la \textbf{direzione} della testina del nastro di ingresso \(T_i\)
  \item \(M_j, \  \forall \, 1 \leq j \leq k\) è la \textbf{direzione} della testina del \(j\)-esimo nastro di memoria
\end{itemize}

La notazione grafica di questa situazione è mostrata in Figura~\ref{fig:esempio-transizione-TM}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, on grid, node distance = 10cm, >=Triangle]
    \node [state](q0) {\(q_0\)};
    \node [state](q1) [right=of q0] {\(q_1\)};

    \path[->, thick]
    (q0) edge [] node {\( i, \langle A_1, A_2, \ldots, A_k \rangle \, | \, \langle  A_1^\prime , A_2^\prime , \ldots, A_k^\prime  \rangle, \langle  M_0 M_1 \ldots, M_k \rangle\)} (q1);
  \end{tikzpicture}

  \caption{Transizione di una TM}
  \label{fig:esempio-transizione-TM}
  \bigskip
\end{figure}

\subsection{Configurazione di una \TM}

Analogamente a quanto visto \PDA (Sezione~\ref{sec:configurazione-PDA}), anche le \TM ammettono la definizione di \textbf{configurazione}.
Essa dovrà includere:

\begin{itemize}
  \item Lo \textbf{stato} del dispositivo di controllo
  \item La \textbf{stringa} sul nastro di ingresso e la \textbf{posizione della testina}
  \item la \textbf{stringa} e la \textbf{posizione della testina} per ogni nastro di memoria
\end{itemize}

\bigskip
\textit{Formalmente:} una configurazione \(c\) di una \TM con \(k\) nastri di memoria è la tupla di \(k+3\) elementi:
\[ c = \langle x \uparrow iy, \alpha_1 \uparrow A_1 \beta_1, \ldots, \alpha_k \uparrow A_k \beta_k, u \uparrow o \rangle \]

\begin{itemize}
  \item \(q \in Q\) è lo \textbf{stato corrente} del dispositivo di controllo
  \item \(x, y \in I^\ast, i \in I\) rappresentano il \textbf{contenuto} del nastro di ingresso
  \item \(\alpha_r, \beta_r \in \Gamma^\ast, \ A_r \in \Gamma \  \forall \, r \, 1 \leq r \leq k\) rappresentano il contenuto dei \textbf{nastri di memoria}
        \begin{itemize}
          \item le testine di ogni nastro sono posizionate sulla cella che memorizza il primo simbolo della stringa che segue il simbolo \(\uparrow\)
          \item \(\uparrow \, \notin \left\{ I \cup \Gamma \cup O \right\}\)
        \end{itemize}
  \item \(uo\) è il contenuto del \textbf{nastro di uscita} (se definito, come nelle \TM traduttrici viste in Sezione~\ref{sec:TM-traduttrice})
        \begin{itemize}
          \item \(u \in O^\ast\) è la stringa già scritta sul nastro di uscita
          \item \(o \in O\) è l'ultimo carattere scritto
        \end{itemize}
\end{itemize}

\subsubsection{Configurazione iniziale}

La \textbf{configurazione iniziale} \(c_0\) di \(M\) è del tipo: \(c_0 = \langle q_0, \uparrow iy, \uparrow Z_0, \ldots, \uparrow Z_0, \uparrow Z_0, \uparrow \blank \rangle\)
dove:

\begin{itemize}
  \item \(q = q_0\), lo stato attuale è quello iniziale
  \item \(x = \epsilon\), nessun carattere della stringa è ancora stata letta
  \item \(A_r = Z_0, \alpha_r = \beta_r = \epsilon \ \forall r\), tutti i nastri di memoria sono vuoti
  \item \(u = \epsilon, o = \blank\) il nastro di uscita è vuoto \textit{(se definito)}
\end{itemize}

Inoltre, tutte le testine sono all'inizio del corrispondente nastro.

\subsection{Transizioni di una \TM}

La relazione di \textbf{transizione} \(\vdash_M\) (detta anche \textit{mossa} o \textit{passo computazionale}) fra due configurazioni \(c\) e \(c^\prime\) di una \TM multinastro \(M\) è definita nel modo seguente:

Siano \(c, c^\prime\) le due configurazioni tra le quali si esegue la transizione:
\begin{align*}
   & c = \langle q, x \uparrow iy, \alpha_1 \uparrow A \beta_1, \ldots, \alpha_k \uparrow A \beta_k, u \uparrow o \rangle                                                                                                 \\
   & c = \langle q^\prime , x^\prime  \uparrow i^\prime y^\prime , \alpha_1^\prime  \uparrow A^\prime  \beta_1^\prime , \ldots, \alpha_k^\prime  \uparrow A^\prime  \beta_k^\prime , u^\prime  \uparrow o^\prime  \rangle
\end{align*}

\smallskip

Sia \(\delta\) la funzione di transizione, definita come:
\[ \delta(q, i, A_1, \ldots, A_k) = \langle p, C_1, \ldots, C_k, N, N_1, \ldots N_k \rangle \]
con:
\begin{gather*}
  p \in Q , \  N \in \{R, L, S\} , \  C_r \in \Gamma , \  N_r \in \{R, L, S\} \ \forall \, 1 \leq r \leq k \\
  x = \overline{xi} , \  y = \overline{jy} , \  \alpha_r = \overline{\alpha_r A_r} , \  \beta_r = \overline{B_r \beta_r}
\end{gather*}

\smallskip
Sia \(\eta\) la funzione di uscita, definita come:
\[ \eta (q, i, A_1, \ldots, A_k) = \langle v, M \rangle\]
con:

\[ v \in O \quad M \in \{R, S\}\]

\bigskip
Allora la transizione \(c \vdash_M c^\prime\) (da \(c\) a \(c^\prime\)) se e solo se:
\begin{gather}
  p = q^\prime \label{eq:transizione-TM-0}
\end{gather}

\bigskip
Se vale \textbf{una delle condizioni mutuamente esclusive}:
\begin{gather}
  x = x^\prime, \ i = i^\prime, \ y = y^\prime, \ N = S \label{eq:transizione-TM-1}                        \\
  x^\prime = xi, \ i^\prime = \overline{j}, \ y^\prime = \overline{y}, \ N = R \label{eq:transizione-TM-2} \\
  x^\prime = \overline{x}, \ i^\prime = i, \ y^\prime = iy, \ N = L \label{eq:transizione-TM-3}
\end{gather}

\bigskip
Inoltre, per \(1 \leq r \leq k\) deve valere anche \textbf{uno dei casi mutuamente esclusivi}:
\begin{gather}
  \alpha_r^\prime = \alpha_r, \ A_r^\prime = C_r, \ \beta_r^\prime = \beta_r, \ N_r = S \label{eq:transizione-TM-4}     \\
  \alpha_r^\prime = \alpha_r C_r, \ A_r^\prime = B_r, \ \beta_r^\prime = \overline{\beta_r}, \ N_r = R \label{eq:transizione-TM-5}     \\
  \alpha_r^\prime = \overline{\alpha_r}, \ A_r^\prime = \overline{A_r}, \ \beta_r^\prime = C_r \beta_r, \ N_r = L \label{eq:transizione-TM-6}
\end{gather}

\bigskip
Infine anche \textbf{una delle due condizioni mutualmente esclusive}:
\begin{gather}
  u^\prime = u, \ o^\prime = v, \ M = S \label{eq:transizione-TM-7} \\
  u^\prime = uv, \ o^\prime = \blank, \ M = R \label{eq:transizione-TM-8}
\end{gather}

\subsubsection{Singificato delle condizioni di transizione della \TM}

\begin{itemize}
  \item La condizione~\ref{eq:transizione-TM-0} vincola lo stato di \(c^\prime\) a essere quello di arrivo della transizione
  \item Le condizioni dal \ref{eq:transizione-TM-1} al \ref{eq:transizione-TM-3} definiscono l'evoluzione del nastro di ingresso nel passaggio da \(c\) a \(c^\prime\):
        \begin{itemize}
          \item Se la testina rimane ferma (condizione~\ref{eq:transizione-TM-1}, in cui \(N=S\)), le tre parti in cui essa divide il nastro \textit{(parte sinistra, destra e simbolo corrente)} rimangono invariate tra le due configurazioni
          \item Se la testina si muove a destra (condizione~\ref{eq:transizione-TM-2}, in cui \(N=R\)), la parte a sinistra della testina in \(c^\prime\) conterrà anche il simbolo corrente di \(c\), il simbolo corrente di \(c^\prime\) sara il primo simbolo della parte destra in \(c\) e la rimanente parte destra di \(c\) sarà la parte destra di \(c'\)
          \item Se la testina si muove a sinistra (condizione~\ref{eq:transizione-TM-3}, in cui \(N=L\)), laa parte a sinistra della testina in \(c^\prime\) conterrà tutti i simboli della parte sinistra in \(c\), tranne l'ultimo che diverrà il simbolo corrente di \(c^\prime\) e la parte destra di \(c^\prime\) sarà composta dal simbolo corrente in \(c\) seguito dalla sua parte destra
        \end{itemize}
  \item Le condizioni dal \ref{eq:transizione-TM-4} alla \ref{eq:transizione-TM-6} specificano l'evoluzione dei nastri di memoria in analogia con i precedenti casi. In particolare:
        \begin{itemize}
          \item La testina rimane ferma, condizione~\ref{eq:transizione-TM-4}
          \item La testina si muove a destra, condizione~\ref{eq:transizione-TM-5}
          \item La testina si muove a sinistra, condizione~\ref{eq:transizione-TM-6}
        \end{itemize}
  \item Le condizioni \ref{eq:transizione-TM-7} e \ref{eq:transizione-TM-8} specificano l'evoluzione del nastro di uscita
        \begin{itemize}
          \item Non è specificato il comportamento per \(N=L\) perché la testina del nastro di uscita si muove solo a destra
          \item Se la \TM non ha un nastro di uscita, queste condizioni non vanno considerate
        \end{itemize}
\end{itemize}

\subsection{Condizioni di accettazione di una \TM}

Sia \(M\) una \TM multinastro\
Una stringa \(x \in I^\ast\) è \textbf{accettata} da \(M\) se e solo se:
\[ c_0 = \langle q_0, \uparrow x, \uparrow Z_0, \ldots, \uparrow Z_0 \rangle \vdash^\ast_M \langle q, x^\prime \uparrow iy, \alpha_1 \uparrow A_1 \beta_1, \ldots, \alpha_k \uparrow A_k \beta_k \rangle \]
dove:
\begin{itemize}
  \item \(c_F\) è detta \textbf{configurazione finale}
  \item \(\vdash^\ast_M\) è la chiusura riflessiva e transitiva della relazione \(\vdash_M\)
  \item \(q \in F\), quindi lo stato \(q\) fa parte dell'insieme di stati finali
  \item Le testine possono trovarsi in un qualsiasi punto dei rispettivi nastri di memoria
\end{itemize}

\bigskip
Il linguaggio \textit{accettato} da \(M\) è definito come:
\[ L(M) = \{x \, | \, x \in I^\ast \text{ e } x \text{ è accettato da } M\} \]

\textit{Intuitivamente}, il linguaggio riconosciuto da una \TM \(M\) è composto da tutte e sole le stringhe che permettono di andare dallo stato iniziale a uno degli stati finali.
Poiché il nastro di ingresso è in grado di muoversi in entrambe le direzioni, non è richiesto che al termine dell'esecuzione la testina si trovi al termine della stringa di ingresso.

Una volta che \(M\) raggiunge uno stato finale, per definizione della funzione di transizione \(\delta\), non potrà più lasciarla e la computazione termina.

\subsection{Operazioni sulle \TM}

È possibile dimostrare che le \TM \textbf{sono chiuse} rispetto a:
\begin{itemize}
  \item \textbf{Intersezione}
  \item \textbf{Unione}
  \item \textbf{Concatenazione}
  \item \textbf{Stella di Kleene}
\end{itemize}

Perché una \TM può facilmente simulare due altre \TM (siano esse in \textit{serie} o \textit{parallelo}), si dimostra la chiusura rispetto alle prime due operazioni.
Di conseguenza, si dimostra la chiusura rispetto alle altre.

\bigskip
Analogamente, si dimostra che \textbf{non sono chiuse} rispetto a:
\begin{itemize}
  \item \textbf{Complemento}
  \item \textbf{Differenza},  conseguenza della non chiusura rispetto al complemento
\end{itemize}

La (non) chiusura rispetto a queste due operazioni è dovuta alla presenza di \textit{cicli} all'interno delle \TM.

Infatti, se essi non fossero presenti in una \TM sarebbe sufficiente definire l'insieme di stati di arresto e partizionarli in stati di di \textit{accettazione} e \textit{non accettazione}.
I problemi sorgono qualora una computazione non dovesse terminare (come visto in Sezione~\ref{sec:TM-non-deterministiche}).

\subsection{Proprietà delle \TM}

Di seguito sono enunciate alcune proprietà delle \TM:
\begin{enumerate}
  \item \label{enum:proprieta-1-tm} Ogni \TM è equivalente ad un'opportuna \TM dotata solo di \textbf{due stati non finali} e di \textbf{uno finale}
        \begin{itemize}
          \item può essere necessario accrescere l'alfabeto
          \item di conseguenza, sono sufficienti \(3\) stati per implementare qualsiasi algoritmo
        \end{itemize}
  \item \label{enum:proprieta-2-tm} Ogni \TM è equivalente ad un'opportuna \TM avente un alfabeto formato solo da \textbf{due simboli distinti}
        \begin{itemize}
          \item può essere necessario accrescere il numero di stati
          \item di conseguenza, sono sufficienti \(2\) simboli \textit{(più il simbolo \texttt{blank})} per implementare qualsiasi algoritmo
        \end{itemize}
\end{enumerate}

Le due proprietà impongono una scelta: è possibile ridurre gli stati di una \TM agendo sull'alfabeto (\ref{enum:proprieta-1-tm}) o viceversa ridurre la dimensione dell'alfabeto agendo sul numero di stati (\ref{enum:proprieta-2-tm}).
La scelta si riduce quindi ad un problema progettuale.

\subsection{\TM traduttrice}
\label{sec:TM-traduttrice}

Quando si definiscono le \TM con il nastro di uscita, esse diventano dei \textit{trasduttori}, ossia possono essere usate per tradurre stringhe di \(I^\ast\) in stringhe di \(O^\ast\).
In questo caso, le \TM vengono viste come le tuple di \(9\) elementi viste in Sezione~\ref{sec:definizione-formale-TM}.

\subsubsection{Traduzione tramite \TM}

Una \TM multinastro \(M\) definisce una \textbf{traduzione} \(\tau_M: I^\ast \rightarrow O^\ast\) secondo la regola seguente:
\[ \tau_M(x) = y \textit{ se e solo se } \langle q_0, \uparrow x, \uparrow Z_0, \ldots, \uparrow Z_0, \uparrow \blank \rangle \vdash^\ast_M \langle q, x^\prime \uparrow iy, \alpha_1 \uparrow A_1 \beta_1, \ldots, \alpha_k \uparrow A_k \beta_k, y \uparrow o \rangle \]
con \(q \in F\).

In generale, una \TM \(M\) definisce una \textbf{traduzione parziale} \(\tau_M: I^\ast \rightarrow O^\ast\).
Infatti, \(\tau_M\) è indefinita se:
\begin{enumerate}
  \item \(M\) raggiunge una configurazione di arresto il cui stato non appartiene a \(F\)
  \item \(M\) non si ferma mai quando opera su \(x\)
\end{enumerate}

\bigskip
\textit{Intuitivamente}, una stringa \(x\) viene tradotta in una stringa \(y\) da una \TM \(M\) se esiste un cammino che parte da una configurazione \textbf{iniziale} con \(x\) sul nastro di ingresso e termina in una configurazione \textbf{finale} con \(y\) sul nastro di uscita.

\subsection{Confronto di \TM con altre macchine}

Si vuole ora confrontare la classe di macchine computazionali appartenente alle \TM con altri tipi di macchine.

\subsubsection{\TM vs \PDA}

Come già visto, i linguaggi \(a^n b^n c^n\) e \(a^n b^n \cup a^n b^{2n}\) non possono essere riconosciuti da un \PDA (Sezione~\ref{par:conseguenza-negativa-pumping-lemma}) mentre tramite \TM il riconoscimento funziona.
Ogni linguaggio riconoscibile da un \PDA può essere riconosciuto da una \TM: si può sempre costruire una \TM che usa un nastro di memoria come se fosse una pila.

I linguaggi accettati dalle \TM sono detti \textbf{ricorsivamente enumerabili}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.6]{image-5.tikz}
  \caption{Relazione tra linguaggi}
  \label{fig:relazione-linguaggi}
  \bigskip
\end{figure}

\subsubsection{\TM vs macchine di \textit{Von Neumann}}

Le \TM possono simulare una macchina di \textit{Vonn Neumann} (in Inglese \VNM), un modello astratto di computer.
La principale differenza avviene nell'accesso alla memoria: mentre nelle \TM è \textit{sequenziale}, nelle \VNM è \textit{diretto}.

Tuttavia, il metodo di accesso alla memoria non influenza il potere espressivo di una macchina: non cambia la classe di problemi risolvibili con essa, ma può cambiarne la complessità.
Infatti si tramite \TM si possono anche calcolare funzioni ed eseguire algoritmi \textit{sebbene implementare questo tipo di operazioni possa essere estremamente complicato}.

\bigskip
In conclusione, la \TM è un modello più astratto di computer con accesso \textbf{sequenziale} al suo spazio di memoria.

\subsection{Memoria delle \TM}

Esistono \TM con diversi modelli di memoria:

\begin{itemize}
  \item A nastro \textbf{singolo} (Figura~\ref{fig:TM-a-nastro-singolo}):
        \begin{itemize}
          \item Normalmente è \textbf{illimitato} in entrambe le direzioni
          \item Serve da contemporaneamente da \textbf{ingresso, uscita e memoria}
          \item È il modello più simile alla macchina originalmente ideata da \textit{Alan Turing}
        \end{itemize}
  \item A nastro \textbf{bidimensionale} (Figura~\ref{fig:TM-bidimensionale}):
        \begin{itemize}
          \item È presente \textbf{una testina per ogni dimensione}
          \item Può essere generalizzato a un numero \textit{arbitrario} di dimensioni
        \end{itemize}
\end{itemize}

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{minipage}[b]{.45\textwidth}
    \centering
    \tikzfig[0.5]{image-6.tikz}
    \caption{\TM a nastro singolo}
    \label{fig:TM-a-nastro-singolo}
  \end{minipage}
  \begin{minipage}[b]{.45\textwidth}
    \centering
    \tikzfig[0.5]{image-7.tikz}
    \caption{\TM bidimensionale}
    \label{fig:TM-bidimensionale}
  \end{minipage}
  \bigskip
\end{figure}

Entrambi i modelli di \TM sono \textbf{equivalenti}, poiché riconoscono la stessa classe di linguaggi.
A tal proposito, si consulti la Figura~\ref{fig:configurazione-MT-nastro-singolo}, che mostra la configurazione della \TM a nastro singolo \(M^\prime\):

\begin{itemize}
  \item \(c(T_x)^\prime\) rappresenta il contenuto non vuoto del nastro \(T_x\) di \(M\) alla sinistra della testina di \(T_x\)
  \item \(c(T_x)^{\prime\prime}\) rappresenta il contenuto dello stesso nastro alla destra della testina, compreso il carattere sotto di essa
  \item \(\ast\) e \(\$\) sono simboli che non appartengono a \(I \cup \Gamma \cup O\) e vengono usati per marcare i limiti fra il contenuto dei diversi nastri e posizioni della testina (rispettivamente)
  \item \(\overline{q}\) è un'opportuna codifica dello stato \(M\)
\end{itemize}

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.8]{image-8.tikz}
  \caption{Equivalenza tra \TM multinastro e \TM a nastro singolo}
  \label{fig:configurazione-MT-nastro-singolo}
  \bigskip
\end{figure}

\clearpage

\section{Modelli operazionali non deterministici}
\label{sec:non-determinismo}

Tutti i modelli considerati fino ad ora (\FSA, \PDA, \TM, \ldots) sono \textbf{deterministici}: una volta fissato uno \textit{stato iniziale} e un \textit{ingresso}, la loro evoluzione è \textit{univocamente determinata}.
In certe situazioni, però, il modello che si desidera modellare non può essere descritto in modo così deterministico, in quanto l'osservatore non possiede informazioni sufficientemente accurate da consentirgli di prevederne l'esatta evoluzione per ogni configurazione.

Per esempio, si consideri la serie di istruzioni:

\begin{verbatim}
  if x >= y then:
    max := x

  if y >= x then:
    max := y
\end{verbatim}

Essa definisce il massimo tra due numeri \texttt{a} e \texttt{b}.
Tale notazione rappresenta una specifica non deterministica: non viene illustrato il comportamento per \texttt{a == b} e l'applicazione di entrambe le regole risulta in un output valido.

Normalmente, nei linguaggi di programmazione, si usa una precedenza di tipo lessicale: viene applicata la prima conzidione scelta.

Un'esempio analogo può essere fatto sfruttando gli algoritmi di \textit{ricerca binaria}.
Essi infatti compiono una \textit{simulazione} di algoritmi non deterministici:

\begin{verbatim}
while true:
  l'elemento ricercato è nella radice?
    termina

  cerca nel sotto albero sinistro
  cerca nel sotto albero destro
\end{verbatim}

Siccome non è possibile determinare a priori quale dei due cammini \textit{(sinistro o destro)} sia migliore, la scelta della priorità nei cammini è spesso \textbf{arbitraria}.
Alternativamente, non viene fatta una scelta del cammino da intraprendere ma entrambi venengono esplorati in \textbf{parallelo}.

In entrambi i casi, grazie al non determinismo, è possibile implementare l'algoritmo senza la necessità di usare \textit{backtracking}, che sarebbe invece obbligatorio nel caso si cercasse di usare un sistema deterministico.

\bigskip
Il \textbf{non determinismo} (\ND per brevità) è quindi un modello di computazione.
Viene spesso sfruttato nei linguaggi di programmazione per consentire la computazione parallela.

Il \ND viene applicato a vari modelli computazionali, inclusi tutti quelli visti fin'ora.

\subsection{\FSA non deterministici}
\label{sec:FSA-non-deterministici}

Un \FSA non deterministico (\NFA) è una tupla di \(5\) elementi \(\langle Q, I, \delta, q_0, F \rangle\):

\begin{itemize}
  \item \(Q, I, q_0, F\) sono definiti come in un \FSA (Sezione~\ref{sec:definizione-formale-FSA})
  \item \(\delta\) è la \textbf{funzione di transizione}:
        \begin{itemize}
          \item \(\delta: Q \times I \rightarrow \wp(Q)\)
                \begin{itemize}[label=\(\rightarrow\)]
                  \item \(\wp(Q)\) rappresenta l'insieme delle parti di \(Q\)
                  \item gli elementi di \(\wp(Q)\) sono insiemi di stati
                \end{itemize}
          \item \(\delta^\ast\) è definito induttivamente a partire da \(\delta\):
                \begin{enumerate}
                  \item \(\delta^\ast(q, \epsilon) = \left\{q\right\}\)
                  \item \(\displaystyle \delta^\ast(q, y \cdot i) = \bigcup_{q^\prime \in \delta^\ast (q, y)} \delta (q^\prime, i)\)
                        \begin{itemize}[label=\(\rightarrow\)]
                          \item \(i\) è l'ultimo carattere della stringa di ingresso
                        \end{itemize}
                \end{enumerate}
        \end{itemize}
\end{itemize}

\bigskip
Nel caso di \FSA accettori con \ND, si dice che \(x \in I^\ast\) è \textbf{accettata} da un \NFA \(\langle Q, I, \delta, q_0, F \rangle\) se e solo se \(\delta^\ast (q_0, x) \cap F \neq \emptyset\).

\textit{Informalmente}: tra le varie possibili esecuzioni (a parità di ingresso) dell'\NFA, è sufficiente che una di esse vada a buon fine \textit{(la stella di Kleene delle transizioni \(\delta^\ast\) ha almeno uno stato che fa parte dell'insieme di stati finali)}.

Questa definizione prende il nome di \textbf{non determinismo esistenziale}, che si oppone al non determinsimo \textbf{universale}: \(\delta^\ast(q_0, x) \subseteq F\).

\bigskip

Gli \NFA non sono più potenti degli \FSA, ma presentano due grossi vantaggi:
\begin{itemize}
  \item Può essere più semplice progettare un \NFA
  \item Il numero di stati di un \NFA può essere esponenzialmente più minore dell'analogo \FSA
\end{itemize}
Per maggiori dettagli, si consulti la Sezione~\ref{sec:DFA-vs-NFA}

\subsubsection{Esempio di un \NFA}

In Figura~\ref{fig:esempio-NFA} è mostrato un esempio di \NFA.
In esso, infatti, \(\delta(q_0, a) = \left\{q_1, q_2\right\}\) e quindi la funzione di transizione \textbf{non è univocamente definita}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, on grid, node distance=3cm, >=Triangle]
    \node [state](q0) {\(q_0\)};
    \node [state](q1) [below left=of q0] {\(q_1\)};
    \node [state](q2) [below right=of q0] {\(q_2\)};

    \path[->, thick]
    (q0) edge [bend right] node [above=0.2cm] {\(a\)} (q1)
    (q0) edge [bend left] node [above=0.2cm] {\(a\)} (q2);

  \end{tikzpicture}

  \caption{Esempio di un \NFA}
  \label{fig:esempio-NFA}
  \bigskip
\end{figure}

\subsubsection{\DFA vs \NFA}
\label{sec:DFA-vs-NFA}

Si osservi il \NFA in Figura~\ref{fig:DFA-e-NFA}.
Partendo da \(q_0\) e leggendo \texttt{ab}, l'automa raggiunge uno stato che appartiene all'insieme \(\left\{q_3, q_4, q_5\right\}\).
Negli \NFA viene ancora chiamato \textbf{stato} l'insieme dei possibile stati in cui esso può trovarsi durante l'esecuzione.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, on grid, node distance=3cm, >=Triangle]
    \node [state, initial, initial where=above, initial text=](q0) {\(q_0\)};
    \node [state](q1) [below left=of q0] {\(q_1\)};
    \node [state](q2) [below right=of q0] {\(q_2\)};
    \node [state](q3) [draw=red, text=red, below left=of q1] {\(q_3\)};
    \node [state](q3) [draw=red, text=red, below left=of q1] {\(q_3\)};
    \node [state](q4) [draw=red, text=red, below left=of q2, below right=of q1] {\(q_4\)};
    \node [state](q5) [draw=red, text=red, below right=of q2] {\(q_5\)};

    \path[->, thick]
    (q0) edge [] node [above=0.2cm] {\(a\)} (q1)
    (q0) edge [] node [above=0.2cm] {\(a\)} (q2)
    (q1) edge [] node [above=0.2cm] {\(b\)} (q3)
    (q1) edge [] node [above=0.2cm] {\(b\)} (q4)
    (q2) edge [] node [above=0.2cm] {\(b\)} (q4)
    (q2) edge [] node [above=0.2cm] {\(b\)} (q5);

  \end{tikzpicture}

  \caption{\DFA e \NFA}
  \label{fig:DFA-e-NFA}
  \bigskip
\end{figure}

\bigskip
\textit{Formalmente}, \NFA e \DFA \textbf{hanno lo stesso potere espressivo.}
Dato un \NFA, \(A\), è possibile costruire un \DFA \(A_D\) che accetti il medesimo linguaggio.

\bigskip
Sia \(A = \langle Q, I, \delta, q_0, F \rangle, \ \delta: Q \times I \rightarrow \wp(Q)\).
Si definisca \(A_D = \langle Q_D, I, \delta_D, q_{0D}, F_D \rangle\) con:

\begin{itemize}
  \item \(Q_D = \wp(Q)\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item l'insieme dei suoi stati è uguale all'insieme delle parti degli stati di \(A\)
        \end{itemize}
  \item \(\displaystyle \delta_D(q_D, i) = \bigcup_{q \in q_D} \delta(q, i) \)
        \begin{itemize}[label=\(\rightarrow\)]
          \item se un insieme di stati è raggiungibile a partire da uno stato del \NFA, allora tale relazione viene preservata nel \DFA sfruttando la costruzione degli stati come insiemi di stati del \NFA
        \end{itemize}
  \item \(F_D = \left\{q_D \, | \, q_D \in Q_D \land F \cap q_D \neq \emptyset \right\}\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item l'insieme dei suoi stati finali è dato dall'insieme di stati finali di \(A\) raggiungibili senza \ND
        \end{itemize}
  \item \(q_{0d} = \left\{q_0\right\}\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item il suo stato iniziale è uguale allo stato iniziale di \(A\)
        \end{itemize}
\end{itemize}

Gli \NFA non sono quindi più potenti dei corrispondenti \DFA ma sono di dimensione ridotta.
Il precedente teorema, infatti, produce un insieme di stati di cardinalità \(2^n\) partendo da \(n\) stati di partenza del \NFA.
Inoltre, la formalizzazione più naturale di un problema è quella descritta mediante un \NFA.

\subsubsection{Conversione da \NFA a \DFA}

\textit{Operativamente}, per convertire un \FSA in un \DFA bisogna:

\begin{enumerate}
  \item Creare l'insieme vuoto di stati del \DFA \(Q^\prime\)
  \item Creare la tabella di transizione del \DFA \(T^\prime\)
  \item Aggiungere lo stato iniziale del del \NFA a \(Q^\prime\)
  \item Aggiungere le transizioni dello stato iniziale alla tabella di transizione \(T^\prime\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item se lo stato di partenza porta a più stati per un certo input, allora essi vanno trattati come un singolo stato del \DFA
        \end{itemize}
  \item \label{enum:loop-in-NFA-to-DFA} Se un nuovo stato è presente in \(T^\prime\):
        \begin{itemize}[label=\(\rightarrow\)]
          \item Aggiungere il nuovo stato a \(Q^\prime\)
          \item Aggiungere le sue transizioni a \(T^\prime\)
        \end{itemize}
  \item Ripetere il passo~\ref{enum:loop-in-NFA-to-DFA} finché non vengono più aggiunti nuovi stati a \(T^\prime\)
  \item \(T^\prime\) è ora la tabella di transizione completa del \DFA ricercato
\end{enumerate}

\subsection{\PDA non deterministici}
\label{sec:definzione-NPDA}

Un \PDA non deterministico (\NPDA) è una tupla di \(7\) elementi \(\langle Q, I, \Gamma, \delta, q_0, Z_0, F \rangle\):

\begin{itemize}
  \item \(Q, I, \gamma, q_0, Z_0, F\) sono definiti come in un \PDA (Sezione~\ref{sec:definizione-formale-PDA})
  \item \(\delta\) è la \textbf{funzione di transizione}:
        \begin{itemize}
          \item \(\delta: Q \times \left(I \cup \left\{\epsilon\right\}\right) \times \Gamma \rightarrow \wp_F (Q \times \Gamma^\ast)\)
          \item \(\wp_F\) indica l'insieme delle parti \textbf{finito} dell'insieme \(Q\)
                \begin{itemize}
                  \item[\(\rightarrow\)] \(\Gamma^\ast\) è un insieme \textbf{infinito}
                \end{itemize}
        \end{itemize}
\end{itemize}

Inoltre, la relazione \(\vdash\) su \(Q \times I^\ast \times \Gamma^\ast\) è definita da \(\langle q, x, \gamma \rangle \vdash \langle q^\prime, x^\prime, \gamma^\prime \rangle\) se e sono se è valida una delle due condizioni mutualmente esclusive:

\begin{gather}
  x = ay,\quad x^\prime = y,\quad \gamma = a \beta,\quad \gamma^\prime = \alpha\beta,\quad \langle q^\prime, \alpha \rangle \in \delta(q, a, A) \\
  x = x^\prime,\quad \gamma = a \beta,\quad \gamma^\prime = \alpha\beta,\quad \langle q^\prime, \alpha \rangle \in \delta(q, \epsilon, A)
\end{gather}

\bigskip
La stringa \(x \in I^\ast\) è accettata dall'automa se e solo se:
\[ \langle q_0, x, Z_0 \rangle \vdash^\ast \langle q, \epsilon, \gamma \rangle, \quad q \in F, \gamma \in \Gamma^\ast \]
\textit{Informalmente}, una stringa è accettata da un \NPDA se esiste un cammino coerente con \(x\) che va dallo stato iniziale a uno stato finale quando essa viene letta \textbf{interamente}.

\bigskip
I \PDA, tuttavia, sono intrinsecamente non deterministici.
Nella loro definizione era infatti stato aggiunto il vincolo per cui:
\[ \delta(q, \epsilon, A) \neq \bot \Rightarrow \delta(q, I, A)=\bot, \ \forall i \in I \]
\textit{Informalmente}, se una transizione da uno stato è  una \(\epsilon\)-mossa, allora la stessa transizione non può essere definita per nessun altro ingresso.

Rimuovere questo vincolo, infatti, priverebbe i \PDA del loro non determinismo.
Analogamente è possibile avere non determinismo cambiando la funzione di transizione di \PDA, modificando al contempo la transizioni tra configurazioni e la condizione di accettazione.

\subsubsection{Chiusura delle \NPDA}

Come per gli altri formalismi con \ND, è possibile dimostrare che un \NPDA può riconoscere ogni linguaggio riconoscibile tramite \DPDA.
Tuttavia, siccome gli \NPDA sono più potenti dei \DPDA, la classe di linguaggi riconosciuti dai primi è più ampia di quella riconosciuta dai secondi e di conseguenza non è scontato che valgano le stesse proprietà di chiusura.

Gli \NPDA, infatti, sono chiusi rispetto all'unione.
È possibile costruire un \NPDA che è collegato con due \(\epsilon\)-mosse agli stati iniziali di due \DPDA, così mostrando una chiusura rispetto all'unione (come mostrato nella Figura~\ref{fig:intersezione-PDA-tramite-NPDA}).
Tuttavia, non rimanendo chiusi rispetto all'intersezione, non possono \textit{(e non sono)} essere chiusi rispetto al complemento.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, node distance=3cm, >=Triangle]
    \node [state, initial above, initial text=, minimum size=1cm](initial) {\(q_F\)};
    \node [state, minimum size=1cm](0)  [below left=of initial] {\(q_0\)};
    \node [state, minimum size=1cm](1)  [below right=of initial] {\(q_1\)};

    \path[->, thick]
    (initial) edge [bend right] node [above=0.2cm] {\(\epsilon\)} (0)
    (initial) edge [bend left] node [above=0.2cm] {\(\epsilon\)} (1);
  \end{tikzpicture}

  \caption{Intersezione di \PDA tramite \NPDA}
  \label{fig:intersezione-PDA-tramite-NPDA}
  \bigskip
\end{figure}

\subsubsection{Complemento e \ND}

Se una macchina è deterministica e la sua computazione termina, il \textbf{complemento} può essere ottenuto:
\begin{enumerate}
  \item \textbf{Completando} la macchina
  \item \textbf{Scambiando gli stati} di accettazione con quelli di non accettazione
\end{enumerate}

Tuttavia, il non determinismo \textit{(analogamente alla non terminazione)} rende questo approccio inapplicabile.
Come nei \DPDA, le computazioni nei \NPDA possono sempre essere fatte terminare.

Tuttavia, si possono avere due computazioni del tipo:
\begin{align*}
  c_0 = \langle q_0, x, Z_0 \rangle & \vdash^\ast c_1 = \langle q_1, \epsilon, \gamma \rangle \\
  c_0 = \langle q_0, x, Z_0 \rangle & \vdash^\ast c_2 = \langle q_2, \epsilon, \gamma \rangle
\end{align*}
Con:
\[ q_1 \in F \quad q_2 \notin F \]

Con questa configurazione, scambiando stati di accettazione e non, \(x\) verrebbe comunque accettato (e di conseguenza il complemento non sarebbe valido).

\subsubsection{Conseguenze della chiusura delle \NPDA}

Grazie a questa proprietà degli \NPDA è possibile riconoscere il linguaggio generato dall'unione di linguaggi \(\left\{a^n b^n \, | \, n \geq 1\right\} \cup \left\{a^n b^{2n} \, | \, n \geq 1\right\}\).

\subsubsection{Linguaggi riconosciuti dalle \NPDA}

I linguaggi riconosciuti da \NPDA prendono il nome di \textbf{linguaggi non contestuali} (o \textbf{context-free}).

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.8]{image-10.tikz}
  \caption{Linguaggi non contestuali}
  \label{fig:linguaggi-non-contestuali}
  \bigskip
\end{figure}

\subsection{\TM non deterministiche}
\label{sec:TM-non-deterministiche}

Un \TM deterministico (\NTM) è un tupla di \(9\) elementi \(\langle Q, I, \Gamma, O, \delta, \eta, q_0, Z_0, F \rangle\):

\begin{itemize}
  \item \(Q, I, \Gamma, O, q_0, Z_0, F\) sono definiti come in una \TM (Sezione~\ref{sec:definizione-formale-TM})
  \item \(\delta\) è la \textbf{funzione di transizione}:
        \begin{itemize}
          \item \(\delta: (Q - F) \times I \times \Gamma^k \rightarrow \wp\left(Q \times \Gamma^k \times \left\{R, L, S\right\}^{k+1} \times \left\{R, S \right\} \right)\)
        \end{itemize}
  \item \(\eta\) è la \textbf{funzione di uscita}:
        \begin{itemize}
          \item \(\eta : (Q - F) \times I \times \Gamma^k \rightarrow \wp\left(O \times \left\{R, S\right\}\right)\)
        \end{itemize}
\end{itemize}

\bigskip
Contrariamente a quanto avvenuto per la definizione delle \NPDA (Sezione~\ref{sec:definzione-NPDA}), non è necessario specificare che nella definizione \(\delta\) l'insieme delle parti sia finito.
Ciò è dovuto al fatto che l'insieme \(Q \times \Gamma^k \times \left\{R, L, S\right\}^{k+1} \times \left\{R, S \right\}\) è finito, poiché costruito dal prodotto cartesiano di insiemi finiti.

\subsubsection{Albero di computazione di una \NTM}

Per come è definita la funzione di transizione di \(M\), se si considera una computazione di \(M\) su una stringa in ingresso, essa è ben descritta da un albero di configurazioni in cui è inserita ogni configurazione raggiungibile dalla configurazione iniziale.
Una parola viene accettata se esiste almeno un cammino che termina in una configurazione finale.
Un esempio di albero di configurazione per una \NTM è mostrato in Figura~\ref{fig:albero-configurazioni-NTM}, dove:

\begin{itemize}
  \item I cerchi indicano configurazioni di accettazione
  \item I rettangoli indicano configurazioni di \texttt{halt} ma non di accettazione
  \item Le linee tratteggiate configurazione che non terminano
\end{itemize}

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.8]{image-9.tikz}
  \caption{Albero delle configurazioni di una \NTM}
  \label{fig:albero-configurazioni-NTM}
  \bigskip
\end{figure}

\subsubsection{Accettazione di una stringa da un \NTM}

Una stringa \(x \in I^\ast\) è accettata da una \NTM se e solo se esiste una computazione che termina in uno stato di accettazione.
Il problema dell'accettazione di una stringa si pu\`o ridurre quindi alla visita di un albero di computazione.

Per cercare la via verso uno stato di accettazione, si riconoscono due tipi di approccio:

\begin{itemize}
  \item Visita in \textit{"profondità"}, detta \textbf{depth first}
        \begin{itemize}
          \item Un percorso viene seguito fino al suo termine
          \item Quando viene raggiunta la fine, un'altro ramo viene selezionato ed esplorato tramite \textit{backtracking}
        \end{itemize}
  \item Visita in \textit{"ampiezza"}, detta \textbf{breadth first}
        \begin{itemize}
          \item Si crea una coda di nodi da visitare
          \item Ogni volta che un nuovo nodo viene trovato, i suoi successori si aggiungono alla fine della coda
        \end{itemize}
\end{itemize}

Si noti tuttavia che se parola non viene accettata, una \TM può entrare in uno ciclo infinito e quindi non terminare mai la sua computazione.
Questo comportamento può essere causato dalla presenza di rami di lunghezza infinita all'interno dell'albero delle configurazioni.
Una ricerca di tipo \textbf{depth first} non può funzionare per questo tipo di problema, perché finirebbe con molta probabilità in un ciclo infinito.

Una macchina deterministica può simulare l'attraversamento di un albero tramite una ricerca \textbf{breadth first}.

\subsubsection{\DTM vs \NTM}

È possibile costruire una \DTM che visita un'albero di computazione costruito da una \NTM livello dopo livello, implementando una ricerca di tipo \textbf{breadth first}.

Quindi, data una \NTM, è possibile costruire:

\begin{itemize}
  \item Una \DTM analoga che determina se la \NTM riconosce una stringa \(x\)
  \item La sua equivalente \DTM
\end{itemize}

Infine, può essere dimostrato che il \ND \textbf{non aggiunge potere espressivo} alle \TM.

\subsubsection{\NPDA vs \NTM}
% slide 31 - 1-06ND.pdf

Nella Figura~\ref{fig:confronto-NPDA-NTM} è possibile osservare varie possibilità di relazione confronto tra \NPDA e \NTM, specialmente nei casi:

\begin{enumerate}[label=\alph*), ref=(\alph*)]
  \item \label{enum:NTM-or-NPDA} \(\NTM \cup\NPDA \neq \emptyset\), quindi \NTM e \NPDA possono riconoscere dei linguaggi in comune
  \item \label{enum:NPDA-in-NTM} \(\NPDA \subseteq \NTM\), quindi le \NTM rappresentano una sottocategoria delle \NPDA
  \item \label{enum:NTM-in-NPDA} \(\NTM \subseteq \NPDA\), quindi le \NPDA rappresentano una sottocategoria delle \NTM
  \item \label{enum:NTM-equiv-NPDA} \(\NTM \equiv \NPDA\), qundi le due categorie coincidono
\end{enumerate}

Tuttavia, si dimostra che i casi:

\begin{itemize}
  \item \ref{enum:NTM-or-NPDA}, \ref{enum:NTM-in-NPDA} sono \textbf{falsi} perché una \NTM può simulare un \NPDA usando il nastro come pila
  \item \ref{enum:NTM-equiv-NPDA} è \textbf{falso} perché la pila \'e una memoria distruttiva, al contrario del nastro
\end{itemize}

Quindi rimane vero il caso \ref{enum:NTM-in-NPDA} e le \NTM hanno potere espressivo superiore.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-13.tikz}
  \caption{Confronto tra \NPDA e \NTM}
  \label{fig:confronto-NPDA-NTM}
  \bigskip
\end{figure}

\clearpage

\section{Grammatiche}
\label{sec:grammatiche}

Fino ad ora, gli automi sono stati usati come modelli astratti nei problemi di riconoscimento dei linguaggi.
Dato un \textit{riconoscitore} \(A\) e il suo linguaggio di ingresso \(I\), il linguaggio da esso definito è:
\[ L(A) = \left\{x \, | \, x \in I^\ast, \, x \text{ è accettata da } A \right\} \]

Tuttavia, esistono altri tipi di formalismi per descrivere un linguaggio: le \textbf{grammatiche formali}.
Essi sono costituiti da un \textbf{insieme di regole} (sono quindi di tipo \textit{generativo}) che costruiscono le frasi di un linguaggio.
Una grammatica formale genera stringhe di un linguaggio attraverso un processo detto di \textit{riscrittura}.
Esso è costituito da un insieme di tecniche per sostituire sottotermini di una formula con altri termini.
Non si applica solo ai linguaggi in senso naturale ma a un'ampia gamma di contesti.

\bigskip
\textit{Esempi} di riscrittura:

\begin{itemize}
  \item \(A \land B\) viene riscritto come \(\lnot (\lnot A \lor \lnot B)\)
  \item \(\lnot A \lor B\) viene riscritto come \(A \rightarrow B\)
\end{itemize}

\bigskip
In generale, un meccanismo di riscrittura è un insieme di \textit{regole linguistiche} che descrivono l'\textit{oggetto principale} (come la frase) come sequenza di \textit{componenti}.
Ogni componente può essere poi \textit{"raffinato"} da oggetti più dettagliati e così via, fino a ottenere una sequenza di \textit{oggetti elementari}.

Una grammatica è quindi composta da:

\begin{itemize}
  \item Oggetto principale, detto \textbf{simbolo iniziale}
  \item Inseme di componenti da sostituire durante il processo di derivazione, detti \textbf{simboli non terminali}
  \item Insieme di elementi di base, detti \textbf{simboli terminali}
  \item Regole di sostituzione, dette \textbf{produzioni}
\end{itemize}

\bigskip

\textit{Noam Chomsky}, uno degli studiosi più importanti delle grammatiche formali, all'interno del libro \inlinequote{On Certain Formal Properties of Grammars, Information and Control} afferma che:

\indentquote{A grammar can be regarded as a device that enumerates the sentences of a language}

\indentquote{A grammar of L can be regarded as a function whose range is exactly L}

\subsection{Definizione formale di grammatica}

Una \textbf{grammatica} (detta anche \textit{grammatica non ristretta}) \(G\) è una tupla di \(4\) elementi \(\langle V_T, V_N, P, S \rangle\):

\begin{itemize}
  \item \(V_T\) è un insieme finito di \textit{simboli terminali}, detto \textbf{alfabeto terminale}
        \begin{itemize}[label=\(\rightarrow\)]
          \item gli elementi di \(V_T\) sono normalmente scritti in \textbf{minuscolo}
        \end{itemize}
  \item \(V_N\) è un insieme finito di \textit{simboli non terminali}, detto \textbf{alfabeto non terminale}
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(V_N\) è costruito in modo che \(V_T \cap V_N = \emptyset\)
          \item l'alfabeto \(V\) è quindi dato da \(V_T \cup V_N = V_T \oplus V_N\)
          \item gli elementi di \(V_N\) sono normalmente scritti in \textbf{maiuscolo}
        \end{itemize}
  \item \(P\) è un insieme finito, detto \textbf{insieme delle produzioni} di \(G\)
        \begin{itemize}
          \item \(P \subseteq V^\ast \cdot V_N^+ \cdot V^\ast \times V^\ast\)
          \item un elemento \(p = \langle \alpha, \beta \rangle\) di \(P\) verrà indicato con \(\alpha \rightarrow \beta\)
          \item la stringa \(\alpha\) è detta \textbf{parte sinistra} di \(p\)
          \item la stringa \(\beta\) è detta \textbf{parte destra} di \(p\)
          \item due regole \(s \rightarrow d_1, s \rightarrow d_2\) con la stessa parte sinistra si possono essere accorpate come \(s \rightarrow d_1 \, | \, d_2\)
        \end{itemize}
  \item \(S\) è un elemento \textit{"particolare"} di \(V_N\), detto \textbf{assioma} o \textbf{simbolo iniziale}
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(S\) non può essere mai parte destra di una derivazione
        \end{itemize}
\end{itemize}

\subsubsection{Relazione di derivazione immediata}

Data una grammatica \(G\), si definisce su \(V^\ast\) la relazione binaria di \textbf{derivazione immediata}, indicata dalla notazione
\[ \alpha \xRightarrow[G]{} \beta \]
è definita se e solo se
\begin{gather*}
  \alpha = \alpha_1 \gamma \alpha_2, \quad \beta = \alpha_1 \delta \alpha_2 \\
  \alpha_1, \alpha_2, \delta \in V^\ast,\quad \gamma \in V_N^+, \quad \gamma \rightarrow \delta \in P
\end{gather*}
Il simbolo \(G\) in \(\xRightarrow[G]{}\) viene normalmente omesso qualora il contesto sia univocamente definito, indicando quindi le derivazioni come \(\xRightarrow{}\).

Come già visto con le operazioni tra linguaggi, (Sezione~\ref{sec:operazioni-linguaggi}), \(\xRightarrow[G]{\ast}, \xRightarrow[G]{+}, \xRightarrow[G]{n}\) indicano rispettivamente la chiusura riflessiva e transitiva, la chiusura transitiva e la potenza \(n\) di \(\xRightarrow[G]{}\).

\subsubsection{Linguaggio di accettazione di una grammatica}

Una grammatica \(G = \langle V_N, V_T, P, S \rangle\) genera un linguaggio \(L(G)\) sull'alfabeto \(V_T\).
Esso è definito come:
\[ L(G) = \left\{x \, | \, S \xRightarrow[G]{\ast} X, x \in V_T^\ast\right\} \]
\textit{Informalmente}, il linguaggio generato da una grammatica è quindi costruito da tutte le \textit{e sole} stringhe di \textbf{soli simboli terminali} che possono essere generate a partire dall'\textbf{assioma} \(S\) applicando un numero qualsiasi di sostituzioni (\textit{passi}).
Notare che gli elementi non terminali di \(G\) non fanno parte della stringa da essa generata.

Le regole non sono univoche: più sostituzioni diverse possono essere applicate alla stessa stringa di partenza.
Il processo di derivazione è quindi intrinsecamente non deterministico.
Inoltre, alcuni cammini di derivazione possono portare a sequenze di terminali e non terminali, generando quindi stringhe non valide.

\subsection{Gerarchia di Chomsky}
\label{sec:gerarchia-di-Chomsky}

La definizione delle grammatiche (come vista in questi appunti) è opera principalmente di \textit{Noam Chomsky}, linguista e filosofo statunitense.
Egli ha infatti introdotto una loro classificazione, nota come \textbf{gerarchia di Chomsky} (il cui diagramma è osservabile in Figura~\ref{fig:gerarchia-Chomsky}).
In Tabella~\ref{tab:gerarchia-Chomsky} sono mostrate alcune caratteristiche di ciascuna grammatica.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.8]{image-11.tikz}
  \caption{Gerarchia di Chomsky}
  \label{fig:gerarchia-Chomsky}
  \bigskip
\end{figure}

\begin{table}[htbp]
  \bigskip
  \centering
  \begin{tabular}{c|c|c|c}
    \textit{tipo} & \textit{grammatica e linguaggio accettato} & \textit{automatismo}              & \textit{forma regole}                            \\ \hline
    \(0\)         & non ristretta - \GG                        & \TM                               & \textit{tutte}                                   \\
    \(1\)         & dipendente dal contesto                    & \textit{linear bounded automaton} & \(\alpha A \beta \rightarrow \alpha\gamma\beta\) \\
    \(2\)         & non contestuale - \CFG                     & \NPDA                             & \(A \rightarrow \gamma\)                         \\
    \(3\)         & regolare - \RG                             & \FSA                              & \(X \rightarrow a\) o \(X \rightarrow aY\)       \\
  \end{tabular}
  \bigskip
  \caption{Gerarchia di Chomsky}
  \label{tab:gerarchia-Chomsky}
\end{table}

% un breakable list
\begin{minipage}{0.95\textwidth}
  La gerarchia è composta dalle grammatiche così come segue:

  \bigskip
  \begin{enumerate}[start=0, label=Tipo \arabic*:]
    \item Include tutte le grammatiche formali
    \item Hanno regole in forma \(\alpha A \beta \rightarrow \alpha\gamma\beta\)
          \begin{itemize}
            \item \(A\) è un non terminale
            \item \(\alpha, \beta, \gamma\) sono stringhe di terminali e non terminali
            \item \(\gamma\) è non vuota
            \item il contesto viene preservato \textit{(solo il non terminale viene sostituito)}
            \item la regola \(S \rightarrow \epsilon\) è consentita se \(S\) non appare a destra in alcuna regola
          \end{itemize}
    \item Hanno regole in forma \(A \rightarrow \gamma\)
          \begin{itemize}
            \item \(A\) è un non terminale
            \item \(\gamma\) è una stringa di terminali e non terminali
          \end{itemize}
    \item Hanno regole in forma \(X \rightarrow a\) e (\(X \rightarrow aY\) o \(X \rightarrow Ya\))
          \begin{itemize}
            \item \(X, Y\) sono non terminali
            \item \(a\) è un terminale
            \item \(a\) può essere seguito (o preceduto) da un terminale \textit{(le due possibilità sono mutualmente esclusive)}
            \item La regola \(S \rightarrow \epsilon\) è consentita se \(S\) non appare a destra in alcuna regola
          \end{itemize}
  \end{enumerate}
  \bigskip
\end{minipage}

Le grammatiche di tipo \(3\) sono le \textbf{meno potenti}, mentre quelle di tipo \(0\) sono le \textbf{più potenti}.

\subsubsection{Grammatiche lineari}

Sia \(G = \langle V_T, V_N, P, S \rangle\) una grammatica.
Si supponga che, per ogni produzione \(\alpha \rightarrow \beta \in P\):

\begin{itemize}
  \item \(|\alpha| = 1\), ossia \(\alpha \in V_N\)
  \item \(\beta\) sia nella forma \(aB\), \(a\) può essere \(\epsilon\)
        \begin{itemize}
          \item \(B \in V_N\), alfabeto dei non terminali
          \item \(a \in V_T\), alfabeto dei terminali
        \end{itemize}
\end{itemize}

Allora \(G\) è una \textbf{grammatica lineare}, oppure \textit{regolare} o di \textit{tipo \(3\)} (si veda la Sezione~\ref{sec:gerarchia-di-Chomsky}).
Le grammatiche di questa categoria hanno al massimo un non terminale nella parte destra di ognuna delle sue derivazioni.

\bigskip
Si riconoscono due tipi particolari di grammatiche lineari:

\begin{itemize}
  \item[\(\leftarrow\)] Lineare \textbf{sinistra} (\(L\)-grammatica)
    \begin{itemize}
      \item Tutte le derivazioni sono nella forma \(\alpha \rightarrow \alpha w\)
      \item \(|\alpha| = 1\) è vuoto o singolo non terminale
      \item \(w\) è una stringa di terminali
    \end{itemize}
  \item[\(\rightarrow\)] Lineare \textbf{destra} (\(R\)-grammatica)
    \begin{itemize}
      \item Tutte le derivazioni sono nella forma \(\alpha \rightarrow w \alpha \)
      \item \(w\) è una stringa di terminali
      \item \(|\alpha| = 1\) è vuoto o singolo non terminale
    \end{itemize}
\end{itemize}

Una \textbf{grammatica} è \textbf{regolare} (in breve \RG) se e solo se è regolare sinistra o regolare destra.
Inoltre, un \textbf{linguaggio} è \textbf{regolare} se e solo se è generato da una grammatica regolare (e quindi esiste almeno una grammatica che lo genera).

\subsubsection{Grammatiche non contestuali}

Sia \(G = \langle V_T, V_N, P, S \rangle\) una grammatica.
Se, per ogni produzione \(\alpha \rightarrow \beta\) si verifica:

\begin{itemize}
  \item \(\alpha \rightarrow \beta \in P\), la produzione è contenuta in \(P\)
  \item \(|\alpha| = 1\),  \(\alpha\) è un non terminale
\end{itemize}

allora \(G\) è una \textbf{grammatica non contestuale} o \CFG (dall'Inglese \textit{context-free grammar}).
Questa denominazione è dovuta al fatto che la riscrittura di \(\alpha\) non dipende dal suo contesto \textit{(la parte della stringa che la circonda)}.

Le \CFG sono anche dette \textit{BNF} (da \textit{Backus-Naur Form}) e vengono impiegate per definire la sintassi di linguaggi di programmazione.
Le \RG sono anche \CFG ma non è vero il contrario:
\[ RG \subseteq CFG \]

\subsubsection{Grammatiche generali}

Le \textbf{grammatiche generali} (per brevità \GG o \textit{non ristrette}) sono tutte le grammatiche che non presentano limitazioni sulle produzioni.
Corrispondono al tipo \(0\) della gerarchia di Chomsky (Sezione~\ref{sec:gerarchia-di-Chomsky}).

Sia le \RG che le \CFG sono \textbf{non ristrette}:
\[ \RG \subseteq \CFG \subseteq \GG \]

\bigskip
In questa categoria cadono le grammatiche \textbf{ricorsivamente enumerabili} e le grammatiche \textbf{ricorsive}.
Per più dettagli sul significato di queste locuzioni, si consultino le Sezioni \ref{sec:insieme-ricorsivamente-enumerabile} e \ref{sec:insiemi-ricorsivi}.

\subsection{\RG e \FSA}

Dato un \FSA \(A\), è possibile costruire una \(R\)-grammatica a esso equivalente, ossia in grado di generare lo stesso linguaggio riconosciuto da \(A\) e viceversa (Sezioni \ref{sec:RG-da-FSA} e \ref{sec:FSA-da-RG}).

La conseguenza diretta è che \RG, \FSA ed espressioni regolari (Sezione~\ref{sec:espressioni-regolari}) sono modelli diversi per descrivere la stessa classe di linguaggi.

\subsubsection{Costruzione di \RG partendo da \FSA}
\label{sec:RG-da-FSA}

Sia \(A = \langle Q, I, \delta, q_0, F \rangle\) un \FSA.
È possibile costruire una \RG \(G = \langle V_N, V_T,  S, P \rangle\) tale che:

\begin{itemize}
  \item \(V_T = I\), l'insieme dei \textbf{terminali} di \(G\) corrisponde all'\textbf{alfabeto di ingresso} di \(A\)
  \item \(V_N = Q\), l'insieme dei \textbf{non terminali} di \(G\) corrisponde agli stati di \(A\)
  \item \(S = q_0\), l'\textbf{assioma} di \(G\) corrisponde allo \textbf{stato iniziale} di \(A\)
  \item Gli elementi di \(P\) \textbf{(l'insieme delle produzioni di \(G\))} hanno la seguente forma:
        \begin{itemize}
          \item \(B \rightarrow bC\) se e solo se \(C \in \delta(B, b)\) - \textit{transizione tra stati}
          \item \(B \rightarrow \epsilon\) se \(B \in F\) - \textit{transizione verso lo stato finale}
          \item \(\delta^\ast(q, x) = q^\prime\) se e solo se \(q \xRightarrow[]{\ast} x q^\prime\)
        \end{itemize}
\end{itemize}

\subsubsection{Costruzione di \FSA partendo da \RG}
\label{sec:FSA-da-RG}

Sia \(G = \langle V_N, V_T, S, P \rangle\) una \RG.
È possibile costruire un \FSA \(A = \langle Q, I, \delta, q_0, F \rangle\) tale che:

\begin{itemize}
  \item \(Q = V_N \cup \{q_F\}\), gli \textbf{stati} di \(A\) sono i \textbf{non terminali} di \(G\)
  \item \(I = V_T\), l'\textbf{alfabeto di ingresso} di \(A\) corrisponde all'\textbf{insieme di terminali} di \(G\)
  \item \(q_0 = S\), lo \textbf{stato iniziale} di \(A\) corrisponde all'\textbf{assioma} di \(G\)
  \item \(F = \left\{q_F\right\}\)
  \item La \textbf{funzione di transizione} \(\delta\) ha la seguente forma:
        \begin{itemize}
          \item \(\delta(A, b) = C\) se e solo se \(A \rightarrow bC\) - derivazione di terminali e non terminali
          \item \(\delta(A, b) = q_F\) se e solo se \(A \rightarrow b\) - derivazioni di terminali
        \end{itemize}
\end{itemize}

\subsection{\CFG e \NPDA}

Analogamente a quanto trattato nella Sezione~\ref{sec:RG-da-FSA}, è possibile costruire un \NPDA partendo da una \CFG \textit{(context-free grammar, grammatica non contestuale)} e viceversa.

\textit{Intuitivamente}, la pila contiene la parte intermedia delle produzioni, fatta di terminale e non terminali.
Rispettando le regole definite della \CFG, è possibile completare le produzioni fino ad ottenere una stringa di non terminali tramite mosse \textbf{non deterministiche} del \NPDA.

Un esempio del funzionamento del \NPDA che emula una \CFG con la regola \(S \Rightarrow aSb \Rightarrow aabb\) è mostrato in Figura~\ref{fig:analogia-cfg-npda}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-17.tikz}
  \caption{Analogia \CFG e \NPDA}
  \label{fig:analogia-cfg-npda}
\end{figure}

\subsection{\GG e \TM}

Analogamente a quanto trattato nella Sezione~\ref{sec:RG-da-FSA}, è possibile costruire una \TM partendo da una \GG \textit{(grammatica generale)} e viceversa.

\subsubsection{Costruzione di \GG partendo da \TM}

Sia \(M\) una \TM a nastro singolo.
È possibile costruire una \GG detta \(G\) tale che \(L(G) = L(M)\) \textit{(il linguaggio generato dalla \GG e accettato dalla \NTM sono equivalenti)}:

\begin{itemize}
  \item Inizialmente, \(G\) genera tutte le stringhe di tipo \(x \$ X\) con
        \begin{itemize}
          \item  \(x \in V_T^\ast\) e \(X\)
          \item \(X\) è la copia di \(x\) composta solo dai simboli \textbf{non terminali}
        \end{itemize}
  \item Successivamente, \(G\) simula le diverse configurazioni di \(M\) sfruttando la stringa a destra del simbolo \(\$\)
        \begin{itemize}
          \item la derivazione \(x \$ X \xRightarrow{\ast} x\) se e solo se \(x\) è accettata da \(M\)
          \item ogni mossa di \(M\) viene \textbf{emulata} con una derivazione immediata di \(G\)
          \item \(G\) ha quindi delle derivazioni della forma \(x \$ X \Rightarrow x \$ q_0 X\) \textit{(configurazione iniziale di \(M\))}
        \end{itemize}
  \item Infine, per simulare le mosse di \(M\) vengono introdotte le seguenti produzioni:
        \begin{enumerate}
          \item Se è definita \(\delta(q, A) = \langle q^\prime, A^\prime, R \rangle\) si definisce in \(G\) la produzione \(qA \rightarrow A^\prime q^\prime\)
          \item Se è definita \(\delta(q, A) = \langle q^\prime, A^\prime, S \rangle\) si definisce in \(G\) la produzione \(qA \rightarrow q^\prime A^\prime\)
          \item Se è definita \(\delta(q, A) = \langle q^\prime, A^\prime, L \rangle\) si definisce in \(G\) la produzione \(BqA \rightarrow q^\prime B A^\prime, \ \forall B\) dell'alfabeto di \(M\)
                \begin{itemize}[label=\(\rightarrow\)]
                  \item gli alfabeti di ingresso, uscita e memoria coincidono
                \end{itemize}
        \end{enumerate}
  \item Per completare la costruzione è infine necessario aggiungere a \(M\) le produzioni che permettono a \(G\) di derivare da \(x \$ \alpha B q A C \beta\) la sola \(x\) solo nei cai in cui \(M\) giunge a una configurazione di accettazione (\(x \$ \alpha B q_F A C \beta\)) cancellando tutto ciò che si trova a destra di \(\$\), esso compreso
\end{itemize}

\subsubsection{Costruzione di \TM partendo da \GG}

Sia \(G = \langle V_N, V_T, S, P \rangle\) una \GG.
È possibile costruire una \NTM \(M\) tale che \(L(M) = L(G)\) \textit{(il linguaggio accettato dalla \NTM e generato dalla \GG sono equivalenti)}:

\begin{itemize}
  \item \(M\) ha un nastro di \textbf{memoria}
  \item La stringa di ingresso \(x\) è sul nastro di \textbf{ingresso}
  \item Il nastro di memoria è inizializzato con l'\textbf{assioma} \(Z_0 S\)
  \item Il nastro di memoria in generale conterrà una \textbf{stringa} \(\alpha \in V^\ast\):
        \begin{itemize}
          \item Viene scandito per cercare la parte \textbf{sinistra} di una produzione \(P\)
          \item Una volta trovata, \(M\) compie una scelta \textbf{non deterministica} e la parte scelta è sostituita dalla parte destra corrispondente
          \item Se ad essa corrispondono \textbf{più parti destre}, ne viene scelta una in modo \textbf{non deterministico}
        \end{itemize}
\end{itemize}

In questo modo vi è in \(G\) una derivazione che porta dalla stringa \(\alpha\) alla stringa \(\beta\) (quindi una derivazione \(\alpha \xRightarrow{\ast} \beta\)) se e solo se esiste una sequenza di mosse tale per cui
\[ c_s = \langle q_s, Z_0 \alpha \rangle \vdash^\ast \langle q_s, Z_0 \beta \rangle \]
per qualche \(q_s\) (come illustrato in Figura~\ref{fig:derivazione-TM-da-GG}).

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.8]{image-12.tikz}
  \caption{Derivazione \(\alpha \xRightarrow{\ast} \beta\) in una \TM costruita da \GG}
  \label{fig:derivazione-TM-da-GG}
  \bigskip
\end{figure}

Se il nastro contiene una stringa \(y \in V_T^\ast\) (composta da soli simboli terminali), essa è confrontata con \(x\):

\begin{itemize}
  \item[\cmark] Se coincidono, allora \(x\) è \textbf{accettata}
  \item[\xmark] Altrimenti, questa particolare sequenza di mosse \textbf{non porta all'accettazione}
\end{itemize}

\paragraph{Note sulla costruzione di \NTM}

\begin{itemize}
  \item  Usare una \NTM facilita la costruzione di una \GG ma non è l'unico metodo ammesso per farlo.
        \begin{itemize}
          \item Poiché il non determinismo può essere emulato da macchine deterministiche, una \TM è sufficiente a tale scopo
        \end{itemize}
  \item Se \(x \notin L(G)\), allora \(M\) può tentare infinite computazioni, nessuna delle quali porta ad accettazione
        \begin{itemize}
          \item alcune di queste potrebbero non terminare mai, quindi senza concludere che \(x \in L(G)\)
          \item analogamente, non si concluderebbe se \(x \notin L(G)\)
        \end{itemize}
  \item La definizione di accettazione richiede che:
        \begin{itemize}
          \item \(M\) raggiunga una configurazione di accettazione se e solo se \(x \in L\)
          \item essa non richiede che \(M\) termini la computazione in uno stato non finale se \(x \notin L\)
          \item il problema del complemento è risolto e si ripresenta l'asimmetria tra risoluzione di un problema in senso positivo o negativo
        \end{itemize}
\end{itemize}

\clearpage

\section{Espressioni regolari - \RE}
\label{sec:espressioni-regolari}

Come visto fino ad ora, i linguaggi possono essere rappresentati tramite diverse \textbf{classi di modelli}, tra cui si riconoscono:

\begin{itemize}
  \item \textit{Insiemi}
  \item \textit{Pattern}
  \item \textit{Espressioni regolari}
  \item \textit{Modelli operazionali}
        \begin{itemize}
          \item Automi \textit{(Sezione~\ref{sec:automi-a-stati-finiti})}
          \item Trasduttori \textit{(Sezione~\ref{sec:trasduttori-a-stat-finiti})}
          \item Reti di Petri
          \item Diagrammi di stato
        \end{itemize}
  \item \textit{Modelli generativi}
        \begin{itemize}
          \item Grammatiche \textit{(Sezione~\ref{sec:grammatiche})}
        \end{itemize}
  \item \textit{Modelli dichiarativi}
        \begin{itemize}
          \item Logica \textit{(Sezione~\ref{sec:logica})}
        \end{itemize}
\end{itemize}

In questo capitolo si tratterà delle \textbf{espressioni regolari} (scritto come \RE o \textit{Regex}).
Esse sono delle espressioni utilizzabili per denotare un linguaggio attraverso la scrittura delle stringhe che lo compongono.

\subsection{Pattern}

Prima di dare una definizione formale di \RE, è necessario introdurre il concetto di \textbf{pattern}.

\bigskip
Un sistema di pattern è una tripla \(\langle A, V, p \rangle\) dove:

\begin{itemize}
  \item \(A\) è un \textbf{alfabeto}
  \item \(V\) è un \textbf{insieme di di variabili}
        \begin{itemize}[label=\(\rightarrow\)]
          \item è definito in modo che \(A \cup V = \emptyset\)
        \end{itemize}
  \item \(p\) è una stringa su \(A \cup V\) detta \textbf{pattern}
\end{itemize}

Il linguaggio generato dal sistema di pattern consiste di tutte le stringhe su \(A\) ottenute da \(p\) sostituendo ogni variabile in \(p\) con una stringa su \(A\).

\bigskip
\textit{Esempio:} \(\langle \underbrace{\left\{0, 1\right\}}_{alfabeto}, \underbrace{\left\{v_1, v_2\right\}}_{variabili}, \underbrace{v_1 v_1 0 v_2}_{pattern} \ \rangle\)
\begin{itemize}
  \item Stringhe che iniziano con \(0 \ (v_1 = \epsilon)\)
  \item Stringhe che iniziano con una stringa su \(A\) ripetuta due volte, seguita da uno \(0\) e da qualunque stringa (inclusa \(\epsilon\))
\end{itemize}

\subsection{Sintassi e semantica delle \RE}

Dato un alfabeto di simboli terminali, mediante le seguenti regole si definiscono le \textbf{espressioni regolari} e i corrispondenti linguaggi denotati.
Sono \RE su un alfabeto \(\Sigma\):

\begin{enumerate}
  \item \(\emptyset\) è una \RE che denota il linguaggio vuoto (\(\emptyset\))
  \item \(\epsilon\) è una \RE che nota il linguaggio \(\{\epsilon\}\)
  \item Ogni simbolo di \(\sigma\) è una \RE che denota il linguaggio \(\{\sigma\}, \ \sigma \in \Sigma\)
  \item Se \(R_1\) e \(R_2\) sono \RE, anche \(R_1 \cup R_2\), \textit{(scritto anche come \(R_1 + R_2\) o \(R_1 \, | \, R_2\))} è una \RE
  \item Se \(R_1\) e \(R_2\) sono \RE, anche \(R_1 \cdot R_2\), \textit{(scritto anche come \(R_1 R_2\))}, è una \RE
  \item Se \(R\) è una \RE, lo è anche \(R^\ast\)
  \item Nient'altro è una \RE
\end{enumerate}

Le \RE seguono la stessa idea dei sistemi di pattern, ma con diverso potere espressivo.
Le espressioni regolari sono diverse dai sistemi di pattern e hanno diverso potere espressivo.

Le \RE \textbf{corrispondono esattamente} ai linguaggi regolari e hanno lo stesso potere espressivo di \RG e \FSA: per ogni \FSA è possibile costruire la \RE equivalente.

\bigskip
\textit{Per dimostare l'enunciato} è sufficiente osservare che:

\begin{itemize}
  \item Ogni linguaggio denotato da una \RE è regolare. Infatti:
        \begin{enumerate}
          \item i casi base sono linguaggi regolari
          \item i linguaggi regolari sono chiusi rispetto a agli operatori di \textbf{concatenazione}, \textbf{unione} e \textbf{stella di Kleene}
        \end{enumerate}
  \item Data una \RG \(G\), è sempre possibile trovare una \RE \(r\) tale che \(L(G) = L(r)\)
\end{itemize}

\subsubsection{Operatori delle \RE}

Sono definiti i seguenti \textbf{operatori} delle \RE:

\begin{itemize}
  \item La \textbf{concatenazione}, \(\cdot\)
  \item L'\textbf{alternativa} (detta anche \textit{pipe}), \(|\)
  \item La \textbf{stella di Kleene} (già definita nella Sezione~\ref{sec:stella-di-Kleene}), \(\ast\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item di conseguenza, anche il \textbf{più di Kleene}, \(+\)
        \end{itemize}
  \item L'\textbf{opzionalità}, \(?\)
\end{itemize}

\subsubsection{\RE e pattern}

Le espressioni regolari seguono la stessa idea dei sistemi di pattern, ma con diverso potere espressivo.
Infatti, per riconoscere i linguaggi generati dai pattern servirà una \TM, mentre per le \RE sono sufficienti le \FSA.

Di conseguenza, le \RE non definiscono la stessa classe di linguaggi definita dai pattern.
Inoltre le due classi non sono confrontabili, e non sono una sottoclasse dell'altra.

Per la prima volta dall'inizio del corso si osserva un caso di non confrontabilità:

\[ \RE \ \neq \text{ pattern} \]

\subsubsection{\RE \texttt{POSIX}}

Lo standard \texttt{POSIX} è una API standard per i sistemi operativi UNIX/LINUX e definisce anche le \RE.
Esso include:

\begin{itemize}
  \item I \textbf{metacaratteri} \(( \ ) \ [ \ ] \ \textsuperscript{$\wedge$} \ \backslash \ \$ \ \ast \ + \ ? \ \{ \ \}\)
  \item La notazione \([\alpha]\) indica \textbf{un singolo carattere} \(\in \alpha\)
  \item La notazione \([\textsuperscript{$\wedge$} \alpha]\) indica \textbf{un qualunque simbolo} \(\notin \alpha\)
  \item Il simbolo \(\textsuperscript{$\wedge$}\) indica \textbf{\(\epsilon\) all'inizio di una riga} di testo
  \item Il simbolo \(\$\) indica \textbf{\(\epsilon\) alla fine di una riga} di testo
  \item I simboli \(\ast, +, \, | \,, (, )\) rappresentano gli \textbf{operatori} come già definiti \textit{(a esempio in Sezione~\ref{sec:operazioni-linguaggi})}
  \item Il simbolo \(\backslash\) funge da \textbf{escape}
\end{itemize}

In aggiunta agli operatori delle \RE, sono definiti:
\begin{itemize}
  \item \(\alpha ?\) indica che \(\alpha\) è \textbf{opzionale}
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(\alpha\) appare \(0\) o \(1\) volte
        \end{itemize}
  \item \(\alpha \{n\}\) indica la \textbf{potenza} \(\alpha^n\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(\alpha\) appare \(n\) volte
        \end{itemize}
  \item \(\alpha\{n, m\}\) indica \(\alpha^n \cup \alpha^{n+1} \cup \ldots \cup a^{m-1} \cup a^m\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(\alpha\) appare tra le \(m\) e le \(n\) volte
        \end{itemize}
\end{itemize}

\clearpage

\section{Logica nell'informatica}
\label{sec:logica}

In questa sezione verrà analizzata la logica dal punto di vista del suo uso nell'ingegneria informatica.
La logica è un \textit{formalismo descrittivo e universale}, cioè permette di descrivere le proprietà che si vogliono ottenere (o evitare) da un sistema, senza dover per forza formalizzare anche quest'ultimo.

Grazie a questa sua proprietà, la logica può essere applicata in numerosi contesti, come:

\begin{itemize}
  \item Le porte logiche nell'architettura dei calcolatori
  \item La specifica e la verifica di sistemi nell'ingegneria del software
  \item La definizione della semantica dei linguaggi di programmazione
  \item La programmazione logica
  \item I database
\end{itemize}

e molti altri.

All'interno del corso verranno usati la \PL e la \FOL \textit{(introdotte nelle Sezioni \ref{sec:logica-proposizionale} e \ref{sec:logica-primo-ordine})} per:

\begin{itemize}
  \item Definire i linguaggi \textit{(Sezione~\ref{sec:logica-linguaggi})}
  \item Specificare le proprietà di programmi \textit{(Sezione~\ref{sec:precondizioni-postcondizioni})}
  \item Specificare le proprietà dei sistemi % TODO aggiungere ref alla sezione in cui avviene
\end{itemize}

\subsection{Logica proposizionale - \PL}
\label{sec:logica-proposizionale}

La logica proposizionale è un linguaggio formale dalla sintassi semplice, basata su proposizioni elementari e su connettivi logici di tipo funzionale.
Opera tra proposizioni \textit{(che possono assumere il valore \vero o \falso)} e relazioni tra proposizioni.
Le proposizioni composte sono formate concatenando proposizioni semplici tramite connettivi logici.

Contrariamente a logiche più complicate \textit{(come la \nameref{sec:logica-primo-ordine}, analizzata nella Sezione~\ref{sec:logica-primo-ordine})}, non permette di operare e predicare tra oggetti non logici e quantificatori.

\subsubsection{Sintassi}

Sia \(\mathcal{L}\) un linguaggio della \textbf{logica proposizionale}.

\begin{itemize}
  \item L'\textbf{alfabeto}, di \(\mathcal{L}\) è composto da:
        \begin{itemize}
          \item Un insieme \textbf{numerabile} \textit{(finito o infinito)} di \textbf{proposizioni}: \(A, B, C, \ldots\)
                \begin{itemize}
                  \item le proposizioni sono simboli di relazione \textbf{nullaria}
                  \item I simboli dell'alfabeto sono privi di significato
                \end{itemize}
          \item Un insieme di \textbf{simboli} \textbf{} \(\lnot, \land, \lor, \rightarrow, \leftrightarrow\)
          \item I \textbf{simboli di punteggiatura} \(  (, ) \) - \textit{parentesi tonde}
        \end{itemize}
  \item L'\textbf{insieme di formule} di \(\mathcal{L}\) è il più piccolo insieme tale che:
        \begin{itemize}
          \item Ogni \textit{proposizione} è una \textbf{formula}
          \item Se \(F\) e \(G\) sono formule, allora \(\lnot F\), \(F \land G\), \(F \lor G\), \(F \rightarrow G\),  \(F \leftrightarrow G\) sono \textbf{formule}
        \end{itemize}
  \item Le \textbf{parentesi sono omesse} ovunque possibile usando l'ordine di precedenza:
        \[ \lnot \quad \land \quad \lor \quad \rightarrow \quad \leftrightarrow \]
  \item Se \(A\) è una proposizione, allora \(A\) e \(\lnot A\) sono detti \textbf{letterali}
        \begin{itemize}
          \item \(A\) è detto letterale \textbf{positivo}
          \item \(\lnot A\) è detto letterale \textbf{negativo}
        \end{itemize}
  \item se \(L\) è un letterale, allora \(\overline{L}\) è il \textbf{letterale complementare} definito come \(\lnot A\) \textit{se} \(L = A\) o \(A\) \textit{se} \(L = \lnot A\)
  \item \textit{Informalmente}, una \textbf{sotto formula} è una formula inclusa in un'altra formula
  \item L'insieme \(\tau(F)\) delle sotto formule di \(\mathcal{F}\) è definito come il più piccolo insieme di formule tale che:
        \begin{itemize}
          \item \(F \in \tau(F)\)
          \item se \(\lnot G \in \tau(F)\), allora \(G \in \tau(f)\)
          \item se \(G \land H, G \lor H, G \rightarrow H, G \leftrightarrow H\) appartengono a \(\tau(F)\), allora \(H, G \in \tau(F)\)
        \end{itemize}
\end{itemize}

\subsubsection{Semantica}
\label{sec:semantica-PL}

La \textbf{semantica} è introdotta per assegnare un significato alle formule.

Nella logica proposizionale, ogni formula può corrispondere a un solo valore di verità (\vero o \falso): è quindi una logica \textbf{a due valori}.

Un'\textbf{interpretazione} \(I\) è una funzione totale dell'insieme di proposizioni ai valori di verità.
Ogni interpretazione può essere convenientemente rappresentata come l'insieme delle proposizioni vere.

Con la notazione \(I \vDash F\) si indica che \(I\) \textbf{rende vera} \(F\).

\bigskip

\textit{Definizioni:}

\begin{itemize}
  \item Se \(I \vDash F\), allora \(I\) è un \textbf{modello} di \(F\). Questa nozione può essere estesa agli insiemi di formule
  \item \(F\) è \textbf{valida} (detta anche \textbf{tautologia}) se e solo se per ogni interpretazione \(I\) vale che \(I \vDash F\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item in questo caso si può anche scrivere \(\vDash F\)
        \end{itemize}
  \item \(F\) è \textbf{soddisfacibile} se e solo se esiste un'interpretazione \(I\) tale per cui \(I \vDash F\)
  \item \(F\) è \textbf{falsificabile} se e solo se esiste un'interpretazione \(I\) tale per cui \(I \nvDash F\)
  \item \(F\) è \textbf{insoddisfacibile} se e solo se per ogni interpretazione \(I\) vale  \(I \nvDash F\)
  \item \(F\) è \textbf{contingente} se e solo se è sia \textit{soddisfacibile} sia \textit{falsificabile}
  \item Ogni formula del tipo \(F \land \lnot F\) è detta una \textbf{contraddizione}, indicata con \(\perp\)
  \item La formula \(F \lor \lnot F\) è detta \textbf{principio del terzo escluso}, indicata con \(\top\)
  \item Un insieme di formule \(\mathcal{F}\) \textbf{comporta logicamente} una formula \(G\) (o \(G\) è una \textit{conseguenza logica} di \(\mathcal{F}\)) se ogni modello di \(\mathcal{F}\) è anche un modello di \(G\) e si scrive con \(\mathcal{F} \vDash G\). \textit{Esempio:}
        \begin{itemize}
          \item \(\{A, A \rightarrow B\} \vDash B, \{A, A\rightarrow B\} \vDash B \land C, \{A, A \rightarrow B\} \nvDash C \)
        \end{itemize}
  \item Per determinare sistematicamente se una formula segua da un insieme di formule si possono usare le \textbf{tabelle di verità}
\end{itemize}

\bigskip

Si considerino:

\begin{itemize}
  \item La \textit{proposizione} \(A\)
  \item La \textit{formula} \(F\)
  \item L'\textit{interpretazione} \(I\)
\end{itemize}

Allora:
\[ I \models A  \quad \textit{sse} \quad I(A) = \vero \]
Di conseguenza:
\begin{align*}
  I \vDash \lnot F \quad             & \textbf{sse} \quad I \nvDash F                                                    \\
  I \vDash F \land G \quad           & \textbf{sse} \quad I \vDash F \textbf{ e } I \vDash G                             \\
  I \vDash F \lor G \quad            & \textbf{sse} \quad I \vDash F \textbf{ o } I \vDash G                             \\
  I \vDash F \rightarrow G \quad     & \textbf{sse} \quad I \nvDash F \textbf{ o } I \vDash G                            \\
  I \vDash F \leftrightarrow G \quad & \textbf{sse} \quad I \vDash F \rightarrow G \textbf{ e } I \vDash G \rightarrow F
\end{align*}

Alternativamente, i connettivi possono anche essere esplicitati sotto forma di tabella (Tabella~\ref{tab:tabella-connettivi}).

\begin{table}[htbp]
  \bigskip
  \centering
  \begin{tabular}{c|c|c|c|c|c|c|c}
    \(F\)  & \(G\)  &  & \(\lnot F\) & \(F \land G\) & \(F \lor G\) & \(F \rightarrow G\) & \(F \leftrightarrow G\) \\ \hline
    \vero  & \vero  &  & \falso      & \vero         & \vero        & \vero               & \vero                   \\
    \vero  & \falso &  & \falso      & \falso        & \vero        & \falso              & \falso                  \\
    \falso & \vero  &  & \vero       & \falso        & \vero        & \vero               & \falso                  \\
    \falso & \falso &  & \vero       & \falso        & \falso       & \vero               & \vero                   \\
  \end{tabular}
  \bigskip
  \caption{Tabella di verità}
  \label{tab:tabella-connettivi}
\end{table}

\paragraph{Esempio di interpretazione}

Se:

\begin{itemize}
  \item \(I_1 = \left\{ A, C\right\}\)
  \item \(I_2 = \left\{ C, D\right\}\)
  \item \(F = (A \lor B) \land (C \lor D)\)
\end{itemize}

Allora:

\begin{itemize}
  \item \(I_1 \vDash F\)
  \item \(I_2 \nvDash F\)
\end{itemize}

\subsubsection{Forme normali}
\label{sec:forme-normali-PL}

\begin{itemize}
  \item Due formule \(F, G\) sono \textbf{semanticamente equivalenti} se e solo se vale sia \(F \vDash G\) che \(G \vDash F\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item in questo caso si usa la notazione \(F \equiv G\)
        \end{itemize}
  \item La formula \(G\), sottoformula di \(F\) può essere sostituita da una formula \(H\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item per indicare la formula risultante si usa la notazione \(F [G \, \backslash H]\)
        \end{itemize}
  \item Un insieme di connettivi è detto \textbf{funzionalmente completo} se e solo se qualunque formula proposizionale può essere trasformata in una formula semanticamente equivalente che contiene solo connettivi dell'insieme
        \begin{itemize}[label=\(\rightarrow\)]
          \item L'insieme \(\{\lnot, \land\}\) è funzionalmente complesso
          \item Esistono altri due connettivi booleani che singolarmente sono funzionalmente completi: \texttt{nand} e \texttt{nor}
        \end{itemize}
\end{itemize}

\bigskip

\textit{Equivalenze notevoli:}

\begin{align*}
  (F \land F)                      & \equiv F                                         & \text{idempotenza di } \land       \\
  (F \lor F)                       & \equiv F                                         & \text{idempotenza di } \lor        \\
  (F \land G)                      & \equiv (G\land F)                                & \text{commutatività di } \land     \\
  (F \lor G)                       & \equiv (G \lor F)                                & \text{commutatività di } \lor      \\
  \left(F \land (G \land H)\right) & \equiv \left((F \land (G \land H\right)          & \text{associatività di } \land     \\
  \left(F \lor (G \lor H)\right)   & \equiv \left((F \lor (G \lor H\right)            & \text{associatività di } \lor      \\
  \left((F \land G) \lor F\right)  & \equiv F                                         & \text{assorbimento}                \\
  \left((F \lor G) \land F\right)  & \equiv F                                         & \text{assorbimento}                \\
  (F \land \left(G \lor H\right))  & \equiv \left((F \land G) \lor (F \land H)\right) & \text{distributività}              \\
  (F \lor \left(G \land H\right))  & \equiv \left((F \lor G) \land (F \lor H)\right)  & \text{distributività}              \\
  \left(\lnot (\lnot F)\right)     & \equiv F                                         & \text{doppia negazione}            \\
  \left(\lnot (F \land G)\right)   & \equiv (\lnot F \lor \lnot G)                    & \text{legge di De Morgan}          \\
  \left(\lnot (F \lor G)\right)    & \equiv (\lnot F \land \lnot G)                   & \text{legge di De Morgan}          \\
  (F \leftrightarrow G)            & \equiv (F \rightarrow G) \land (G \rightarrow F) & \text{equivalenza}                 \\
  (F \rightarrow G)                & \equiv (\lnot F \lor G)                          & \text{implicazione materiale}      \\
  (F \rightarrow G)                & \equiv (\lnot G \rightarrow \lnot F)             & \text{implicazione contronominale} \\
\end{align*}

Il \textit{teorema di sostituzione} e le \textit{equivalenze notevoli} possono essere utilizzate per introdurre le cosiddette \textbf{forme normali}:

\begin{itemize}
  \item Una formula è in \textbf{forma normale negativa} se e solo se è composta solo da letterali, congiunzioni (\(\land\)) e disgiunzioni (\(\lor\))
  \item Una formula è in \textbf{forma normale congiuntiva} (\textit{CNF}) se e solo se ha la forma \(C_1 \land C_2 \land \ldots \land C_n\) dove ogni \(C_i\) è la disgiunzione di letterali
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(C_1 \land C_2 \land \ldots \land C_n \equiv C_1 \land C_2 \land \ldots \land C_n \land \top\), quindi si può affermare che \(\top\) è in \textit{CNF} con \(n = 0\)
        \end{itemize}
  \item Una formula è in \textbf{forma normale disgiuntiva} (\textit{DNF}) se e solo se ha la forma \(C_1 \lor C_2 \lor \ldots \lor C_n\) dove ogni \(C_i\) è la congiunzione di letterali
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(D_1 \lor D_2 \lor \land \lor D_n \equiv D_1 \lor D_2 \lor \ldots \lor D_n \lor \perp\), quindi si può affermare che \(\perp\) è in \textit{DNF} con \(n = 0\)
        \end{itemize}
  \item I \(C_i\) sono detti \textbf{clausole}, mentre i \(D_i\) sono detti \textbf{clausole duali}. Normalmente si usa una notazione insiemistica
\end{itemize}

\subsubsection{Sistemi formali - \textit{calculi}}

Un \textit{sistema formale (assiomatico deduttivo)}, oppure \textbf{calculus} in Inglese, consiste in un insieme di assiomi e un insieme di regole di inferenza che producono conseguenze logiche all'interno di una logica.
Questi elementi definiscono una \textit{relazione di derivabilità} (detta anche \textbf{dimostrabilità}) tra un insieme di formule \(\mathcal{F}\) e una formula \(G\).

Se una formula \(G\) può essere ottenuto da \(\mathcal{F}\) applicando solo regole di inferenza e assiomi, si scrive che \(\mathcal{F} \vdash G\).
Idealmente, la relazione di derivabilità dovrebbe essere \textbf{corretta} (cioè se \(\mathcal{F} \vdash G\) allora \(\mathcal{F} \vDash G\)) e \textbf{completa} (cioè se \(\mathcal{F} \vDash G\) allora \(\mathcal{F} \vdash G\)).

Se una formula \(F\) piò essere derivata in una teoria \(\mathcal{F}\) usando gli assiomi e le regole d'inferenza di un sistema, allora diciamo che \(F\) è un \textbf{teorema}.

\subsection{Logica del primo ordine - \FOL}
\label{sec:logica-primo-ordine}

La logica proposizionale (vista in Sezione~\ref{sec:logica-proposizionale}) ha molte applicazioni, ma il suo potere espressivo è ristretto.
Infatti, frasi come \inlinequote{tutti gli esseri umani sono mortali} e \inlinequote{ogni bambino ha dei genitori} possono essere espresse come proposizioni in un modo che non cattura le relazioni sottintese tra esseri umani, mortali, bambini e genitori.

\textit{Gottlob Frege}, logico tedesco, ha sviluppato la \FOL (\textbf{logica del prim'ordine} o \textbf{logica dei predicati}) nel \(1879\), estendendo la logica proposizionale con \textit{funzioni}, \textit{variabili} e \textit{quantificatori}.

\begin{itemize}
  \item Dal punto di vista \textit{epistemologico} (stati della conoscenza) sia \PL che \FOL considerano \textbf{verità} e \textbf{falsità}
        \begin{itemize}
          \item quindi vengono considerati i valori di \vero e \falso assunti dalla logica
        \end{itemize}
  \item Dal punto di vista \textit{ontologico} (ciò che esiste), \PL considera i \textit{fatti}, mentre \FOL considera anche:
        \begin{itemize}
          \item \textbf{oggetti}, quali: \textit{persone, case, numeri, corsi, \ldots}
          \item \textbf{proprietà} e \textbf{relazioni}, quali: \textit{essere rosso, essere felice, essere più grande di, essere parte di, \ldots}
          \item \textbf{funzioni}, quali: \textit{padre di, età di, successore di, \ldots}
        \end{itemize}
\end{itemize}

\subsubsection{Sintassi}

\begin{itemize}
  \item L'\textbf{alfabeto} di un linguaggio della \FOL \(\mathcal{L}\) è composto da:
        \begin{itemize}
          \item Un insieme infinito numerabile di \textbf{variabili}: \(X, Y, Z, \ldots\)
          \item Un insieme di simboli di \textbf{funzione}: \(f, g, \ldots\)
          \item Un insieme di simboli di \textbf{predicati}: \(p, q, r, \ldots\)
          \item I seguenti \textbf{connettivi}: \(\lnot, \land, \lor, \rightarrow, \leftrightarrow\)
          \item I seguenti \textbf{quantificatori}: \(\exists, \forall\)
          \item I \textbf{simboli di punteggiatura}: \((, ), ","\) - \textit{parentesi tonde e virgola}
        \end{itemize}
  \item Ogni simbolo di funzione e relazione ha una \textbf{arietà} fissata che indica il numero di argomenti a esso associati:
        \begin{itemize}
          \item la notazione \(p/n\) indica che il \textit{predicato} (o \textit{funzione}) \(p\) ha \textbf{arietà} \(n\)
          \item un \textit{predicato} (o \textit{funzione}) con arietà zero è detto \textbf{nullario}
          \item un \textit{predicato nullario} \(p()\) è scritto semplicemente come \(p\)
          \item una \textit{funzione nullaria} \(c()\) è semplicemente scritta come \(c\)
        \end{itemize}
  \item Le funzioni nullarie sono dette \textbf{costanti}, i predicati nullari sono detti \textbf{proposizioni}
  \item I \textbf{termini} denotano tutti gli oggetti che \(\mathcal{L}\) può trattare
        \begin{itemize}
          \item sono definiti induttivamente come segue:
                \begin{enumerate}
                  \item Ogni \textit{variabile} è un \textit{termine}
                  \item Se \(f/n\) è un simbolo di funzione e \(t_1, \ldots t_n\) sono termini, allora \(f (t_1, \ldots t_n)\) è un \textit{termine}
                \end{enumerate}
          \item i \textbf{termini generici} sono tipicamente indicati con \(s, t, \ldots\)
        \end{itemize}
  \item L'insieme di \textbf{formule della \FOL} è definito induttivamente come il più piccolo insieme tale che:
        \begin{itemize}
          \item Se \(p/n\) è un simbolo di relazione e \(t_1, \ldots t_n\) sono termini, allora \(p(t_1, \ldots, t_n)\) è una formula detta \textbf{formula atomica} o \textbf{atomo}
          \item se \(F, G\) sono formule e \(X\) è una variabile, allora sono formule anche:
                \[\lnot F \quad F \land G \quad F \lor G \quad F \rightarrow G \quad F \leftrightarrow G \quad \exists \, X F \quad \forall \, X F \]
        \end{itemize}
  \item I \textbf{letterali} sono atomi \textit{(letterali positivi)} o atomi negati \textit{(letterali negativi)}
  \item Le \textbf{parentesi sono omesse ovunque possibile} usando l'ordine di precedenza tra gli operatori:
        \[ \exists \quad \forall \quad \lnot \quad \land \quad \lor \quad \rightarrow \quad \leftrightarrow\]
  \item Per convenienza:
        \begin{align*}
          \exists \, X_1 \left(\ldots \left(\exists \, X_n (F) \right) \ldots \right) \quad & \text{è abbreviato come} \quad \exists \, X_1, \ldots, X_n(F) \\
          \forall \, X_1 \left(\ldots \left(\forall \, X_n (F) \right) \ldots \right) \quad & \text{è abbreviato come} \quad \forall \, X_1, \ldots, X_n(F) \\
        \end{align*}
  \item Se \(QX(F)\) è una formula e \(Q\) è un quantificatore, allora \(F\) si dice \textbf{ambito} di \(Q\) e \(Q\) è \textbf{applicato} a \(F\)
  \item Un'occorrenza di una variabile in una formula è \textbf{legata} se e solo se la sua occorrenza è entro l'ambito di un quantificatore che impiega quella variabile
        \begin{itemize}[label=\(\rightarrow\)]
          \item è legata al quantificatore di ambito più piccolo che la rende legata
          \item in caso contrario, è \textbf{libera}
        \end{itemize}
  \item Una formula è \textbf{chiusa} se e solo se non contiene occorrenze libere di variabili
        \begin{itemize}
          \item la valutazione è omessa quando si considerano formule chiuse
        \end{itemize}
  \item Un'interpretazione \(\mathcal{I}\) è detta \textbf{modello} per \(F\) sse per ogni valutazione \(\Phi\) si ha \(\mathcal{I} \vDash_\Phi F\)
        \begin{itemize}
          \item se \(\mathcal{F}\) è un insieme di formule, un'interpretazione è un \textbf{modello} di \(\mathcal{F}\) sse è un modello \(\forall \, F \in \mathcal{F}\)
        \end{itemize}
\end{itemize}

\paragraph{Osservazioni sulle traduzioni in \FOL}

\begin{itemize}
  \item Il connettivo principale usato con \(\forall\) è \(\rightarrow\)
        \begin{itemize}
          \item \inlinequote{al Polimi sono tutti brillanti} \texttt{(bleah)} si può tradurre con \[ \forall \, X \, (at(X, polimi) \rightarrow smart(X)) \]
          \item notare che la formula \[ \forall \, X \, (at(X, polimi) \land smart(X)) \] significa che \textbf{tutti} sono al Polimi e \textbf{tutti} sono brillanti
        \end{itemize}
  \item Analogamente, il connettivo principale da usare con \(\exists\) è \(\land\)
        \begin{itemize}
          \item \inlinequote{al Polimi qualcuno è brillante} si pu\`o tradurre con \[ \exists \, X \, (at(X, polimi) \land smart(X)) \] % a volte la o accentata mi sfancula il documento intero e non va più niente
          \item notare che la formula \[ \forall \, X \, (at(X, polimi) \rightarrow smart(X)) \] significa che c'è qualcuno che o non è al Polimi o è brillante \textit{(o entrambe)}
        \end{itemize}
\end{itemize}

\subsubsection{Semantica}

Come per \PL, anche \FOL ha una semantica a due valori di verità basata sulla nozione di interpretazione.

Una interpretazione \(\mathcal{I}\) di un alfabeto \(\mathcal{A}\) è un \textbf{dominio} non vuoto \(D\) (indicato anche come \(|\mathcal{I}|\)) a una funzione che associa:

\begin{itemize}
  \item Ogni costante \(c \in \mathcal{A}\) a un elemento \(c_{\mathcal{I}} \in D\)
  \item Ogni simbolo di funzione \(f/n \in \mathcal{A}\) a una funzione \(f_{\mathcal{I}}: D^n \rightarrow D\)
  \item Ogni simbolo di predicato \(p/n \in \mathcal{A}\) a una relazione \(p_{\mathcal{I}} \subseteq \underbrace{D \times \ldots \times D}_{n \text{ volte}}\)
\end{itemize}

Tuttavia, prima di assegnare un significato alle formule, va definito il significato di ciascun termine.
Poiché essi possono contenere variabili, occorre una \textbf{valutazione} (o \textbf{stato}) ossia una funzione dalle variabili di \(\mathcal{A}\) a \(|\mathcal{I}|\).

Il \textbf{significato} \(\Phi_{\mathcal{I}}(t)\) di un termine \(t\) nell'interpretazione \(\mathcal{I}\) e valutazione \(\Phi\) è quindi definito induttivamente come:

\begin{enumerate}
  \item \(c_{\mathcal{I}}\) se \(t\) è una costante \(c\)
  \item \(\Phi(X)\) se \(t\) è una variabile \(X\)
  \item \(f_{\mathcal{I}} \left( \Phi_{\mathcal{I}}(t_1), \ldots, \Phi_{\mathcal{I}}(T_n) \right)\) se \(t\) è della forma \(f(t_1, \ldots, t_n)\)
\end{enumerate}

\paragraph{Proprietà delle valutazioni}

Si considerino:

\begin{itemize}
  \item Una \textit{valutazione} \(\Phi\)
  \item Una \textit{variabile} \(X\)
  \item Un'\textit{interpretazione} \(\mathcal{I}\), \(c_{\mathcal{I}} \in |\mathcal{I}|\)
\end{itemize}

Allora \(\Phi[X \mapsto c_{\mathcal{I}}]\) è una valutazione analoga a \(\Phi\) che \textit{mappa} \(X\) in \(c_{\mathcal{I}}\).

Il significato di una formula è un valore di verità che è definito induttivamente come segue:

\begin{enumerate}
  \item Si indica \(I \vDash_\Phi F\) una formula \(F\) vera rispetto a \(\mathcal{I}\) e \(\Phi\)
  \item Si applicano le seguenti uguaglianze:
        \begin{align*}
          \mathcal{I} & \vDash_\Phi p(t_1, \ldots, t_n)   & \textit{sse} \quad & \langle \Phi_{\mathcal{I}}(t_1), \ldots,  \Phi_{\mathcal{I}}(t_n) \rangle \in P_{\mathcal{I}}          \\
          \mathcal{I} & \vDash_\Phi (\lnot F)             & \textit{sse} \quad & \mathcal{I} \nvDash_\Phi F                                                                             \\
          \mathcal{I} & \vDash_\Phi (F \land G)           & \textit{sse} \quad & \mathcal{I} \vDash_\Phi F \textit{ e } \mathcal{I} \vDash_\Phi G                                       \\
          \mathcal{I} & \vDash_\Phi (F \lor G)            & \textit{sse} \quad & \mathcal{I} \vDash_\Phi F \textit{ o } \mathcal{I} \vDash_\Phi G                                       \\
          \mathcal{I} & \vDash_\Phi (F \rightarrow G)     & \textit{sse} \quad & \mathcal{I} \nvDash_\Phi F \textit{ o } \mathcal{I} \vDash_\Phi G                                      \\
          \mathcal{I} & \vDash_\Phi (F \leftrightarrow G) & \textit{sse} \quad & \mathcal{I} \vDash_\Phi (F \rightarrow G)  \textit{ e } \mathcal{I} \vDash_\Phi (G \rightarrow F)    G \\
          \mathcal{I} & \vDash_\Phi (\forall \, X(F))     & \textit{sse} \quad & \mathcal{I} \vDash_{\Phi[X \mapsto c_{\mathcal{I}}]} F \  \forall \, c_{\mathcal{I}} \in |\mathcal{I}| \\
          \mathcal{I} & \vDash_\Phi (\exists \, X(F))     & \textit{sse} \quad & \mathcal{I} \vDash_{\Phi[X \mapsto c_{\mathcal{I}}]} F \  \exists \, c_{\mathcal{I}} \in |\mathcal{I}| \\
        \end{align*}
\end{enumerate}

\paragraph{Da \PL a \FOL}

La relazione di conseguenza logica \(\vDash\) tra insiemi di formule e formule può essere esteso anche a \FOL, così come i concetti di \textbf{validità, soddisfacibilità, falsificabilità, contingenza} e \textbf{insoddisfacibilità} (visti nella Sezione~\ref{sec:semantica-PL}).

Analogamente, anche le equivalenze mostrate per \PL (Sezione~\ref{sec:forme-normali-PL}) possono essere estese a \FOL, con l'aggiunta di:

\begin{align*}
  \forall \, X (F)                       & \equiv \lnot \left(\exists \, (\lnot F)\right) & \textit{ dualità dei quantificatori } 1           \\
  \exists \, X (F)                       & \equiv \lnot \left(\forall \, (\lnot F)\right) & \textit{ dualità dei quantificatori } 2           \\
  \forall \, X(F) \land (\forall \, X) G & \equiv \forall \, X (F \land G)                                                                    \\
  \exists \, X(F) \land (\exists \, X) G & \equiv \exists \, X (F \lor G)                                                                     \\
  (\forall \, X) (\forall \, Y) F        & \equiv (\forall \, Y) (\forall \, X) F                                                             \\
  (\exists \, X) (\exists \, Y) F        & \equiv (\exists \, Y) (\exists \, X) F                                                             \\
  (\forall \, X(F)) \land G              & \equiv \forall \, X (F \land G)                & \textit{ solo se } X \textit{ non è libera in } G \\
  (\forall \, X(F)) \lor G               & \equiv \forall \, X (F \lor G)                 & \textit{ solo se } X \textit{ non è libera in } G \\
  (\exists \, X(F)) \land G              & \equiv \exists \, X (F \land G)                & \textit{ solo se } X \textit{ non è libera in } G \\
  (\exists \, X(F)) \lor G               & \equiv \exists \, X (F \lor G)                 & \textit{ solo se } X \textit{ non è libera in } G \\
\end{align*}

\subsection{Logica e linguaggi}
\label{sec:logica-linguaggi}

La logica può aiutare a descrivere i linguaggi: gli insiemi di stringhe possono essere visti come abbreviazioni di formule \FOL.
Tramite formalismi matematici, infatti, è possibile mostrare i linguaggi in modo descrittivo focalizzandosi sulle proprietà rilevanti delle stringhe anziché sul modo in cui esse sono generate o riconosciute.
Per raggiungere tale obiettivo, bisogna analizzare:

\begin{itemize}
  \item \textbf{Cosa} va descritto
  \item \textbf{Come} le varie parti vanno definite
  \item \textbf{Quali primitive} possono essere assunte
\end{itemize}

\bigskip
Una volta che la lingua viene definita in modo logico, occorrerebbe definire tutti i predicati e le funzioni non elementari \textit{(come \(=, >, +, -, \times, \ast, \cdot, \ldots\))}.
Per evitare di doversi imbarcare in questa operazione, si fa normalmente riferimento a logiche più ristrette dalla sintassi specifica già predisposta.

\subsubsection{Esempi di descrizione della lingua tramite \FOL}

\paragraph{Esempio 1}
\label{par:esempio-fol-1}

L'insieme:
\[ \left\{ a^n b^n \, | \, n \geq 0 \right\} \]
può essere vista come una abbreviazione di:
\[ x \in L \Leftrightarrow \exists \, n \, | \, n \geq 1 \land x = a^n b^n \]

All'interno di questa formula si possono riconoscere:

\begin{itemize}
  \item I \textbf{predicati} \(\in L, \geq, =\)
  \item Le \textbf{funzioni} di \textit{concatenazione, elevamento a potenza}
\end{itemize}

Come già enunciato, sarebbe necessario definire ciascuno di questi elementi sopra elencati.
Per esempio, \(x^n\) può essere descritto \textit{ricorsivamente} come:
\[ \forall \, n \, \forall \, x\,  ((n = 0 \Rightarrow x^n = \epsilon) \land (n > 0 \Rightarrow x^n = x^{n-1} \cdot x)) \]

\paragraph{Esempio 2}
\label{par:esempio-fol-2}

Sia \(L\) il linguaggio definito come:
\[ L = a^\ast b^\ast \]

Quindi \(L\) è il linguaggio delle stringhe su \(\left\{ a, b \right\}\) che iniziano con \(a\).
Più precisamente, fanno parte di \(L\) le stringhe:

\begin{itemize}
  \item vuota (\(\epsilon\))
  \item composta da un carattere \(a\) e un suffisso appartenente a \(L\)
  \item composta da un prefisso appartenente a \(L\) e da un carattere \(b\)
\end{itemize}

Questo linguaggio può quindi essere espresso in \FOL come:
\[ x \in L \Leftrightarrow (x = \epsilon) \lor \left( \, \exists \, y \, | \, x = at \land y \in L \right) \lor \left( \, \exists \, y \, | \, x = yb \land y \in L \right) \]

\paragraph{Esempio 3}
\label{par:esempio-fol-3}

Sia \(L\) il linguaggio delle stringhe su \(\{a, b, c\}\) definito come:
\[ L = a^\ast b^\ast c^\ast \]

Quindi \(L\) è il linguaggio delle stringhe su \(\left\{ a, b, c \right\}\) con tutte le \(a\) all'inizio, poi tutte le \(b\) e poi tutte le \(c\).

Tra i molti modi possibili di rappresentare formalmente \(L\), si consideri il seguente: è possibile definire due linguaggi \textit{"ausiliari"} più semplici \(L_1 = a^\ast b^\ast, L_2 = b^\ast c^\ast\) e affermare che una stringa appartiene a \(L\) se:

\begin{itemize}
  \item è in \(L_1\)
  \item è in \(L_2\)
  \item composta da un carattere \(a\) e un suffisso appartenente a \(L\)
  \item composta da un prefisso appartenente a \(L\) seguito da un carattere \(c\)
\end{itemize}

Questo linguaggio può quindi essere espresso in \FOL come:
\[ x \in L \Leftrightarrow (x \in L_1) \lor (x \in L_2) \lor \, \exists \, y \left((x = ay \land y \in L) \lor (x = yc \land y \in L)\right) \]
in cui \(L_1, L_2\) sono definite come nell'esempio precedente (Sezione~\ref{par:esempio-fol-2}).

\paragraph{Esempio 4}
\label{par:esempio-fol-4}

Sia \(L\) il linguaggio delle stringhe su \(\left\{ a, b \right\}\) in cui il numero di \(a\) sia uguale al numero di \(b\).

Per definire questo linguaggio in \FOL si può introdurre la funzione di \textit{arietà} \(2\) \(\#(a, x)\), che conta il numero di apparizioni del simbolo \(a\) nella stringa \(x\).
Questa funzione è definita formalmente dalla formula:
\[ \left(x = \epsilon \Rightarrow \#(a, x) = 0 \right) \land \, \forall \, y \left((x = ay \Rightarrow \#(a, x) = \#(a, y) + 1\right) \land \left(x =  b y \Rightarrow \#(a, x) = \#(a, y) \right) \]
la cui definizione dipende dall'alfabeto.

\bigskip
Questo linguaggio può quindi essere espresso in \FOL come:
\[ x \in L \Leftrightarrow (x \in \{a, b\}^\ast) \land (\#(a, x) = \#(b, x)) \]

\subsubsection{Osservazioni sulla formulazione logica}

Non esiste una \inlinequote{formula magica} o un sistema di risoluzione unico per ottenere la descrizione in \FOL di un linguaggio, ma può essere utile evidenziare delle linee guida:

\begin{itemize}
  \item Dall'esempio nel Paragrafo~\ref{par:esempio-fol-3} si ricava che, quando si rivolge l'attenzione all'ordine con cui si susseguono le lettere in un linguaggio, si può far ricorso a formule in cui le stringhe siano decomposte nella concatenazione di sottostringhe, appartenenti ad altri linguaggi, eventualmente definiti \textit{ricorsivamente}
  \item Quando è necessario contare le occorrenze di alcune lettere, risulta utile definire una funzione in grado di contare i simboli a cui si è interessati, come nell'esempio nel Paragrafo~\ref{par:esempio-fol-4}
\end{itemize}

\subsection{Logica monadica del primo ordine - \MFO}
\label{sec:logica-monadica-primo-ordine}

Dato un alfabeto di ingresso \(\Sigma\), le formule sulla logica monadica del primo ordine (o \MFO per brevità) sono costruite dai seguenti elementi:

\begin{itemize}
  \item \textbf{Variabili del primo ordine}
        \begin{itemize}
          \item rappresentate da lettere minuscole (\(x, y, \ldots\))
          \item interpretate sull'insieme di numeri positivi \(\mathbb{N}\)
        \end{itemize}
  \item \textbf{Predicati monadici} (unari), uno per ogni simbolo di \(\Sigma\)
        \begin{itemize}
          \item rappresentate come (\(a(\cdot), b(\cdot), \ldots\))
          \item \(a(x)\) è valutata come \vero in una stringa \(w\) se e solo se il carattere di \(w\) in posizione \(x\) è \(a\)
        \end{itemize}
  \item La \textbf{relazione di minore} \(<\) tra numeri naturali
  \item Le normali proposizioni connettive e i quantificatori di primo ordine
\end{itemize}

Più precisamente, sia \(\mathcal{V}\) un insieme finito di variabili del primo ordine, e sia \(\Sigma\) un alfabeto.
Le formule ben formate (\textit{WFF}, dall'Inglese \textit{well formed formula}) della logica \MFO sono definite secondo la seguente sintassi:
\[ \phi \coloneqq a(x) \, | \, x < y \, | \, \lnot \phi \, | \, \phi \lor \phi \, | \, \exists \, x(\phi) \]
con \(a \in \Sigma, \ x, y \in \mathcal{V}\).

\bigskip
Sono verificate le seguenti definizioni di \textbf{connettivi proposizionali}:

\begin{align*}
  \phi_1 \land \phi_2           & \triangleq \lnot (\lnot \phi_1 \lor \lnot \phi_2)                            \\
  \phi_1 \lor \phi_2            & \triangleq \lnot (\lnot \phi_1 \land \lnot \phi_2)                           \\
  \phi_1 \Rightarrow \phi_2     & \triangleq \lnot \phi_1 \lor \phi_2                                          \\
  \phi_1 \Leftrightarrow \phi_2 & \triangleq ( \phi_1 \Rightarrow \phi_2 ) \land ( \phi_2 \Rightarrow \phi_1 ) \\
  \forall \, x (\phi)           & \triangleq \nexists \, x (\lnot \phi)
\end{align*}

I seguenti \textbf{quantificatori del primo ordine}:

\begin{align*}
  x \geq y & \triangleq \lnot (x < y)           \\
  x \leq y & \triangleq \lnot (x > y)           \\
  x = y    & \triangleq x \leq y \land y \leq x \\
  x \neq y & \triangleq \lnot (x = y)           \\
  x > y    & \triangleq y < x
\end{align*}

E le seguenti definizioni di:

\begin{itemize}
  \item \textbf{Costante}: \(x = 0 \triangleq \forall \, y \lnot (y < x)\)
  \item \textbf{Successore} di un numero naturale: \(\succop(x, y) \triangleq x < y \lnot \, \exists \, (x < z \lor z < y)\)
  \item \textbf{Somma} di valori costanti: \(y = x + k \triangleq \exists \, z_0, \ldots, z_k (z_0 = x \land \succop (z_0, z_1) \land \ldots \succop (z_{k-1}, z_k) \land y = z_k) \)
  \item \textbf{Primo}: \(\firstop(x) \triangleq \lnot \exists \, y \, (y < x)\)
        \begin{itemize}
          \item equivalente a \(x = 0\)
        \end{itemize}
  \item \textbf{Ultimo}: \(\firstop(x) \triangleq \lnot \exists \, y \, (y > x)\)
  \item \textbf{Sottrazione} di valori costanti: \(y = x - k \triangleq x = y + k\)
        \begin{itemize}
          \item[\(\rightarrow\)] \(k\) è una costante in \(\mathbb{N}\)
        \end{itemize}
\end{itemize}

\subsubsection{Esempi di \MFO}

\begin{itemize}
  \item Formula che è vera se tute e sole le parole il cui primo simbolo esiste ed è \(a\): \[ \exists \, x \, (x = 0 \land a(x)) \]
  \item Formula che è vera su tutte le parole in cui ogni \(a\) è seguita da una \(b\): \[ \forall \, x \, \left(a(x) \Rightarrow \exists \, y \, (y = \succop(x) \land b(y) ) \right)\]
  \item Parole non vuote il cui ultimo simbolo è \(a\): \[ \exists \, x \, ( \lastop(x) \land a(x) )\]
  \item Parole di almeno \(3\) simboli in cui il terzultimo simbolo è \(a\): \[ \exists \, x \left( a(x) \land \exists \, y \, (y =  x + 2 \land \lastop(y) ) \right) \]
\end{itemize}

\subsubsection{Semantica}

Una formula \MFO è interpretata su una stringa \(w \in \Sigma^+\) rispetto all'assegnazione \(v : \mathcal{V} \rightarrow U\) con \(U = \{0, \ldots, |w| - 1 \} \), che mappa \(\mathcal{V}\) a una posizione nella stringa \(w\).

La relazione di assegnamento (indicata con \(\vDash\)) è definita nel modo seguente:

\begin{align*}
  w, v & \vDash a(x)                & \textit{ sse } w = uav, |u| = v(x)                                                          \\
  w, v & \vDash x < y               & \textit{ sse } v(x)  < v(y)                                                                 \\
  w, v & \vDash \lnot \Phi          & \textit{ sse } w, v \nvDash \Phi                                                            \\
  w, v & \vDash \Phi_1 \land \Phi_2 & \textit{ sse } w, v \vDash \Phi_1 \land w, v \vDash \Phi_2                                  \\
  w, v & \vDash \forall \, x(\Phi)  & \textit{ sse } w, v^\prime \vDash \Phi \, \forall \, v^\prime, v^\prime(y) = v(y), y \neq x
\end{align*}

Data una stringa \(\phi\), il linguaggio \(L(\phi)\) è definito come:
\[ L(\phi) = \left\{ w \in \Sigma^+ \, | \, \exists \, v :  w, v \vDash \phi \right\} \]

\subsubsection{Proprietà di \MFO}
\label{sec:proprieta-mfo}

I linguaggi esprimibili mediante \MFO sono \textbf{chiusi} rispetto a:

\begin{itemize}
  \item \textbf{Unione}
  \item \textbf{Intersezione}
  \item \textbf{Complemento}
\end{itemize}

Mentre non \textbf{non sono chiusi} rispetto alla stella di Kleene.
Infatti, i linguaggi definiti tramite questa logica prendono il nome di \textit{star-free}, cioè definibili unicamente tramite unione, intersezione, complemento e concatenazione di linguaggi finiti.

\bigskip
Ne consegue che:

\begin{itemize}
  \item \MFO è strettamente meno potente degli \FSA
        \begin{itemize}
          \item data una formula \MFO si può sempre costruire un \FSA equivalente
          \item \(L_P\) può invece essere riconosciuto facilmente mediante \FSA
        \end{itemize}
  \item In \MFO non è possibile esprimere il linguaggio \(L_P\) rappresentante tutte e sole le parole di lunghezza pari con \(l = \{ a \}\)
        \begin{itemize}
          \item La formula \MFO \(a(0) \land a(1) \land \lastop(1)\) definisce il linguaggio \(L_{P2}\) fatto dalla sola parola \(\{aa\}\) di lunghezza 2
          \item Poiché \(L_P = L_{P2}^\ast\), a causa della mancata chiusura rispetto alla stella di Kleene, si spiega perché
        \end{itemize}
\end{itemize}

\subsection{Logica monadica del secondo ordine - \MSO}

Per ottenere lo stesso potere espressivo degli \FSA è necessario poter quantificare sui predicati monadici.

Quindi la \textbf{logica monadica del secondo ordine} è costruita sugli stessi elementi della \MFO (visti nella Sezione~\ref{sec:logica-monadica-primo-ordine}) con in aggiunta:

\begin{itemize}
  \item \textbf{Variabili del secondo ordine}
        \begin{itemize}
          \item rappresentate da lettere maiuscole \(X, Y, \ldots\)
          \item interpretate su \textit{insiemi} di numeri naturali
        \end{itemize}
\end{itemize}

Le formule ben formate \textit{(WFF)} della logica \MSO sono definite secondo la seguente sintassi:
\[ \phi \coloneqq a(X) \, | \, X(x) \, | \, x < y \, | \, \lnot \phi \, | \, \phi \lor \phi \, | \, \exists \, x (\phi) \, | \, \exists \, X (\phi) \]
con \(a \in \Sigma, \ x, y \in \mathcal{V}_1, X \in \mathcal{V}_2\).

\bigskip
Le definizioni della logica \MFO sono sempre valide, con l'aggiunta di:

\begin{align*}
  x \in X       & \triangleq X(x)                                          \\
  X \subseteq Y & \triangleq \forall \, x \, (x \in X \Rightarrow x \in Y) \\
  X = Y         & \triangleq (X \subseteq Y) \land (Y \subseteq X)         \\
  X \neq Y      & \triangleq \lnot (X = Y)
\end{align*}
con \(a \in \Sigma, \ x, y \in \mathcal{V}_1, \ X, Y \in \mathcal{V}_2\).

\subsubsection{Semantica}

Una formula \MSO è interpretata su una stringa \(w \in \Sigma^+\) rispetto alle assegnazioni:

\begin{align*}
  v_1 : \mathcal{V}_1 & \rightarrow \{0, \ldots, |w| - 1 \}      \\
  v_2 : \mathcal{V}_2 & \rightarrow \wp(\{0, \ldots, |w| - 1 \})
\end{align*}

dove:
\begin{itemize}
  \item \(v_1\) mappa ogni variabile di prim'ordine di \(\mathcal{V}_1\) a una posizione nella stringa \(w\)
  \item \(v_2\) mappa ogni variabile di second'ordine di \(\mathcal{V}_2\) a un insieme di posizioni della stringa \(w\)
\end{itemize}

Quindi, la relazione di assegnamento per le formule della logica \MSO è definita nel modo seguente:

\begin{align*}
  w, v_1, v_2 & \vDash  a(x)                 & \text{ sse } w = w_1 a w_2 \land |w_1| = v_1(x)                                                                                                     \\
  w, v_1, v_2 & \vDash X(x)                  & \text{ sse } v_1(x) \in v_2(X)                                                                                                                      \\
  w, v_1, v_2 & \vDash x < y                 & \text{ sse } v_1(x) < v_1(y)                                                                                                                        \\
  w, v_1, v_2 & \vDash \lnot \phi            & \text{ sse } w, v_1, v_2 \nvDash \phi                                                                                                               \\
  w, v_1, v_2 & \vDash \phi_1 \lor \phi_2    & \text{ sse } w, v_1, v_2 \vDash \phi_1 \lor     w, v_1, v_2 \vDash \phi_2                                                                           \\
  w, v_1, v_2 & \vDash \, \exists \, x(\phi) & \text{ sse } w, v^\prime_1, v_2 \vDash \phi \text{ per un } v^\prime_1, \ v^\prime_1(y) = v_1(y) \, \forall \, y \in \mathcal{V}_1 \backslash \{x\} \\
  w, v_1, v_2 & \vDash \, \exists \, X(\phi) & \text{ sse } w, v_1, v^\prime_2 \vDash \phi \text{ per un } v^\prime_2, \ v^\prime_2(Y) = v_2(Y) \, \forall \, Y \in \mathcal{V}_2 \backslash \{X\}
\end{align*}

\subsubsection{Espressività della logica \MSO}

Poiché ogni formula \MFO è anche formula di \MSO, e poiché è risultato impossibile descrivere il linguaggio \(L_P\) (come nella Sezione~\ref{sec:proprieta-mfo}), si deduce immediatamente che la logica \MSO è più espressiva della logica \MFO.

Per lo stesso motivo, si può dimostrare che la \MSO ha la stessa espressività degli \FSA, e quindi per ogni \FSA sarà possibile costruire una formula \MSO e viceversa.

\paragraph{Da \FSA a \MSO}

In generale, grazie alle quantificazioni del second'ordine è possibile trovare, per ogni \FSA, una formula \MSO equivalente.
L'idea generale della costruzione consiste nell'usare una variabile di secondo ordine \(X_q\) per ogni stato \(q\) dell'\FSA \(M\).
Il valore di ciascun \(X_q\) corrisponde all'insieme delle posizioni di tutti i caratteri che \(M\) può leggere in una transizione partendo dallo stato \(q\).

Assumendo che l'insieme di stati di \(M\) sia \(Q = \{0, 1, \ldots, k\}\) per un qualsiasi \(k\), con \(0\) che indica lo stato iniziale, la definizione di \(M\) che riconosce il linguaggio \(L\) è data dalla congiunzione di molteplici condizioni, ognuna traducente una parte della definizione di \(M\).

\paragraph{Da \MSO a \FSA}

Data una formula \MSO \(\Phi\), è possibile costruire un \FSA che accetta un linguaggio \(L\) definito da \(\Phi\) tramite il teorema di \textit{Büchi-Elgot-Trakhtenbrot}.

\bigskip
I passaggi che portano alla costruzione non sono oggetto di questo corso e non saranno mostrati.

\subsection{Precondizioni e postcondizioni}
\label{sec:precondizioni-postcondizioni}

Quando si programma una funzione è importante definire precisamente cosa fa, senza necessariamente specificare come lo fa.
Questo è lo scopo di \textbf{precondizioni} e \textbf{postcondizioni}.

\bigskip
A questo scopo viene introdotta la \textbf{notazione di Hoare}:

\begin{center}
  \{Precondizione: \(Pre\)\} \\
  Funzione \(F\) \\
  \{Postcondizione: \(Post\)\}
\end{center}

dove:
\begin{itemize}
  \item La \textbf{precondizione} indica cosa deve valere \textbf{prima} che la funzione \(F\) sia invocata
  \item La \textbf{postcondizione} indica cosa deve valere \textbf{dopo} che la funzione \(F\) ha finito terminato la propria esecuzione
\end{itemize}

Le precondizioni e postcondizioni possono essere definite in modi diversi, come:
\begin{itemize}
  \item Linguaggi naturali
  \item Linguaggi per asserzioni
  \item Linguaggi ad hoc
\end{itemize}

Tra di questi si riconosce innazitutto la \FOL, che può essere usata a questo scopo.

\subsubsection{Specifiche}
Una specifica deve essere usata come un \inlinequote{contratto}, che contiene tutte le informazioni necessarie senza assunzioni a priori.
Quando una qualche condizione viene eliminata dalla precondizione, la specifica diventa insoddisfacente.

\bigskip
Una \textbf{specifica formale} è una descrizione matematica di un sistema.
Come per le specifiche, esistono più linguaggi di specifica diversi.
Dopo aver specificato i requisiti di un algoritmo (o di un sistema), è necessario verificare la correttezza del medesimo.
Utilizzando un modello matematico dell'implementazione costruita, è possibile ottenere la prova di correttezza come una dimostrazione di teorema.

\bigskip
Il problema nella specifica dei sistemi è detto \textbf{frame problem}.
Esso rappresenta il problema di esprimere un dominio in logica senza dover descrivere esplicitamente le condizioni che non sono modificate da un'azione.
Per questo motivo situazioni estremamente semplici possono richiedere formalizzazioni complesse.

\subsubsection{Esempi di specifiche}

\paragraph{Algoritmo di ricerca}

Sia \(F\) una funzione che implementa la ricerca di un elemento \(x\) in un array ordinato \(a\) di \(n\) elementi.

Allora, \textit{informalmente}:

\begin{itemize}
  \item \textit{Precondizione}: l'array \(a\) è ordinato
  \item \textit{Postcondizione}: la variabile logica \texttt{found} deve essere vera se e solo se l'elemento \(x\) esiste nell'array \(a\)
\end{itemize}

Formalizzando le precedenti definizioni in \FOL si ottiene:

\begin{itemize}
  \item \textit{Precondizione:} \(\forall \, i \, (1 \leq n \leq n-1 \rightarrow a[1] \leq a[i+1])\)
  \item \textit{Postcondizione:} \(\texttt{found} \leftrightarrow \exists \, i \, (1 \leq i \leq n \land a[i] = x)\)
\end{itemize}

\paragraph{Agoritmo di ordinamento}

Sia \(ORD\) un programma che ordina un'array \(a\) di \(n\) elementi senza ripetizioni.

Allora, \textit{informalmente}:

\begin{itemize}
  \item \textit{Precondizione}: l'array \(a\) non contiene ripetizioni
  \item \textit{Postcondizione}: l'array ottenuto \(o\) è ordinato
\end{itemize}

Tuttavia questa definizione \textbf{non è sufficiente}: bisogna anche indicare che ogni elemento di \(a\) deve essere presente in \(o\).
Si aggiunge un array \(b\) (non utilizzato da \(ORD\)) con gli stessi elementi di \(a\) per poter fare rifermento a quest'ultimo prima che venga modificato.

Formalizzando le precedenti definizioni in \FOL si ottiene:

\begin{itemize}
  \item \textit{Precondizione:} \[\lnot \exists \, i, j \, (1 \leq i \leq n \land 1 \leq j \leq n \land i \neq j \land a[i] = a[j]) \land \forall \, i \, (1 \leq i \leq n \rightarrow a[i] = b[i])\]
  \item \textit{Postcondizione:}
        \begin{align*}
           & \forall \, i \, (i \leq i \leq n \rightarrow a[i] \leq a[i+1]) \ \land                                    \\
           & \forall \, i \, (1 \leq j \leq n \rightarrow \exists \, j \, (1 \leq j \leq n) \land a[i] = b[j]) \ \land \\
           & \forall \, j \, (1 \leq i \leq n \rightarrow \exists \, i \, (1 \leq i \leq n) \land a[i] = b[j])
        \end{align*}
\end{itemize}

\subsubsection{Esempi di specifica formale}

\paragraph{Comportamento di una lampada}

\textit{Se premo il tasto, la luce si accende entro \(\Delta\) unità di tempo.}

È necessario introdurre dei predicati opportuni:

\begin{itemize}
  \item \(P_B(t)\): istante di pressione del tasto
  \item \(L_{ON}(T)\): istante di accensione della luce
\end{itemize}

A questo punto si potrebbe \textbf{erroneamente} definire la equivalente formula \FOL della specifica come:
\[\forall \, t \, (P_B(t) \rightarrow \exists \, t_1 \, ((t \leq t_1 \leq t + \Delta) \land L_{ON}(t_1)))\]

Tuttavia:
\begin{itemize}
  \item \textbf{Non è specificato} che qualcuno debba premere il pulsante affinché la luce si possa accendere.
  \item \textbf{Non è specificato} cosa succede dopo che la luce si è accesa
  \item \textbf{Non è specificato} cosa succede se si preme il pulsante quando la luce è accesa
  \item \textbf{Non è specificato} cosa succede se si preme il pulsante due volte
  \item \textbf{Non è specificato} se la luce può essere accesa senza premere il tasto
\end{itemize}

\bigskip
Per poter dare una specifica corretta, si altera leggermente la definizione di funzionamento, supponendo che dopo essersi accesa la lampada rimanga nel suo stato per \(k\) unità di tempo per poi spegnersi.
Lo schema di funzionamento è riportato nella Figura~\ref{fig:schema-funzionamento-lampada}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-14.tikz}
  \caption{Schema di funzionamento della lampada}
  \label{fig:schema-funzionamento-lampada}
  \bigskip
\end{figure}

La specifica corretta sarà:

\begin{gather*}
  \forall \, t \, ((P_B(t) \land L_{OFF}(t)) \rightarrow \forall \, t_1 \, (t \leq t_1 \leq t + k) \rightarrow L_{ON}(t_1) \land L_{OFF}(t+k)) \\ \land \\
  \forall \, t_3, t_4 \, ((L_{OFF}(t_3) \land \, \forall \, t_5 \, (t_3 \leq t_5 \leq t_4) \rightarrow \lnot P_B(t_5)) \rightarrow L_{OFF}(t_4))
\end{gather*}

\clearpage

\section{Teoria della computabilità}

Dopo aver illustrato un numero significativo di modelli atti a descrivere problemi di elaborazione delle informazioni e la loro soluzione e dopo averne studiato le capacità e i limiti, ci dedicheremo ora a studiare quali problemi possano essere affrontati e risolti mediante macchine da calcolo.
Prima di poter continuare, è importante riassumere alcuni concetti appresi fino ad ora:

\begin{enumerate}
  \item Automi, grammatiche e altri formalismi possono essere considerati dispositivi meccanici per risolvere problemi matematici (e quindi i problemi pratici che essi modellano)
  \item Alcuni formalismi sono più potenti di altri, ossia sono in grado di riconoscere alcuni linguaggi che altri sono incapaci di riconoscere
  \item Nessun formalismo, tra quelli visti fino ad ora, è più potente di una \TM
        \begin{itemize}[label=\(\rightarrow\)]
          \item alcuni hanno tuttavia la stessa medesima potenza delle \TM
          \item questi modelli verranno d'ora in poi chiamati \textbf{formalismi massimi}
        \end{itemize}
\end{enumerate}

Una volta costruita una visione di insieme più completa, può venire naturale porsi delle domande del tipo:

\begin{itemize}
  \item \inlinequote{I formalismi introdotti sono adeguati per catturare l'essenza di un solutore meccanico?}
  \item \inlinequote{La capacità di un meccanismo di risolvere un problema dipende da come è formalizzato?}
  \item \inlinequote{Esistono formalismi e modelli più potenti delle \TM?}
  \item \inlinequote{Una volta che un problema è stato formalizzato adeguatamente, è sempre possibile risolverlo tramite dispositivi meccanici?}
\end{itemize}

Per rispondere a queste \textit{(e altre)} domande, è nata una branca dell'informatica detta \textbf{teoria della computabilità}.

\subsection{Formalizzazione di un problema matematico}

I formalismi fino ad ora studiati sono adeguati per tutti i problemi con domini numerabili, ovverosia gli insiemi i cui elementi possono essere messi in corrispondenza biunivoca con gli elementi di \(\mathbb{N}\).
Il problema originale, quindi, si riduce a quello del calcolo di una funzione \(f: \mathbb{N} \rightarrow \mathbb{N}\), esattamente come una traduzione o come un riconoscimento di linguaggio in un determinato alfabeto.

\bigskip
Si consideri, come \textit{esempio}, il problema relativo alla ricerca delle soluzioni di un sistema di equazioni del tipo:
\[\begin{cases}
    a_1 x_1 + a_2 x_2 = c_1 \\
    b_1 x_1 + b_2 x_2 = c_2
  \end{cases}\]
Esso può essere descritto come il calcolo della funzione \(f\) da \(\mathbb{Z}^6\) a \(\mathbb{Q}^2\) definita come:
\[ f(a_1, a_2, b_1, b_2, c_1, c_2) = \langle x_1, x_2 \rangle \]
dove \(x_1, x_2\), se esistono, sono i numeri razionali che soddisfano il sistema.

Si nota poi che è possibile far corrispondere biiettivamente \(\mathbb{Z}\) \textit{(numeri interi)} e \(\mathbb{Q}\) \textit{(numeri razionali)}  con \(\mathbb{N}\)  \textit{(numeri interi positivi)}, riducendo il problema al calcolo dell'opportuna funzione \(f: \mathbb{N} \rightarrow \mathbb{N}\) come proposto prima.

Poiché tutti e 3 gli insiemi sono \textbf{enumerabili}, la correlazione esiste e può essere costruita.
Un esempio di tale correlazione è mostrata nella Tabella~\ref{tab:correlazione-z-n}.

\begin{table}[htbp]
  \bigskip
  \centering
  \begin{tabular}{c|cccccccc}
    \(\mathbb{Z}\) & \(0\) & \(1\) & \(-1\) & \(2\) & \(-2\) & \(3\) & \(-3\) & \dots \\ \hline
    \(\mathbb{N}\) & \(0\) & \(1\) & \(2\)  & \(3\) & \(4\)  & \(5\) & \(6\)  & \dots \\
  \end{tabular}
  \bigskip
  \caption{Correlazione tra \(\mathbb{Z}\) e \(\mathbb{N}\)}
  \label{tab:correlazione-z-n}
\end{table}

\subsection{Riconoscimento e traduzione}

Il \textbf{riconoscimento} e la \textbf{traduzione} sono due formulazioni di un problema che possono essere ridotte l'una nell'altra.

Infatti, il problema di stabilire se, per una data stringa \(x\) e un dato linguaggio \(L\), \(x \in L\) si può anche impostare come la traduzione \(\tau_L\), dove

\[ \tau_L(x) =
  \begin{cases}
    1 & \text{se } x \in L \\
    0 & \text{altrimenti }
  \end{cases} \]

\bigskip
Viceversa, data una traduzione \(\tau : V^\ast_1 \rightarrow V^\ast_2\) si può definire il linguaggio:
\[ L_\tau = \left\{ z \, | \, z = x \$ y, \ x \in V^\ast_1, \ y \in V^\ast_2, \ \$ \notin (V_1 \cup V_2), \ y = \tau(x) \right\} \]
cioè il linguaggio delle stringhe formate da una stringa su \(V^\ast_1\), seguita dalla sua traduzione separata dal carattere speciale \(\$\).

Un dispositivo che riconosce \(L_\tau\) può essere usato come trasduttore che calcola \(\tau\): per ogni \(x\), è possibile enumerare \textbf{tutte le infinite} \(y \in V^\ast_2\) e verificare se \(x \$ y \in L_\tau\).
Se ciò avviene, si può concludere che la stringa \(y\) è la traduzione rispetto a \(\tau\) della stringa \(x\), cioè \(\tau(x) = y\).
Tuttavia questo processo termina se e solo se \(\tau(x)\) è definita, perché se così non fosse si procederebbe a enumerare le \(y \in V^\ast_2\) senza mai trovare una stringa che appartenga a \(L_\tau\).

\bigskip
Infine, è importante ricordare che:

\begin{itemize}
  \item Tutti i formalismi esaminati sono discreti, perché domini matematici numerabili definiti in modo finito
  \item La classe dei problemi che possono essere risolti da una \TM è indipendente dall'alfabeto scelto \textit{(a patto sia composto da almeno due simboli)}
\end{itemize}

\subsection{\TM e linguaggi di programmazione}

Data una \TM \(M\), è possibile scrivere un programma che la simuli in un qualsiasi linguaggio.
Analogamente, dato un qualunque programma, in un qualsiasi linguaggio di programmazione, è possibile costruire una \TM che calcola la stessa funzione eseguita dal primo.

Quindi, è immediato dedurre che \textbf{le \TM hanno lo stesso potere espressivo dei linguaggi di programmazione di alto livello} e di conseguenza sarà possibile sfruttare una \TM per eseguire un algoritmo.

\subsubsection{Algoritmi}
\label{sec:algoritmi}

Il concetto di \textbf{algoritmo} è uno dei concetti centrali dell'informatica.
\textit{Intuitivamente}, esso indica la procedura di risoluzione di un problema mediante un dispositivo automatico di calcolo, come un calcolatore elettronico.
Alternativamente può indicare un metodo per descrivere una serie astratta di comandi elementari, indipendenti dal linguaggio comprensibile a un calcolatore, per risolvere un dato problema.

Una definizione formale di algoritmo sarebbe di difficile formulazione, quindi ci si limita a descriverne delle proprietà in modo informale:

\begin{enumerate}[label=\Alph*), ref=(\Alph*)]
  \item \label{enum:algoritmi-1} Un algoritmo contiene una sequenza \textbf{finita} di istruzioni
  \item Ogni istruzione deve essere \textbf{immediatamente eseguibile} tramite qualche procedimento meccanico
  \item  \label{enum:algoritmi-3} Il processore è dotato di \textbf{dispositivi di memoria} in cui possono essere immagazzinati i risultati intermedi
  \item \label{enum:algoritmi-4} La computazione è un processo \textbf{discreto}
        \begin{itemize}[label=\(\rightarrow\)]
          \item l'informazione è codificata in forma digitale
          \item la computazione procede attraverso passi discreti
        \end{itemize}
  \item \label{enum:algoritmi-5} Gli algoritmi sono eseguiti in modo \textbf{deterministico}
  \item \textbf{Non esiste un limite} alla quantità di \textbf{dati} di ingresso e uscita
  \item \textbf{Non esiste un limite} alla quantità di \textbf{memoria} richiesta per effettuare i calcoli
  \item \label{enum:algoritmi-8} \textbf{Non esiste limite} al numero di passi discreti richiesti per effettuare un calcolo
        \begin{itemize}[label=\(\rightarrow\)]
          \item si possono avere computazioni infinite
        \end{itemize}
\end{enumerate}

Le ipotesi dalla \ref{enum:algoritmi-1}~alla~\ref{enum:algoritmi-4} si applicano agli algoritmi nei calcolatori digitali, mentre quelle dalla \ref{enum:algoritmi-5}~alla~\ref{enum:algoritmi-8} si applicano in senso generale.
Il punto~\ref{enum:algoritmi-4} relativo al non determinismo è una semplificazione \textit{(analogamente al punto~\ref{enum:algoritmi-3} riguardante la memoria)} rispetto al formalismo delle \TM.

\subsection{Tesi di Church}
\label{sec:tesi-di-church}

La tesi di Church è intrinsecamente non dimostrabile perché è basata solo sull'esperienza precedente e sull'evidenza intuitiva.
Per questo motivo non è considerato un teorema.

È divisa in due parti.

\subsubsection{Prima parte}
La prima parte della \textbf{tesi di Church} afferma che:

\indentquote{Non esiste alcun formalismo, per modellare una determinata computazione meccanica, che sia più potente delle \TM e dei formalismi a esse equivalenti.}

Con la locuzione \textit{potenza di calcolo} si intende la classe di problemi risolvibili tramite un determinato formalismo e non lo \inlinequote{sforzo} che la macchina impiega nel calcolo della soluzione.

Questa tesi è da verificare ogni qualvolta viene inventato un modello più potente di calcolo \textit{(come i computer quantistici)} ma tutt'oggi non è ancora stata provata la sua falsità.

\subsubsection{Seconda parte}

La seconda parte della \textbf{tesi di Church} afferma che:

\indentquote{Ogni algoritmo per la soluzione automatica di un problema può essere codificato in termini di una \TM (o di un formalismo equivalente).}

Ciò implica che nessun algoritmo può risolvere problemi che non possano già essere risolti da una \TM: essa infatti rappresenta il più potente calcolatore che sia possibile costruire.

\bigskip
Per la definizione di algoritmo, si faccia riferimento alla Sezione~\ref{sec:algoritmi}.

\subsection{Enumerazione delle \TM e \TM universali}
\label{sec:enumerazione-TM-UTM}

Fino ad ora le \TM sono state analizzate in quanto dispositivi in grado di risolvere \textbf{un particolare} problema.
In particolare, è stato definito il problema definito dalla funzione di transizione \(\delta\), facente parte della struttura della macchina stessa.

Per questo motivo, le \TM possono essere considerate come macchine calcolatori astratti, specializzati e non programmabili.
Una volta caricato il programma nella memoria \textit{(l'automa di controllo)}, la macchina potrà eseguire solo quella particolare sequenza di istruzioni.

A questo punto può venire naturale chiedersi:

\indentquote{È possibile modellare un calcolatore programmabile con una \TM? Le \TM sono in grado di calcolare tutte le funzioni da \(\mathbb{N}\) a \(\mathbb{N}\)?}

Le prossime Sezioni si dedicheranno alla ricerca di una soluzione a entrambe le domande.

\subsubsection{Enumerazione algoritmica}
\label{sec:enumerazione-algoritmica}

Dato un qualsiasi insieme \(S\), esso può essere \textbf{numerato algoritmicamente} se è possibile stabilire una relazione biettiva fra lo stesso \(S\) e l'insieme dei numeri naturali \(\mathbb{N}\), tramite un algoritmo o una \TM \textit{(grazie alla tesi di Church)}.
È possibile dimostrare formalmente che le \TM possono essere enumerate \textbf{algoritmicamente}.

\bigskip
Per chiarire le idee sulla enumerazione, si consideri il seguente \textit{esempio}:

Sia \(L\) l'insieme \(\{a, b\}^\ast\) delle stringhe sull'alfabeto \(\{a, b\}\).
Tale insieme è algoritmicamente enumerabile: per esempio, si possono ordinare le stringhe per lunghezza crescente e, a parità di lunghezza, assegnare un ordine tra gli elementi dell'alfabeto su cui sono definite.
Il risultato è mostrato nella Tabella~\ref{tab:enumerazione-algoritmica}.

\begin{table}[htbp]
  \bigskip
  \centering
  \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
    \(\epsilon\) & \(a\) & \(b\) & \(aa\) & \(ab\) & \(ba\) & \(bb\) & \(aaa\) & \(aab\) & \dots \\ \hline
    \(0\)        & \(1\) & \(2\) & \(3\)  & \(4\)  & \(5\)  & \(6\)  & \(7\)   & \(8\)   & \dots
  \end{tabular}
  \bigskip
  \caption{Enumerazione algoritmica su \(\{a, b\}^\ast\)}
  \label{tab:enumerazione-algoritmica}
\end{table}

\subsubsection{Algoritmo di enumerazione}

Per semplicità, si fissi un alfabeto \(A\) e si consideri l'insieme \(\{\TM_A\}\) delle \TM a nastro singolo e senza stati finali, aventi \(A\) come insieme di simboli.
Le \TM in \(\{\TM_A\}\) accettano una stringa se e solo se arrivano in uno stato di \texttt{halt}. In caso contrario, compierebbero infinite mosse, entrando in un loop da cui non possono più uscire.
Queste semplificazioni non comportano perdita di generalità perché queste operazioni sono possibili su ciascuna \TM vista fino ad ora.

L'obiettivo di questa dimostrazione sarà spiegare come \(\{\TM_A\}\) possa essere numerato, ossia come sia possibile stabilire una biiezione \(\mathscr{E}: \mathbb{N} \leftrightarrow \{\TM_A\}\).

Si osservi in primo luogo che \(\forall \, k \in \mathbb{N}\) esiste un numero finito di \TM, con \(k\) stati e con alfabeto \(A\).
Infatti, la funzione di transizione \(\delta\) sarà definita su dominio e codominio finiti.
In senso più generale, data una funzione \(f: D \rightarrow R\), con \(D, R\) finiti, vi sono esattamente \(|R|^{|D|}\) funzioni totali \(f\) diverse.

Seguendo lo stesso ragionamento per le \TM, poiché la loro funzione di transizione di transizione parziale \(\delta\) è definita come
\[ \delta : Q \times A \rightarrow Q \times A \times \{R, L, S\}^{k+1} \cup \{\bot\}\]
esisteranno esattamente:
\[ (1 + 3 \times |Q| \cdot |A|)^{|Q| \cdot |A|} = (1 + 3 \times h \cdot k)^{h \cdot k} \]
\TM, aventi \(|A| = h, |Q| = k\) \textit{(cardinalità dell'alfabeto e degli stati)}.
Il termine unitario è dovuto alla presenza di \(\bot\) nel codominio.

Per poter enumerare le \TM dell'insieme sarà però prima necessario ordinarle in base all'\textit{ordine lessicografico}.
A tal fine è sufficiente imporre un ordinamento all'interno degli insiemi \(Q\), \(A\) ed \(\{R, L, S\}\), ottenendo un insieme ordinato:
\[ \mathscr{E}: \{TM_A\} \mapsto \mathbb{N} \]

L'enumerazione \(\mathscr{E}\) è algoritmica: è possibile implementare la costruzione con un algoritmo \textit{(sia esso implementato in un linguaggio di programmazione o a sua volta una \TM)} che emetta in uscita tutti gli elementi di \(\{\TM_A\}\) nell'ordine lessicografico appena imposto.
Analogamente, sarà possibile costruire un algoritmo che faccia l'operazione inversa, cioè che partendo da una \TM ne estragga il corrispondente \(k \in \mathbb{N}\).

\bigskip
Infine l'enumerazione calcolabile da una \TM prende il nome di \textbf{Gödelizzazione} e il numero naturale associato verrà chiamato \textbf{numero di Gödel} dell'oggetto.
Nella precedente enumerazione, ogni \TM è biiettivamente identificata dal suo numero di Gödel.

\subsubsection{Riassunto dell'enumerazione}
\label{sec:riassunto-enumerazione}

Per riassumere i concetti visti nell Sezione precedente, si ottiene un enunciato fondamentale:

\begin{enumerate}
  \item Per ogni alfabeto \(A\), il corrispondente l'insieme di \TM \(\{\TM_A\}\) può essere numerato algoritmicamente, perché può essere sempre stabilita una biiezione \(\mathscr{E}: \{TM_A\} \leftrightarrow \mathbb{N}\)
  \item Tutte le funzioni \textbf{algoritmicamente computabili} si possono enumerare algoritmicamente
  \item Tutti i linguaggi \textbf{algoritmicamente riconoscibili} si possono enumerare algoritmicamente
\end{enumerate}

Si noti che, sebbene \(\mathscr{E}\) sia una funzione biettiva, le enumerazioni delle funzioni computabili e dei linguaggi riconoscibili non sono sempre tali, perché si verifica che molte \TM sono in grado di calcolare la medesima funzione.

\bigskip
Per comodità, nel resto del corso si farà riferimento a \TM che si comportano come dispositivi che calcolano funzioni da \(\mathbb{N}\) a \(\mathbb{N}\).
Per ogni funzione \(f: D \rightarrow R\), con \(D\), \(N\) diversi da \(\mathbb{N}\), si presupporrà implicitamente la codifica algoritmica di \(D\) e \(R\) in funzione di \(\mathbb{N}\).
Analogamente si farà riferimento alla \(y\)-esima \TM generata dalla enumerazione \(\mathscr{E}\) con \(M_y\) (quindi \(M_y = \mathscr{E}(y)\)).

\subsubsection{Macchina di Turing universale - \UTM}
\label{sec:macchina-turing-universale}

Fino a ora le \TM sono state considerate come calcolatori astratti, specializzati e non programmabili.
In questa sezione verrà introdotta la \textbf{macchina di Turing universale} (o \UTM, dall'Inglese \textit{Universal Turing Machine}), ossia una \TM in grado di modellare dispositivi di risoluzione dei problemi, in cui il problema da risolvere non viene codificato nella struttura del dispositivo ma viene fornito in ingresso insieme ai dati su cui operare.

\bigskip
La \UTM può essere definita come una \TM che calcola la funzione \(g(y, x) = f_y(x)\), dove:

\begin{itemize}
  \item \(y \in \mathbb{N}\) indica l'indice della \textbf{funzione} \(f_y\) calcolata dalla \TM \(M_y\)
  \item \(x \in \mathbb{N}\) rappresenta l'\textbf{ingresso} su cui opera \(M_y\)
\end{itemize}

Apparentemente la \UTM così definita non sembrerebbe appartenere all'insieme \(\{\TM_A\}\) in quanto le funzioni a esso associate sono in una variabile, mentre \(g\) è definita a due variabili.
Questo problema si dimostra facilmente risolvibile in quanto esiste \textbf{sempre} una biezione \(\mathbb{N} \times \mathbb{N} \mapsto \mathbb{N}\).
Un esempio di tale biiezione è dato dalla funzione:
\[ d(x, y) = \dfrac{(x+y)(x+y+1)}{2} + x \]
che permette di associate una coppia \(x, y \in \mathbb{N}\) a un numero  \(k \in \mathbb{N}\) in modo biunivoco.
Allo stesso modo sarà possibile costruire la funzione inversa \(d^{-1}\).

La Figura~\ref{fig:associazione-biunivoca} illustra il funzionamento della biezione.

\bigskip
Poiché si dimostra che \(d\) e \(d^{-1}\) sono computabili, la \TM sarà \textbf{sempre} un grado di calcolarla.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-15.tikz}
  \caption{Illustrazione dell'associazione biunivoca}
  \label{fig:associazione-biunivoca}
  \bigskip
\end{figure}

Si osservi che \(g\) è una funzione \TM-computabile, ossia esiste un \(i \in \mathbb{N}\) tale che \(f_i = g\) ed è possibile calcolarla mediante i passi seguenti:

\begin{enumerate}
  \item Si sceglie un alfabeto finito \(A\) per codificare ogni informazione richiesta per la computazione.
        \begin{itemize}[label=\(\rightarrow\)]
          \item qualunque \(A\) con \(|A| \geq 2\) è adatto allo scopo
        \end{itemize}
  \item Si traduce la rappresentazione di \(n\) nella opportuna coppia \(\langle x, y \rangle\) tramite la funzione biettiva scelta
  \item Si traduce il numero \(y\) in un'opportuna codifica della \TM \(y\)-esima nella enumerazione \(\mathscr{E}\), \(M_y\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item La traduzione viene memorizzata sul nastro della \UTM
        \end{itemize}
  \item Si simula la computazione di \(M_y\) su \(x\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item la \UTM lascia sul nastro \(f_y(x)\) se e solo se \(M_y\) termina la sua computazione su \(x\)
        \end{itemize}
\end{enumerate}

Quindi viene dimostrato che la \UTM è in grado di calcolare \(g(x, y) = f_y(x) \ \forall \, x, y\), cioè il comportamento di ogni altra \TM.
La \UTM è quindi in grado di calcolare anche il comportamento di \textbf{sé stessa}.

\bigskip
Con questo enunciato viene risposta la prima domanda posta nella Sezione~\ref{sec:enumerazione-algoritmica}.

\subsection{Problemi algoritmicamente irrisolvibili}

Prima di potersi occupare della risolvibilità dei problemi \textit{(e della loro relativa irrisolvibilità)}, è necessario studiare il numero delle stesse funzioni computabili.
Come concluso nella Sezione~\ref{sec:riassunto-enumerazione}, infatti, tutte le funzioni computabili \(f_y : \mathbb{N} \rightarrow \mathbb{N}\) sono enumerabili.
Questo significa che la cardinalità del loro insieme è \(\aleph_0\).

Analogamente, la cardinalità della classe \(\mathscr{F}\) delle funzioni su \(\mathbb{N}\), cioè \(\mathscr{F} = \left\{ f \, | \, f: \mathbb{N} \rightarrow \mathbb{N} \right\}\) è:
\[ |\mathscr{F}| = |\mathbb{N}| \times |\mathbb{N}| = \aleph_0 \times \aleph_0 = 2^{\aleph_0} \]
Questo valore è noto come \textbf{cardinalità del continuo} perché pari alla cardinalità dell'insieme \(\mathbb{R}\) dei numeri reali.

Quindi rapportando le due cardinalità \textit{(rispettivamente \(\aleph_0\) e \(2^{\aleph_0}\) per le funzioni risolvibili e per tutte le funzioni)} si conclude che la prima è minore della seconda (\(\aleph_0 < 2^{\aleph_0}\)), e quindi \textbf{non tutte le tutte le funzioni sono risolvibili}.

\bigskip
Così facendo è stata data risposta anche alla seconda domanda posta nella Sezione~\ref{sec:enumerazione-algoritmica}.

\subsubsection{Problemi definibili}

\textit{Intuitivamente}, le conclusioni raggiunte nella sezione precedente non forniscono grandi limitazioni.
Infatti, nonostante la classe di tutte le funzioni \(f: \mathbb{N} \rightarrow \mathbb{N}\) sia di cardinalità superiore alla classe delle funzioni computabili, le sole funzioni \textit{interessanti} da calcolare sono quelle possono essere \textit{definite}.

Per poter quindi definire un problema, è necessario usare una stringa di un qualche linguaggio, sia esso di tipo naturale o formale.
Per esempio, alcune descrizioni di problemi possono essere:

\begin{itemize}
  \item \textit{il numero che moltiplicato per se stesso è uguale a y}
  \item \(f(x) = x^4- \pi \cdot x^2\)
  \item \(\displaystyle f(x) = \int_0^x z \cdot sin(z) dz\)
  \item \(f(x) = \displaystyle \lim_{x \rightarrow \infty} \dfrac{e^x}{x^2+x}\)
\end{itemize}

Ogni linguaggio è un sottoinsieme di \(A^\ast\) \textit{(il linguaggio delle \UTM prese in esame)} e quindi è numerabile.
Così si prova che l'insieme dei problemi che si possono definire è esso stesso numerabile e:
\[ \{\text{problemi risolvibili}\} \subseteq \{\text{problemi definibili}\}\]
ma, \textit{purtroppo}, i due insiemi non coincidono \textbf{e parte dei problemi definibili non sarà risolvibile}.
La relazione tra i due insiemi sarà quindi:
\[ \{\text{problemi risolvibili}\} \subset \{\text{problemi definibili}\}\]

\textit{In pratica}, la classe dei problemi non definibili è rappresentata da tutti quei problemi che, per carenze di tipo linguistico \textit{(sia esso naturale, matematico o altro)} non è possibile esprimere.
L'assenza di una soluzione per essi non è rilevante in quanto non è neanche possibile porsi il problema di cercarla.

\subsection{Problema della terminazione - \textit{halting problem}}
\label{sec:problema-della-terminazione}

Una delle proprietà che si vuole garantire durante la stesura di un programma, qualunque sia il linguaggio, è la capacità del programma stesso di \textit{terminare} correttamente, ossia che \inlinequote{non vada in loop}.

Sarebbe estremamente utile poter determinare a priori, prima di eseguire una funzione con un determinato input, se essa porti a risultati in tempo finito o cada in loop infiniti che la portino a non terminare mai.

Questo problema, detto \textbf{problema della terminazione} (o in Inglese \textit{halting problem}), è \textit{descrivibile}, ma purtroppo \textit{non decidibile}.

\bigskip
Analizzando il problema in modo \textit{formale}, si indicherà con \(M_y\) la \TM \(y\)-esima secondo la numerazione \(\mathscr{E}\) e con \(f_y\) la funzione calcolata da \(M_y\).
Allora, è verificato che nessuna \TM può calcolare la funzione totale \(g: \mathbb{N} \times \mathbb{N} \rightarrow \{0, 1\}\) definita nel seguente modo:
\[ g(x, y) =
  \begin{cases}
    1 \text{ se } f_y(x) \neq \bot \\
    0 \text{ altrimenti}
  \end{cases}
\]
e che quindi nessuna \TM può decidere se, per una generica \TM \(M\) e un generico ingresso \(x\), \(M\) si arresta in uno stato finale \textit{(denotato come \(f_y(x) = \bot\))} con l'ingresso \(x\).

La dimostrazione avverrà tramite diagonalizzazione, dopo l'introduzione della tecnica stessa nella prossima Sezione.

\subsubsection{Dimostrazioni per diagonalizzazione}

Il ragionamento originale per diagonalizzazione fu usato da \textit{Georg Cantor} nel 1891 per dimostrare che \(\mathbb{R}\) ha una cardinalità maggiore di \(\mathbb{N}\).
È un metodo spesso usato per mostrare l'indecidibilità di alcuni problemi famosi e si riferisce a uno schema comune: si cerca una contraddizione quando esiste un insieme di funzioni \(A \rightarrow B\) indicizzato su \(A\).
Se \(f_k\) è una funzione indicizzata da \(k \in A\), un ragionamento per diagonalizzazione è dato dall'analisi degli elementi sulla diagonale della tabella ottenuta enumerando le \(f_k\), come nella Tabella~\ref{tab:dimostrazione-per-diagonalizzazione} (in cui la diagonale è evidenziata in \textbf{bianco su grigio}).

\begin{table}[htbp]
  \bigskip
  \centering
  \begin{tabular}{c | c c c c c}
    n          & 1                        & 2                        & 3                        & 4                        & \(\ldots\) \\ \hline
    \(f_1\)    & \whiteongray{\(f_1(1)\)} & \(f_1(2)\)               & \(f_1(3)\)               & \(f_1(4)\)               & \(\ldots\) \\
    \(f_2\)    & \(f_2(1)\)               & \whiteongray{\(f_2(2)\)} & \(f_2(3)\)               & \(f_2(4)\)               & \(\ldots\) \\
    \(f_3\)    & \(f_3(1)\)               & \(f_3(2)\)               & \whiteongray{\(f_3(3)\)} & \(f_3(4)\)               & \(\ldots\) \\
    \(f_4\)    & \(f_4(1)\)               & \(f_4(2)\)               & \(f_4(3)\)               & \whiteongray{\(f_4(4)\)} & \(\ldots\) \\
    \(\vdots\) & \(\vdots\)               & \(\vdots\)               & \(\vdots\)               & \(\vdots\)               & \(\ddots\) \\
  \end{tabular}
  \bigskip
  \caption{Dimostrazione per diagonalizzazione}
  \label{tab:dimostrazione-per-diagonalizzazione}
\end{table}

Allora si definisce l'elemento diagonale come una funzione \(d: A \rightarrow B\) che differisce dalla diagonale in ogni valore.
Poiché \(d\) è una funzione \(A \rightarrow B\), essa appare nella lista.

Visto che essa differisce anche da ogni elemento della lista sulla diagonale, la dimostrazione è avvenuta \textbf{per assurdo}.

\subsubsection{Dimostrazione della cardinalità del continuo}

La dimostrazione avviene per assurdo ed è strutturata in passi:

\begin{enumerate}
  \item Si supponga, per assurdo, che l'intervallo \([0, 1]\) sia numerabile
  \item Ciò implica che gli elementi di \([0, 1]\) possano essere messi in corrispondenza biunivoca con i numeri naturali, tramite successione di numeri reali \(\{n_1, n_2, \ldots\}\) che esaurisce tutti i numeri reali compresi tra \(0\) e \(1\)
  \item È possibile rappresentare ciascun numero della successione in forma decimale e visualizzare la successione di numeri reali come una matrice infinita che avrà circa questo aspetto:
        \[\begin{matrix}
            n_1 = 0, & 3 & 1 & 3 & 5 & 6 & 3 & 1 & \ldots \\
            n_2 = 0, & 6 & 8 & 1 & 3 & 9 & 8 & 7 & \ldots \\
            n_3 = 0, & 1 & 6 & 6 & 5 & 7 & 1 & 1 & \ldots \\
            n_4 = 0, & 1 & 3 & 6 & 6 & 5 & 5 & 8 & \ldots \\
            n_5 = 0, & 8 & 5 & 2 & 4 & 1 & 8 & 4 & \ldots \\
            n_6 = 0, & 4 & 1 & 2 & 4 & 2 & 5 & 7 & \ldots \\
            n_7 = 0, & 7 & 8 & 8 & 4 & 6 & 6 & 3 & \ldots \\
          \end{matrix}\]
  \item Si osservino ora le cifre lungo la diagonale della matrice, cioè sulla successione il cui \(k\)-esimo elemento è la \(k\)-esima cifra decimale di \(r_k\) come mostra la figura:
        \[\begin{matrix}
            n_1 = 0, & \whiteongray{3} & 1               & 3               & 5               & 6               & 3               & 1               & \ldots \\
            n_2 = 0, & 6               & \whiteongray{8} & 1               & 3               & 9               & 8               & 7               & \ldots \\
            n_3 = 0, & 1               & 6               & \whiteongray{6} & 5               & 7               & 1               & 1               & \ldots \\
            n_4 = 0, & 1               & 3               & 6               & \whiteongray{6} & 5               & 5               & 8               & \ldots \\
            n_5 = 0, & 8               & 5               & 2               & 4               & \whiteongray{1} & 8               & 4               & \ldots \\
            n_6 = 0, & 4               & 1               & 2               & 4               & 2               & \whiteongray{5} & 7               & \ldots \\
            n_7 = 0, & 7               & 8               & 8               & 4               & 6               & 6               & \whiteongray{3} & \ldots \\
          \end{matrix}\]
        Questa successione di cifre sulla diagonale, vista come un'espansione decimale, definisce un numero reale (che in questo caso può corrispondere a \(0.3866153 \ldots\)).
  \item Si consideri ora un nuovo numero reale \(x\) che abbia tutte le cifre differenti dalla sequenza sulla diagonale, definendolo come:
        \begin{itemize}
          \item se la \(k\)-esima cifra decimale di \(n_k\) è \(5\), allora la \(k\)-esima cifra di \(x\) è \(4\)
          \item se la \(k\)-esima cifra decimale di \(n_k\) non è \(5\), allora la \(k\)-esima cifra di \(x\) è \(5\)
        \end{itemize}
  \item All inizio della dimostrazione si era supposto che la lista \(\{n_1, n_2, \ldots\}\) enumerasse tutti i numeri reali compresi tra \(0\) e \(1\), quindi dovrebbe essere verificato che \(n_k = x_k\) per un qualche \(k\) e poiché \(x\) non ha dei \(9\) tra le cifre decimali, la sua rappresentazione è unica e sarà presente nella riga \(k\)
  \item A questo punto emerge la contraddizione: sia \(a\) la \(n\)-esima cifra decimale di \(n_k = x\). Essa può essere \(4\) o \(5\): per la definizione, \(a\) potrà essere \(4\) se e solo se è uguale a \(5\), e \(5\) negli altri casi. Questo è impossibile e ne segue che l'ipotesi di partenza è falsa.
\end{enumerate}

\subsubsection{Dimostrazione del Problema di Terminazione}
\label{sec:dimostrazione-problema-terminazione}

Come già annunciato, la dimostrazione avviene tramite tecnica diagonale.
Si assuma, per assurdo, che

\begin{minipage}{0.95\textwidth}
  \bigskip
  \begin{minipage}[]{0.55\textwidth}
    \[ g(y, x) =
      \begin{cases}
        1 \text{ se } f_y(x) \neq \bot & \quad \textit{ termina}    \\
        0 \text{ altrimenti}           & \quad \textit{non termina}
      \end{cases}
    \]
  \end{minipage}
  \begin{minipage}[]{0.35\textwidth}
    \begin{verbatim}
      int g(y, x) {
        if (halts(f_y(x)))
          return 1;

        return 0;
      }
    \end{verbatim}
  \end{minipage}
  \bigskip
\end{minipage}

\textit{(descritta sia matematicamente che in pseudocodice)} sia computabile.

Partendo da \(g\) si definisce ora una funzione \(h\) tale che:

\begin{minipage}{0.95\textwidth}
  \bigskip
  \begin{minipage}[]{0.55\textwidth}
    \[ h(x) =
      \begin{cases}
        1 \text{ se } g(x, x) = 0 & \quad f_x(x) \textit{ termina}      \\
        \bot \text{ altrimenti}   & \quad  f_x(x) \textit{ non termina}
      \end{cases}\]
  \end{minipage}
  \begin{minipage}[]{0.35\textwidth}
    \begin{verbatim}
      int h(x) {
        if (g(x, x) == 0)
          return 1;

        while (1) {};
      }
    \end{verbatim}
  \end{minipage}
  \bigskip
\end{minipage}

dove \(h(x) = 1 \Rightarrow g(x, x) = 0,\ f_x(x) = \bot\).
L'indefinizione non implica che la funzione \(h\) non sia calcolabile, ma significa che la funzione calcolata dalla corrispondente \TM non termina.

Usando \(h\), ci si trova quindi nella \textit{diagonale} della tabella delle \(f_y(x)\).

\bigskip
Se \(g\) fosse computabile, allora anche \(h\) lo sarebbe.
Analogamente, se \(h\) fosse computabile, allora si verificherebbe che \(h(i) = f_i(i)\) per una qualche \(i\).

Si calcoli ora \(h(i)\) ricordando che \(f_x(i) = h(i)\) e applicando la definizione appena data:
\[ h(i) =
  \begin{cases}
    1 \Rightarrow f_i(i) \neq \bot \Rightarrow g(i, i) = 0 \Rightarrow f_i(i) = \bot       & \textbf{\color{BrickRed}{assurdo}} \\
    \bot \Rightarrow f_i(i) = \bot \Rightarrow g(i, i) = \bot \Rightarrow f_i(i) \neq \bot & \textbf{\color{BrickRed}{assurdo}}
  \end{cases}
\]
Quindi entrambi i casi sono inammissibili e la dimostrazione è terminata.

\subsubsection{Lemma 1}

Sia \(h^\prime\) la funzione definita come:
\[ h^\prime(x) =
  \begin{cases}
    1 \text{ se } f_x(x) \neq \bot \\
    0 \text{ altrimenti }
  \end{cases}
\]
Allora \(h^\prime\) \textbf{non è computabile.}

Si noti che \(h^\prime(x)\) \inlinequote{si muove} ancora sulla diagonale (coincide con \(g(x, x)\)) ed è quindi una semplificazione di \(f(x)\).
È un problema analogo al problema di terminazione, di cui è appunto un lemma, ma non dipende da esso.

\paragraph[Dimostazione del Lemma 1]{Dimostrazione del Lemma \(1\)}

La dimostrazione segue la traccia della dimostrazione del Teorema di Terminazione, nel avvenuta nella Sezione~\ref{sec:dimostrazione-problema-terminazione}.

\bigskip
Si definisca inizialmente la funzione

\[ h(x) =
  \begin{cases}
    1 \text{ se } k(x) = 0 \\
    \bot \text{ altrimenti }
  \end{cases}
\]
Si osservi ora che \(h\) è computabile se \(k\) è computabile.
Una volta ottenuta una \TM \(M\) in grado di calcolare \(k\), sono sufficienti solo semplici modifiche per adattarla al calcolo di \(h\).

Considerando ora \(x_0\) il numero di Gödel di una terminata \TM \(M_{x_0}\) che calcola \(h\), ossia \(h = f_{x_0}\) e quindi \(h\) è la funzione calcolata dalla \(x_0\)-esima \TM.

Si verifica facilmente, tuttavia, che \(h(x_0)= 1\) implica \(f_{x_0}(x_0) = 1\), mentre \(h(x_0) = \bot\) implica \(h(x_0) = 1\).

Quindi \(f_{x_0}(x_0) = h(x_0) = 1\), che è una contraddizione.

\subsubsection{Lemma 2}

La funzione \(h^\prime(x)\) è un caso particolare della funzione \(g(y, x)\) perché:
\begin{itemize}
  \item \(h^\prime(x) = g(x, y)\)
  \item \(g\) non è computabile, perché \(g(y, x)\) è essa stessa non computabile
\end{itemize}

Se un problema non è risolvibile, allora un suo caso speciale può essere risolvibile.
Contrariamente, la generalizzazione di un problema non risolvibile è necessariamente non risolvibile.

Infine, se un problema è risolvibile, allora una sua generalizzazione potrebbe non essere risolvibile, mentre qualunque sua specializzazione è certamente risolvibile.

\subsubsection{Lemma 3}

Sia \(k(y)\) la funzione definita come:
\[ k(y) =
  \begin{cases}
    1 \text{ se } \forall \, x \in \mathbb{N}, \ f_y(x) \neq \bot \\
    0 \text{ altrimenti }
  \end{cases}
\]
Allora \(k(y)\) \textbf{non è computabile.}

Questa è una quantificazione su tutti i possibili dati in ingresso.
In qualche caso potrebbe essere possibile stabilire se \(f_y(x) \neq \bot\) per un qualche \(x\) specifico, ma non sarà mai possibile per ogni \(x \in \mathbb{N}\) \textit{(perché è un insieme infinito)}.

Se ciò, \textit{per assurdo}, fosse possibile, determinare se \(f_y\) è una funzione totale sarebbe impossibile.

\paragraph[Dimostrazione del Lemma 3]{Dimostrazione del Lemma \(3\)}

La dimostrazione segue la traccia della dimostrazione del Teorema di Terminazione, nel avvenuta nella Sezione~\ref{sec:dimostrazione-problema-terminazione}.

Per ipotesi, sia \(k(y)\) la funzione definita come:
\[ k(y) =
  \begin{cases}
    1 \text{ se } \forall \, x \in \mathbb{N}, \ f_y(x) \neq \bot \\
    0 \text{ altrimenti }
  \end{cases}
\]
Sempre per ipotesi, \(k(y)\) è \textbf{totale} e \textbf{computabile}.

Allora sarà possibile definire \(g(x) = w\), con \(w\) pari al numero di Gödel della \(x\)-esima \TM (definita nell'insieme \(\mathscr{E}\)) che calcola una funzione totale.

Se \(k\) fosse veramente computabile e totale, allora lo sarebbe anche \(g\).
Infatti:

\begin{enumerate}
  \item Sarebbe possibile calcolare \(k(0),\ k(1),\ \ldots\)
  \item Sia \(w_0\) il primo valore tale per cui \(k(w_0) = 1\)
  \item Sia \(g(0) = w_0\)
\end{enumerate}

La procedura è algoritmica, e siccome esistono infinite funzioni totali, \(g(x)\) è sicuramente definita per ogni \(x \in \mathbb{N}\), quindi è \textbf{totale} e \textbf{strettamente monotona}, perché \(w_{x+1} > w_x\).

Di conseguenza, anche \(g^{-1}\) è strettamente monotona ma non totale, perché \(g^{-1}\) è definita solo se \(w\) è il numero di Gödel di una funzione totale.

Allora si definisce la funzione \(h(x)\) come
\[ h(x) = f_{g(x)} + 1 = f_w(x) + 1 \]
e poiché \(f_w(x)\) è computabile e totale, anche \(h(x)\) lo sarà.

Per qualche valore di \(w_i\), infine, si verifica \(h(x) = f_{w_i}(x)\) e poiché \(h\) è totale, \(g^{-1}(w_i) = x_i \Rightarrow g^{-1}(w_i) \neq \bot\).

Confrontando gli ultimi due valori ottenuti, si giunge alla conclusione che:

\begin{align*}
  h(x_i) = f_{g(x_i)}(x_i) + 1 = f_{w_i}(x_1) + 1 \\
  h(x) = f_{w_i}(x) \Rightarrow h(x_i) = f_{w_i}(x_1)
\end{align*}

Cadendo quindi in una evidente contraddizione.

\subsection{Osservazioni sulla risolvibilità}

È importante notare che sapere che un problema è risolvibile non implica che il metodo di risoluzione sia noto.
In matematica è spesso necessario dare dimostrazioni non costruttive: si mostra che un oggetto matematico esiste, ma non lo si calcola.

In questo caso:
\begin{itemize}
  \item Un problema è \textbf{risolvibile} se esiste una \TM che lo risolve
  \item Per alcuni problemi si può concludere che esiste una \TM che li risolve, \textbf{senza tuttavia poterla costruire}
\end{itemize}

\subsection{Problema di decisione}

Un \textbf{problema di decisione} è una domanda che ha due possibili risposte: \textcolor{ForestGreen}{\texttt{SI}} o \textcolor{BrickRed}{\texttt{NO}}.
La domanda \textit{(e la consecutiva risposta)} sono relative a un qualche particolare ingresso.

\bigskip
\textit{Esempi} di problemi di decisione:
\begin{itemize}
  \item Dato un grafo \(G\) e un insieme di vertici \(K\), quest'ultima è una cricca \textit{(clique)}?
  \item Dato un grafo \(G\) e un insieme di lati \(M\), quest'ultimo è un albero ricoprente \textit{(spanning tree)}?
  \item Dato un insieme di assiomi, un insieme di regole e una formula, è possibile dimostrare quest'ultima a partire dai primi due? \textit{problema di incompletezza}
\end{itemize}

\subsubsection{Decidibilità}

Un problema si dice \textbf{decidibile} se esiste un algoritmo \textit{(procedura di decisione)} tale che:

\begin{itemize}
  \item È un procedimento \textbf{automatico} \textit{(da definizione di algoritmo)}
  \item Se il problema è \textbf{vero} per un determinato ingresso, allora ritorna \textcolor{ForestGreen}{\texttt{SI}}
  \item Se il problema è \textbf{falso} per un determinato ingresso, allora ritorna \textcolor{BrickRed}{\texttt{NO}}
\end{itemize}

\subsubsection{Semidecidibilità}

Un problema si dice \textbf{semidecidibile} se esiste un algoritmo \textit{(procedura di semidecisione)} tale che:

\begin{itemize}
  \item È un procedimento \textbf{automatico} \textit{(da definizione di algoritmo)}
  \item Se il problema è \textbf{vero} per un determinato ingresso, allora ritorna \textcolor{ForestGreen}{\texttt{SI}}
  \item Negli altri casi, \textbf{potrebbe non terminare mai}
\end{itemize}

Il problema della terminazione (Sezione~\ref{sec:problema-della-terminazione}) è \textbf{indecidibile} e \textbf{semidecidibile} allo stesso tempo.

\paragraph{Problema della terminazione e semidecidibilità}

È dimostrato che il problema della terminazione (Sezione~\ref{sec:problema-della-terminazione}) è \textbf{semidecidibile}.
Si consideri infatti l'analoga formulazione \(\exists \, z \, | \, f_x(z) \neq \bot\).

\textit{Schema di dimostrazione}:
\begin{itemize}
  \item Se si calcola \(f_x(0)\) e ne risulta che \(f_x(0) \neq \bot\) allora la risposta è \texttt{SI}
  \item Se la computazione non termina, come è possibile accorgersene?
  \item Dimostrazione con metodo diagonale:
        \begin{enumerate}
          \item Si simuli 1 passo di esecuzione di \(f_x(0)\). Se termina, allora la risposta è \texttt{SI}
          \item Altrimenti, si simuli un passo di computazione di \(f_x(1)\). Se termina, allora la risposta è \texttt{SI}
          \item Ancora una volta si simuli 2 passi di \(f_x(0)\), 1 passo di \(f_x(2)\), 2 passi di \(f_x(1)\), 3 di \(f_x(0)\) e così via, secondo lo schema in Figura~\ref{fig:associazione-biunivoca-2} \textit{(già vista nella Sezione relativa alla \nameref{sec:macchina-turing-universale})}
        \end{enumerate}
\end{itemize}

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.75]{image-15.tikz}
  \caption{Illustrazione dell'associazione biunivoca}
  \label{fig:associazione-biunivoca-2}
  \bigskip
\end{figure}

\paragraph{Osservazioni}

Grande parte dei problemi indecidibili è \textbf{anche} semidecidibile.
Un tipico esempio di questa affermazione è data dagli errori a \textit{runtime} nei programmi.

Si nota che in questi casi il problema semidecidibile è dato dalla ricerca della presenza dell'errore, non nella dimostrazione della sua assenza.
Ciò porta a una importante implicazione sulla verifica basata sul testing.
Secondo Dijkstra:

\indentquote{Il testing può dimostrare la presenza di errori e non la loro assenza.}

Molti problemi, seppur ben definiti, non possono essere risolti mediante procedimenti algoritmici.
Il risultato sarà trovato tramite tecniche differenti.

Allo stesso modo esistono problemi la cui soluzione algoritmica è nota, senza che il procedimento stesso sia noto.

\subsection{Insiemi ricorsivi}
\label{sec:insiemi-ricorsivi}

In questa sezione ci si dedicherà a studiare i sottoinsiemi di \(\mathbb{N}\), che verranno indicati d'ora in poi con la lettera \(S\).
Dati \(x \in \mathbb{N}, S \subseteq \mathbb{N}\), si vuole studiare l'appartenenza dell'elemento \(x\) allo stesso \(S\).

Questa formalizzazione del problema è del tutto generale perché applicabile a qualsiasi insieme che si possa mettere in corrispondenza biunivoca ed effettiva con \(\mathbb{N}\), ossia per tutti gli elementi numerabili.

\subsubsection{Funzione caratteristica di un insieme}

Dato un insieme \(S\), la sua \textbf{funzione caratteristica} \(c_s : \mathbb{N} \rightarrow \{0, 1\}\) è definita come:
\[
  c_s = \begin{cases}
    1 \text{ se } x \in S \\
    0 \text{ altrimenti }
  \end{cases}
\]
L'appartenenza di un elemento a un insieme e la ricorsività dello stesso è un problema la cui risolvibilità dipende dalla computabilità della sua funzione caratteristica.

\subsubsection{Definizione di insieme ricorsivo}

Un insieme \(S\) viene detto \textbf{ricorsivo} (oppure \textit{decidibile})  se e solo se la sua funzione caratteristica \(c_s\) è \textbf{computabile}.

Si ricordi che, per definizione, \(c_s\) è \textbf{totale} per ogni insieme \(S\).
Infatti, dato un qualsiasi elemento \(x \in \mathbb{N}\), esso appartiene (se \(c_1=1\)) o non appartiene (se \(c_s = 0\)) a \(S\).

\subsection{Insieme ricorsivamente enumerabile}
\label{sec:insieme-ricorsivamente-enumerabile}

Un insieme viene detto \textbf{ricorsivamente enumerabile} (oppure \textit{semidecidibile}) se e solo se è l'\textbf{insieme vuoto} oppure è l'\textbf{immagine di una funzione} totale e computabile \(g_s\), cioè:
\[ S = I_{g_s} = \left\{ x \, | \, \exists \, y, \ y \in \mathbb{N} \land x = g_s(y)  \right\} \]
Gli insiemi ricorsivamente enumerabili devono il loro nome al fatto che il problema dell'appartenenza può essere risolto \textbf{meccanicamente} \textit{(oppure algoritmicamente)}.
Un calcolatore meccanico in grado di implementare la funzione caratteristica di un insieme fornirà necessariamente una risposta alla domanda \(x \in S \ \forall \, x\).
Gli insiemi ricorsivamente enumerabili sono una definizione \textit{più debole} degli insiemi \textit{ricorsivi}.

\bigskip
Il termine \textit{“semidecidibile”} puo essere spiegato intuitivamente come:

\indentquote{se si suppone che \(x \in S\), allora enumerando gli elementi di \(S\) anche \(x\) verrà trovato e sarà possibile rispondere alla domanda. Ma se \(x \notin S\)?}

\subsection[Teorema 1/2 + 1/2 = 1]{Teorema \(\frac{1}{2} + \frac{1}{2} = 1\)}

Sia \(S\) un insieme.
Allora:

\begin{enumerate}[label=\Alph*), ref=(\Alph*)]
  \item \label{enum:teorema-0.5-0.5-1-a} Se \(S\) è \textbf{ricorsivo}, è anche \textbf{ricorsivamente enumerabile}
  \item \label{enum:teorema-0.5-0.5-1-b} \(S\) è \textbf{ricorsivo} se e solo se sia \(S\) che \(\overline{S} = \mathbb{N} - S\) sono \textbf{ricorsivamente enumerabili}
\end{enumerate}

\textbf{Corollario}: la classe di insiemi decidibili \textit{(linguaggi, problemi, \dots)} è chiusa rispetto al complemento.

\subsubsection{Dimostazione punto \ref{enum:teorema-0.5-0.5-1-a}}

Sia \(c_s\) la funzione caratteristica di \(S\).
Se \(S\) è vuoto, allora è ricorsivamente enumerabile per definizione.
Altrimenti, esiste almeno un elemento \(k \in S\).

Sia \(g_s(x)\) la funzione definita come:
\[ g_s(x) = \begin{cases}
    x \text{ se } c_s(x) = 1 \\
    k \text{ altrimenti }
  \end{cases}
\]

Allora \(g_s(x)\) è totale, computabile e \(I_{g_s} = S\).
Quindi poiché \(S\) \'e l'immagine di una funzione totale e computabile, è ricorsivamente enumerabile per definizione.

\bigskip
Questa è una definizione \textbf{costruttiva}.
Infatti, non è definito né se \(S = \emptyset\) né l'algoritmo per trovare un \(k\) specifico.
Viene definita solo l'esistenza di \(g_s\) per \(S \neq \emptyset\).

\subsubsection{Dimostazione punto \ref{enum:teorema-0.5-0.5-1-b}}

\textbf{Dimostrazione che se \(S\) è ricorsivo, è anche ricorsivamente enumerabile.}

Se \(S\) è vuoto o coincide con \(\mathbb{N}\), allora l'enunciato è ovvio.

Altrimenti, esistono due funzioni totali e computabili \(g_s, g_{\overline{s}}\) tali che \(S = I_{g_s}, \overline{s} = I_{g_{\overline{s}}}\) nella forma:

\begin{align*}
  S            & = \left\{ g_s(0), g_s(1), \ldots \right\}                           \\
  \overline{S} & = \left\{ g_{\overline{s}}(0), g_{\overline{s}}(1), \ldots \right\}
\end{align*}

Per costruzione
\[ S \cup \overline{S} = \mathbb{N} \quad S \cap \overline{S} = \emptyset \]
e quindi:
\[ \forall \, x \, \in \mathbb{N} \, (\exists \, y \, | \, x = g_s(y) \lor x = g_{\overline{s}} \land \lnot (\, \exists \, z, w \, | \, x = g_s(z) \land x = g_{\overline{s}}(w) )) \]
cioè ogni \(x\) appartiene in modo esclusivo a \(S\) o a \(\overline{S}\).

\bigskip
Si consideri ora l'enumerazione:
\[ \mathscr{E}({L}) = \left\{ g_s(0), g_{\overline{s}}(0),  g_s(1), g_{\overline{s}}(1), \ldots \right\} \]
Poiché \(\mathscr{E} \equiv \mathbb{N}\), \(\forall \, x, x \in \mathscr{L}\) e, se \(x\) compare in \(\mathscr{L}\) in posizione parti, allora non compare mai in posizione dispari e viceversa.

Si definisca \(c_s\) come:
\[
  c_{s_i} = \begin{cases}
    1 \text{ se } x = s_i \in \mathscr{L} \text{ e } i = 2\cdot k + 1 \\
    0 \text{ altrimenti }
  \end{cases}
\]
Chiaramente \(c_s\) è ben definito, poiché \(x\) potrà apparire in \(\mathscr{L}\) solo in posizione o pari o dispari (e non entrambe).

Inoltre risulta totale e computabile, in quanto l'enumerazione \(\mathscr{L}\) può essere effettuata da una qualche \TM.
Basta infatti cercare se \(x\) (elemento dato) appare in posizione pari o dispari di \(\mathscr{L}\) per concludere se appartiene o meno a \(S\).

\bigskip
\textbf{Dimostrazione che se \(S\) è ricorsivo, allora anche \(\overline{S}\) lo è.}

Per definizione, la funzione caratteristica di \(\overline{S}\) è definita come:
\[  g_{\overline{s}}(x) = \begin{cases}
    1 \text{ se } c_s(x) = 1 \\
    0 \text{ altrimenti }
  \end{cases}  \]

Quindi in virtù del punto~\ref{enum:teorema-0.5-0.5-1-b} del teorema appena dimostrato, sia \(S\) che \(\overline{S}\) sono ricorsivamente enumerabili.

\subsubsection{Osservazioni di interesse pratico}

Si consideri l'insieme \(S\) con le seguenti caratteristiche:

\begin{itemize}
  \item \(i \in S \rightarrow f_i\) totale (cioè \(S\)) contiene solo indici di funzione \textbf{totali} e \textbf{computabili}
  \item \(f\) è \textbf{totale} e \textbf{computabile}, \(\exists \, i \in S \, | \, f_i = f\) e quindi \(S\) contiene ogni \(i\)
\end{itemize}

\(S\) è quindi l'insieme di indici di funzioni totali e computabili e non è ricorsivamente enumerabile.

Questa affermazione è dimostrabile per diagonalizzazione.
Quindi non esiste un formalismo ricorsivamente enumerabile che possa definire tutte le funzioni computabili e totali e solo quelle.
Gli \FSA possono definire solo alcune \textit{(non tutte)} funzioni totali, mentre le \TM definiscono tutte le funzioni computabili, anche quelle non totali.

Il linguaggio \texttt{C} permette di codificare qualunque algoritmo, ma anche quelli che non terminano.
Non esiste nessun sottoinsieme del linguaggio \texttt{C} che definisca esattamente tutti i programmi che terminano.

\bigskip
La relazione tra insiemi di problemi è illustrata nella Figura~\ref{fig:relazione-tra-insiemi-di-problemi}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-16.tikz}
  \caption{Relazione tra insiemi di problemi}
  \label{fig:relazione-tra-insiemi-di-problemi}
  \bigskip
\end{figure}

\subsection{Teoremi di \textit{Kleene} e \textit{Rice}}

\subsubsection{Teorema del punto fisso di \textit{Kleene}}

Sia \(t\) una funzione \textbf{totale} e \textbf{computabile}.
Allora è sempre possibile trovare un intero \(p\) tale che:
\[ f_p = f_{t(p)} \]

La funzione \(f_p\) è detta \textbf{punto fisso} di \(t\) perché \(t\) trasforma \textit{(una macchina che calcola)} \(f_p\) in \textit{(una macchina che calcola)} \(f_p\) stessa.

\paragraph{Dimostrazione del Teorma di \textit{Kleene}}

Sia \(u\) un qualsiasi numero naturale.
Si definisca un \TM che realizza la seguente procedura applicata al valore in ingresso \(x\):

\begin{enumerate}
  \item Calcola \(z = f_u(u)\)
  \item Quando la computazione di \(f_u(u)\) si ferma, se ciò avviene, calcola \(f_z(x)\) che non è detto che sia definito
\end{enumerate}

Poiché la procedura precedente è effettiva, esiste una \TM in grado di implementarla.
Inoltre è possibile costruire tale macchina e calcolare poi il suo indice \(g(u) \, \forall \, u\), usando la numerazione \(\mathscr{G}\) introdotta nella Sezione~\ref{sec:enumerazione-TM-UTM}.

\subsubsection{Teorema di \textit{Rice}}

Sia \(F\) un insieme qualunque funzioni \textbf{computabili}.

L'insieme \(S = \left\{ x \, | \, f_x \in F \right\}\) degli indici di \TM che calcolano le funzioni di \(F\) è \textbf{decidibile} (e quindi \textbf{ricorsivo}) se e solo se \(F = \emptyset\) oppure \(F\) è l'insieme di \textbf{tutte} le funzioni computabili.

Di conseguenza in tutti i casi non banali \(S\) \textbf{non è decidibile}.

\paragraph{Dimostrazione del Teorema di \textit{Rice}}

Per assurdo, si supponga che \(S\) sia ricorsivo, che \(F \neq \emptyset\) e che \(F\) non coincida con l'insieme di tutte le funzioni computabili.

Si consideri ora la funzione caratteristica \(c_s\) di \(S\) definita come:
\[ c_s(x) = \begin{cases}
    1 \text{ se } f_x \in F \\
    0 \text{ altrimenti }
  \end{cases} \]
Per ipotesi assurda, si supponga che \(c_s\) sia computabile.

Sempre per ipotesi, enumerando effettivamente tutte le \TM \(M_i\) si può trovare:
\begin{enumerate}[label=\alph*), ref=(\Alph*)]
  \item \label{enum:dimostrazione-rice-1} Il primo \(i \in S\) per cui \(f_i \in F\)
  \item \label{enum:dimostrazione-rice-2} Il primo \(j \notin S\) per cui \(f_j \notin S\)
  \item \label{enum:dimostrazione-rice-3} La funzione \(\overline{c}_s(x)\), computabile e totale, definita come
        \[ \overline{c}_s = \begin{cases}
            i \text{ se } f_x \notin F \\
            j \text{ altrimenti }
          \end{cases} \]
  \item \label{enum:dimostrazione-rice-4} Per il teorema di Kleene, un \(\overline{x}\) tale che \(f_{\overline{c}_s(\overline{x})} = f_{\overline{d}}\)
\end{enumerate}

\bigskip
Supponendo che \(\overline{c}_s (\overline{x}) = i\), per \ref{enum:dimostrazione-rice-3}, segue che \(f_{\overline{x}} \notin F\).
Tuttavia, per \ref{enum:dimostrazione-rice-4}, si ha che \(f_{\overline{x}}\) e per \ref{enum:dimostrazione-rice-1} si ha che \(f_i \in F\), una \textbf{contraddizione}.

Supponendo invece che \(\overline{c}_s (\overline{x}) = k\), per \ref{enum:dimostrazione-rice-4} si ha che \(f_{\overline{x}} = f_j\).
Tuttavia, per \ref{enum:dimostrazione-rice-3} si ha che \(f_{\overline{x}} = f_j\) e, per \ref{enum:dimostrazione-rice-2} si ha che \(f_j \notin F\), ottenendo ancora una \textbf{contraddizione}.

\paragraph{Conseguenze}

Il teorema di \textit{Rice} ha un forte impatto pratico negativo.

Ad esempio, ponendo \(f = \{g\}\), quindi un insieme composto dalla sola funzione \(g\), per il teorema \textbf{non è decidibile} se una generica \TM calcoli \(g\) o meno.
Tuttavia, per la tesi di \textit{Church} (Sezione~\ref{sec:tesi-di-church}) il risultato non è ristretto alle \TM e al formalismo delle funzioni.
Non è quindi possibile stabilire algoritmicamente se un dato algoritmo sia in grado di risolvere un determinato problema o se due programmi siano equivalenti \textit{(cioè se calcolano la stessa funzione)}.

Inoltre, non è possibile stabilire se un algoritmo risolve un problema che goda di una qualsiasi proprietà non banale \textit{(e quindi una proprietà che non sia appartenenza ad una categoria non esistente o coincidente con tutti i problemi risolvibili)}.

\subsection{Riduzione di problemi}

Un problema \(P^\prime\) è \textbf{ridotto} ad un problema \(P\) se un algoritmo per risolvere \(P\) viene usato per risolvere \(P^\prime\).

Quindi \(P\) è \textbf{risolvibile} ed esiste un algoritmo che, per ogni data istanza di \(P^\prime\):

\begin{enumerate}
  \item \textbf{Determina} una corrispondente istanza di \(P\)
  \item \textbf{Costruisce} algoritmicamente la soluzione dell'istanza di \(P^\prime\) dalla soluzione dell'istanza di \(P\)
\end{enumerate}

\textit{Formalmente}:

\begin{enumerate}
  \item Si vuole verificare se \(x \in S\)
  \item Si è in grado di verificare se \(y \in S^\prime\)
  \item Se esiste una funzione \(t\), computabile e totale tale che \[ x \in S \Leftrightarrow t(x) \in S^\prime \] allora è possibile determinare algoritmicamente se \(x \in S\)
\end{enumerate}

\bigskip
\textit{Esempio}: si supponga che il problema di cercare un elemento in un insieme sia risolvibile.
Allora si può risolvere anche il problema di calcolare l'intersezione tra due sistemi, attuando una riduzione.

\subsubsection{Implicazione della riduzione}

Il procedimento della riduzione funziona anche al contrario:

\begin{itemize}
  \item Si vuole sapere se è possibile risolvere \(x \in S\)
  \item Si sa che non è possibile risolvere \(y \in S^\prime\) (quindi \(S^\prime\) \textit{non è decidibile})
  \item Se esiste una funzione \(t\), computabile e totale, tale che \[ y \in S^\prime \Leftrightarrow t(y) \in S \] allora è possibile concludere che \(x \in S\) non è decidibile
\end{itemize}

In conclusione, è immediato dedurre che:

\begin{itemize}
  \item Se \(P^\ast\) è \textbf{risolvibile}, lo è anche \(P\)
  \item Se \(P\) è \textbf{irrisolvibile}, lo è anche \(P^\ast\)
\end{itemize}

\clearpage

\section{Complessità del calcolo}

Fin'ora è stato affrontato la determinazione della computazione di un problema, senza che venisse però analizzato il vero \inlinequote{costo} della soluzione.

Si supponga infatti di avere accesso al più potente super calcolatore al mondo.
Anche sfruttando tutta la sua capacità di calcolo, alcuni problemi \textit{(come la determinazione della partita perfetta a scacchi)} può richiedere una quantità di tempo superiore all'intera vita dell'universo \textit{(e si dimostra che questa tesi corrisponde alla realtà)}.

Di conseguenza, alcuni problemi vengono definiti \textbf{intrattabili}, perché pur esistendo un algoritmo per giungere alla sua soluzione, la sua esecuzione richiederebbe un tempo \inlinequote{inaccettabile}.
L'intrattabilità costituisce una grossa limitazione alle applicazioni pratiche, in quanto esistono parecchi problemi \inlinequote{interessanti} che sono anche intrattabili.

Il concetto di \textbf{trattabilità}, è strettamente legato a quello della \textbf{complessità}, che può essere vista come la misura del \inlinequote{costo} della risoluzione: all'aumentare della prima, il secondo lieviterà.
L'obiettivo di questa Sezione sarà definire in modo formale la nozione e lo studio del costo della soluzioni dei problemi, per poi essere in grado di progettare e combinare algoritmi e strutture dati in modo da realizzare soluzioni efficienti e non solo corrette.

\subsection{Analisi della complessità}

La tecnica che prende il nome di \textbf{analisi della complessità} consiste nel costruire strumenti per valutare la complessità di un calcolo, per poter successivamente analizzare algoritmi e strutture di nati notevoli.

Essi verranno sottoposti ad analisi quantitative su:

\begin{itemize}
  \item Tempo di calcolo impiegato - \textit{complessità temporale}
  \item Spazio di memoria occupato - \textit{complessità spaziale}
\end{itemize}

L'analisi si limiterà a criteri di costo \textbf{oggettivi} e formalizzabili, quindi non terrà conto di parametri come i costi di sviluppo e \textit{tradeoff} tra obiettivi contrastanti.

\bigskip
Per la Tesi di Church \textit{(Sezione~\ref{sec:tesi-di-church})}, un problema è calcolabile indipendentemente dallo strumento usato, purché esso sia Turing completo.

Questa affermazione non è valida per la complessità di calcolo: non è verosimile affermare che modelli di calcolo differenti impieghino lo stesso tempo per eseguire programmi analoghi.
Infatti, poiché non esiste una \inlinequote{Tesi di Church per la complessità}, sarà necessario costruire uno strumento in grado di valutare la complessità temporale e spaziale di un calcolo che tralasci \inlinequote{considerazioni superflue} e che sia utilizzabile per la maggioranza dei modelli di calcolo.

Contrariamente a quanto appena affermato, nonostante l'inesistenza di tale teorema, sarà possibile correlare in modo sistematico soluzioni proposte per modelli diversi.
Poiché non esiste un formalismo di calcolo perfettamente adatto a costruire il modello, l'analisi avverrà a partire dalle \TM deterministiche.

\subsection{Complessità temporale e spaziale}

Sia \(M\) una \TM \textbf{deterministica} con \(k\) nastri e sia \(c_0 \vdash^\ast c_r\) una computazione su \(M\).
Allora è possibile definire:

\begin{itemize}
  \item La \textbf{complessità temporale} come:
        \begin{itemize}
          \item \(T_M(x) = r\) se \(M\) termina in \(c_r\)
          \item \(\infty\) se \(M\) non termina
          \item poiché \(M\) è deterministica, la computazione sarà \textbf{unica} sull'ingresso \(x\)
        \end{itemize}
  \item La \textbf{complessità spaziale} come:
        \begin{itemize}
          \item \(\displaystyle S_M(x) = \sum^k_{j=1} \max_{i \in \{0, \ldots, r\}} \left(|\alpha_{ij}|\right)\)
                \begin{itemize}[label=\(\rightarrow\)]
                  \item \(\left(|\alpha_{ij}|\right)\) indica il contenuto del \(j\)-esimo nastro alla \(i\)-esima mossa
                  \item la complessità è pari alla somma delle quantità massime di nastro occupate per ogni nastro
                \end{itemize}
          \item la complessità \textbf{spaziale} è sempre minore della complessità \textbf{temporale}
                \begin{itemize}[label=\(\rightarrow\)]
                  \item \(\forall \, x \ \dfrac{S_M(x)}{k} \leq T_M(X)\)
                \end{itemize}
        \end{itemize}
\end{itemize}

In queste definizioni tuttavia si tiene conto della natura dell'ingresso \(x\), mentre risulta più conveniente studiare la complessità di un algoritmo in funzione della \textbf{dimensione} di \(x\).
Quindi si semplifica lo studio di \(T_M(x)\) ed \(S_M(x)\) ponendo \(n = |x|\) \textit{(la dimensione di \(x\))} e studiando \(T_M(n)\) e \(S_M(n)\).

Questa semplificazione implica une perdita di generalità perché ingressi diversi, seppur di dimensioni uguali, potrebbero avere complessità diverse.
\textit{Formalmente}:

\begin{align*}
  |x_1| = |x_2| \nRightarrow & \ T_M(x_1) = T_M(x_2) \\
  |x_1| = |x_2| \nRightarrow & \ T_S(x_1) = T_S(x_2)
\end{align*}

\bigskip
Per gestire la variabilità dell'ingresso così introdotta, è necessario distinguere tra:
\begin{itemize}[itemsep=5pt]
  \item \textbf{Caso pessimo} \(T_M(n) = \displaystyle \max_{n = |x|} \left(T_M(x)\right) \)
  \item \textbf{Caso ottimo} \(T_M(n) = \displaystyle \min_{n = |x|} \left(T_M(x)\right) \)
  \item \textbf{Caso medio} \(T_M(n) = \dfrac{\displaystyle \sum_{n = 0}^{|x|} T_M(x)}{|I|^n} \)
        \begin{itemize}[label=\(\rightarrow\)]
          \item somma dei tempi per parole lunghe \(n\) \textit{diviso} il numero di parole lunghe \(n\)
          \item è una media pesata
        \end{itemize}
\end{itemize}

Tipicamente, si considera il caso pessimo, perché è normalmente il più rilevante \textit{(fornisce un limite superiore)} e l'analisi risulta più semplice che nel caso medio \textit{(che dovrebbe tenere conto di ipotesi probabilistiche sulla distribuzione dei dati, complicando ulteriormente l'analisi stessa)}.

\subsubsection[Crescita di T(n) e S(n)]{Crescita di \(T_M(n)\) e \(S_M(n)\)}

Subito ci si può accorgere di come i valori esatti di \(T_M(n)\) e \(S_M(n)\) non siano particolarmente utili ai fini dello studio della complessità di un particolare problema.
Ciò che risulta rilevante, infatti, è il comportamento \textbf{asintotico} delle funzioni di costo, ossia quando \(n \rightarrow \infty\).

Questa è una semplificazione \inlinequote{aggressiva}, perché porta a perdere la distinzione tra casi del tipo

\begin{align*}
  T_M(n) & = n^4 - 5n^2 + 2    \\
  T_M(n) & = 10^{80} \cdot n^4
\end{align*}

entrambi \textit{ridotti} a \(n^4\), seppur con andamenti radicalmente diversi per valori finiti.

\bigskip
Si introducono quindi tre notazioni per indicare il comportamento asintotico di una funzione:
\begin{itemize}
  \item \bigO-grande: limite asintotico \textbf{superiore}
  \item \(\Omega\)-grande: limite asintotico \textbf{inferiore}
  \item \(\Theta\)-grande: limite asintotico sia \textbf{superiore} che \textbf{inferiore}
\end{itemize}

L'uso di questa notazione (tramite gli insiemi \(\bigO,\ \Omega,\ \Theta\)) permette di evidenziare con facilità la parte più importante di una funzione di complessità.

Questo modello tuttavia presenta delle limitazioni:

\begin{itemize}
  \item Potrebbe non essere sufficientemente accurato per valori \textbf{piccoli} di \(n\)
        \begin{itemize}
          \item la notazione \(\Omega\)-grande è più precisa rispetto alla notazione \bigO-grande per i valori piccoli di \(n\)
        \end{itemize}
  \item Algoritmi con complessità asintotica maggiore possono essere più veloce di uno a complessità minore per valori \textbf{piccoli} di \(n\)
\end{itemize}

Per ovviare a questi problemi, si faranno uso di tutte e tre le notazioni in base ai casi presi in analisi.

\subsubsection[Notazione O-grande]{Notazione \bigO-grande}

\textbf{Definizione}: siano \(f(n),\ g(n)\) funzioni \(\mathbb{N} \rightarrow \mathbb{R}^+\).
Allora \(\bigO(g(n))\) è l'insieme di funzioni definito come:

\[ \bigO(g(n)) = \left\{ f(n) \, | \, \exists \, c > 0, n_0 > 0 \text{ tali che } \forall \, n > n_0, \ 0 \leq f(n) \leq c \cdot g(n) \right\} \]

\bigskip
\textit{Informalmente}, le funzioni in \(\bigO(g(n))\) sono \textit{dominate} da \(c \cdot g(n)\) a partire da \(n_0\).
Un'illustrazione di questa relazione è rappresentata in Figura~\ref{fig:notazione-o-grande}.

\bigskip
Per brevità, è comune scrivere \(f(n) = \bigO(g(n))\) al posto della più corretta notazione \(f(n) \in \bigO(g(n))\).

\subsubsection[Notazione Omega-grande]{Notazione \(\Omega\)-grande}

\textbf{Definizione}: siano \(f(n),\ g(n)\) funzioni \(\mathbb{N} \rightarrow \mathbb{R}^+\).
Allora \(\Omega(g(n))\) è l'insieme di funzioni definito come:

\[ \Omega(g(n)) = \left\{ f(n) \, | \, \exists \, c > 0, n_0 > 0 \text{ tali che } \forall \, n > n_0, \ 0 \leq c \cdot g(n) \leq f(n)  \right\} \]

\bigskip
\textit{Informalmente}, le funzioni in \(\Omega(g(n))\) \textit{dominano}  \(c \cdot g(n)\) a partire da \(n_0\).
Un'illustrazione di questa relazione è rappresentata in Figura~\ref{fig:notazione-omega-grande}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{minipage}[b]{0.45\textwidth}
    \centering
    \tikzfig[0.5]{image-20.tikz}
    \caption{Notazione \bigO-grande}
    \label{fig:notazione-o-grande}
  \end{minipage}
  \begin{minipage}[b]{0.45\textwidth}
    \centering
    \tikzfig[0.5]{image-21.tikz}
    \caption{Notazione \(\Omega\)-grande}
    \label{fig:notazione-omega-grande}
  \end{minipage}
  \bigskip
\end{figure}

\subsubsection[Notazione Theta-grande]{Notazione \(\Theta\)-grande}

\textbf{Definizione}: siano \(f(n),\ g(n)\) funzioni \(\mathbb{N} \rightarrow \mathbb{R}^+\).
Allora \(\Theta(g(n))\) è l'insieme di funzioni definito come:

\[ \Theta(g(n)) = \left\{ f(n) \, | \, \exists \, c_1 > 0, c_2 > 0, n_0 > 0 \text{ tali che } \forall \, n > n_0, \ 0 \leq c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n)  \right\} \]

\bigskip
\textit{Informalmente}, le funzioni in \(\Theta(g(n))\) \textit{sono comprese} tra  \(c_1 \cdot g(n)\) e \(c_2 \cdot g(n)\) a partire da \(n_0\).
Un'illustrazione di questa relazione è rappresentata in Figura~\ref{fig:notazione-theta-grande}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.5]{image-22.tikz}
  \caption{Notazione \(\Theta\)-grande}
  \label{fig:notazione-theta-grande}
  \bigskip
\end{figure}

\paragraph{Definzioni come limiti}

Se è vero che:
\[ \displaystyle \lim_{n \rightarrow \infty} \dfrac{f(n)}{g(n)} = c, \quad c \neq 0, \quad c \neq \infty \]
allora \(f(n) \in \Theta(g(n))\) e gli andamenti asintotici di \(f,\ g\) differiscono per una costante moltiplicativa.

\bigskip
Allo stesso modo, se è vero che:
\[ \displaystyle \lim_{n \rightarrow \infty} \dfrac{f(n)}{g(n)} = 0 \]
allora \(f(n) \in \bigO(g(n)), \ f(n) \notin \Theta(g(n))\) e \(f\) cresce più velocemente di \(g\).

Alternativamente si usa la notazione \(\Theta(f(n)) < \Theta(g(n))\).

\subsubsection[Proprietà di O, Omega, Theta]{Proprietà di \(\bigO,\ \Omega,\ \Theta\)}

\begin{itemize}
  \item Per definizione:
        \begin{itemize}
          \item \(f \in \Theta(g) \Leftrightarrow f \in \bigO(g) \lor f \in \Omega(g)\)
        \end{itemize}
  \item Proprietà \textbf{transitiva}
        \begin{itemize}
          \item se \(f(n) = \Theta(g(n)),\ g(n) = \Theta(h(n)) \Rightarrow f(n) = \Theta(h(n))\)
          \item se \(f(n) = \bigO(g(n)),\ g(n) = \bigO(h(n)) \Rightarrow f(n) = \bigO(h(n))\)
          \item se \(f(n) = \Omega(g(n)),\ g(n) = \Omega(h(n)) \Rightarrow f(n) = \Omega(h(n))\)
        \end{itemize}
  \item Proprietà \textbf{riflessiva}
        \begin{itemize}
          \item \(f(n) = \Theta(f(n))\)
          \item \(f(n) = \bigO(f(n))\)
          \item \(f(n) = \Omega(f(n))\)
        \end{itemize}
  \item \textbf{Simmetria}
        \begin{itemize}
          \item \(f(n) = \Theta(g(n)) \Leftrightarrow g(n) = \Theta(f(n))\)
        \end{itemize}
  \item \textbf{Simmetria trasposta}
        \begin{itemize}
          \item \(f(n) = \bigO(g(n)) \Leftrightarrow g(n) = \Omega(f(n))\)
        \end{itemize}
\end{itemize}

\subsubsection{Complessità dei modelli deterministici di calcolo}

Si vuole ora studiare la complessità dei modelli \textbf{deterministici} di calcolo studiati fin'ora.
Il risultato della trattazione è illustrato in Tabella~\ref{tab:complessita-modelli-calcolo}.

\begin{table}[htbp]
  \bigskip
  \centering
  \begin{tabular}{c|c|c|c}
    \textit{modello}              & \textit{complessità spaziale} & \textit{complessità temporale} & \textit{note}                               \\ \hline
    \FSA                          & \(S(n) \in \Theta(1)\)        & \(T(n) \in \Theta(n)\)         & \footnotesize più precisamente \(T(n) = n\) \\
    \PDA                          & \(S(n) \in \bigO(n)\)         & \(T(n) \in \Theta(n)\)                                                       \\
    \TM \textit{a nastro singolo} & \(S(n) \in \Theta(n)\)        & \(T(n) \in \Theta(n^2)\)       & \footnotesize meno efficienti dei \PDA      \\
  \end{tabular}
  \bigskip
  \caption{Complessità dei modelli di calcolo}
  \label{tab:complessita-modelli-calcolo}
\end{table}

\subsection{Teoremi di accelerazione lineare}

I quattro teoremi di accelerazione lineare e le loro dimostrazioni sono riportate nelle seguenti Sezioni, insieme a degli importanti risvolti pratici.

\subsubsection[Teorema 1]{Teorema \(1\)}
\label{sec:teorema-accelerazione-1}

\indentquote{Se \(L\) è accettato da una \TM \(M\) a \(k\) nastri in \(S_M(n)\), per ogni \(c \in \mathbb{R}^+\) è possibile costruire una \TM \(M^\prime\) a \(k\) nastri che accetta \(L\) con \(S_{M^\prime}(n) < c \cdot S_M(n)\)}

\paragraph[Dimostrazione Teorema 1]{Dimostrazione Teorema \(1\)}
\label{par:dimostrazione-teorema-accelerazione-1}

\textit{Schema della dimostrazione:}
\begin{enumerate}
  \item Si sceglie una fattore di compressione \(r\) tale che \(r \cdot c > 2\)
  \item Per ogni alfabeto \(\Gamma_i\) dell'\(i\)-esimo nastro di \(M\) si costruisce \(\Gamma^\prime_i\) dell'\(i\)-esimo nastro di \(M^\prime\) assegnando un elemento per ogni \(s \in \Gamma^r_i\)
  \item Si costruisce l'organo di controllo di \(M^\prime\) in modo tale per cui:
        \begin{itemize}
          \item calcoli i nuovi simboli sui nastri emulando le mosse di \(M\) spostando le testine sui nastri ogni \(r\) movimenti di \(M\)
          \item memorizzi la posizione della testina arricchendo ulteriormente gli alfabeti di nastro \(\Gamma_i\) \textbf{oppure} arricchendo l'insieme degli stati
        \end{itemize}
\end{enumerate}

\subsubsection[Teorema 2]{Teorema \(2\)}
\label{sec:teorema-accelerazione-2}

\indentquote{Se \(L\) è accettato da una \TM \(M\) a \(k\) nastri in \(S_M(n)\), è possibile costruire una \TM \(M^\prime\) a \(1\) nastro che accetta \(L\) con \(S_{M^\prime}(n) = S_M(n)\), concatenando tutti i nastri su una solo}

\subsubsection[Teorema 3]{Teorema \(3\)}

\indentquote{Se \(L\) è accettato da una \TM \(M\) a \(k\) nastri in \(S_M(n)\), per ogni \(c \in \mathbb{R}^+\) è possibile costruire una \TM \(M^\prime\) a \(1\) nastro che accetta \(L\) con \(S_{M^\prime}(n) < S_M(n)\)}

Il risultato è analogo a quello del \nameref{sec:teorema-accelerazione-2}, con in aggiunta la compressione già vista nel \nameref{sec:teorema-accelerazione-1}.

\subsubsection[Teorema 4]{Teorema \(4\)}

\indentquote{Se \(L\) è accettato a una \TM \(M\) a \(k\) nastri in \(T_M(n)\), per ogni \(c \in \mathbb{R}^+\), è possibile costruire una \TM \(M^\prime\) a \(k+1\) nastri che accetta \(L\) con \(T_{M^\prime}(n) = \max(n + 1, c \cdot T_M(n))\)}

Il risultato è analogo a quello del \nameref{sec:teorema-accelerazione-2}, con in aggiunta la compressione già vista nel \nameref{sec:teorema-accelerazione-1}.

\paragraph[Dimostrazione Teorema 4]{Dimostrazione Teorema \(4\)}

L'approccio della dimostrazione è analogo a quello già visto in \ref{par:dimostrazione-teorema-accelerazione-1}

\begin{enumerate}
  \item Si codifica in modo compresso i simboli dell'alfabeto di \(M\)
  \item Poiché la compressione avviene a \textit{runtime} il caso ottimale è \(T_{M^\prime}(n) = n\)
  \item Comprimendo \(r\) simboli in \(1\) nel caso pessimo servono \(3\) mosse di \(M^\prime\) per emularne \(r+1\) di \(M\)
\end{enumerate}

\subsubsection{Conseguenze pratiche}

Questi teoremi portano alle seguenti conseguenze pratiche:

\begin{itemize}
  \item Lo schema di dimostrazione usato per le \TM vale anche per il modello di calcolatore di Von Neumann
        \begin{itemize}[label=\(\rightarrow\)]
          \item È possibile avere accelerazioni lineari arbitrariamente grandi \textbf{aumentando il parallelismo fisico}
        \end{itemize}
  \item Miglioramenti più che lineari nel tempo di calcolo possono essere ottenuti solo \textbf{cambiando algoritmo}
        \begin{itemize}[label=\(\rightarrow\)]
          \item concepire ed utilizzare algoritmi \textbf{efficienti} è molto più efficiente di procedere di \textit{forza bruta}
        \end{itemize}
  \item Un calcolatore è in grado di eseguire operazioni aritmetiche su tipi a dimensione finita in tempo costante, mentre la \TM richiede di propagare gli effetti al singolo \texttt{bit}, uno alla volta
        \begin{itemize}[label=\(\rightarrow\)]
          \item il calcolatore opera su un alfabeto più vasto, di dimensione \(|I|=2^w\), con \(w\) dimensione della parola
          \item un calcolatore può accedere direttamente ad una cella di memoria, mentre una \TM impiega \(\Theta(n)\), con \(n\) pari alla distanza di quest'ultima dalla testina
        \end{itemize}
\end{itemize}

\subsection{Macchina \RAM}

L'introduzione della macchina \RAM consente di fare un passo per arrivare ad un livello di astrazione più vicino alla realtà.
È infatti un modello classico, ispirato all'architettura di \textit{Von Neumann}.

La\textbf{ macchina \RAM} è dotata di un nastro di \textbf{lettura} (\texttt{IN}) e uno di \textbf{scrittura} (\texttt{OUT}), similarmente ad una \TM.
Il programma è cablato all'interno dell'organo di controllo tramite una serie \textit{(finita)} di istruzioni in un certo linguaggio.
Esse \textbf{non sono alterabili} durante il funzionamento della macchina.
L'indice dell'istruzione da eseguire in ogni dato momento è indicato dal \textit{program counter}.

L'accesso alla memoria avviene tramite indirizzamento diretto, in cui ogni cella (contenente un intero) viene letta o scritta dalla \textit{unità aritmetica}. Ogni cella di memoria è quindi associata ad un intero e definita come \(\texttt{N}[n],\ n \in \mathbb{N}\).
Grazie alla sua struttura, l'accesso ad una cella di memoria non implica uno scorrimento delle stesse.
Le istruzioni del programma caricato nell'organo di controllo usano come primo operando sorgente o destinazione il primo elemento della memoria, \(\texttt{N}[0]\), detto \textbf{accumulatore}.

\bigskip
La struttura di una macchina \RAM è mostrata nella Figura~\ref{fig:struttura-macchina-ram}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-23.tikz}
  \caption{Struttura di una macchina \RAM}
  \label{fig:struttura-macchina-ram}
  \bigskip
\end{figure}

\bigskip

I programmi all'interno dell'unità di controllo sono codificati tramite una semantica simile al codice \texttt{Assembly}, seppur di molto semplificato.
Viene definita una specifica dei comandi (un \textit{Instruction Set}) che permette operazioni matematiche, logiche e di salto.

Un'esempio di tale linguaggio è mostrato nella Tabella~\ref{tab:instruction-set-macchina-RAM}.

\begin{table}[htbp]
  \bigskip
  \centering
  \begin{minipage}[t]{.3\textwidth}
    \scalebox{0.8}{
      \centering
      \begin{tabular}[t]{ll}
        \textit{istruzione} & \textit{semantica}                                                       \\ \hline
        \texttt{LOAD X}     & \(\texttt{N}[0] \leftarrow \texttt{N}[\texttt{X}]\)                      \\
        \texttt{LOAD= X}    & \(\texttt{N}[0] \leftarrow \texttt{X}\)                                  \\
        \texttt{LOAD* X}    & \(\texttt{N}[0] \leftarrow \texttt{N}[\texttt{N}[\texttt{X}]]\)          \\
        \texttt{STORE X}    & \(\texttt{N}[\texttt{X}] \leftarrow \texttt{N}[0]\)                      \\
        \texttt{STORE* X}   & \(\texttt{N}[\texttt{N}[\texttt{X}]] \leftarrow \texttt{N}[0]\)          \\
        \texttt{ADD X}      & \(\texttt{N}[0] \leftarrow \texttt{N}[0] + \texttt{N[\texttt{X}]}\)      \\
        \texttt{SUB X}      & \(\texttt{N}[0] \leftarrow \texttt{N}[0] - \texttt{N[\texttt{X}]}\)      \\
        \texttt{MUL X}      & \(\texttt{N}[0] \leftarrow \texttt{N}[0] \times \texttt{N[\texttt{X}]}\) \\
        \texttt{DIV X}      & \(\texttt{N}[0] \leftarrow \texttt{N}[0] \div \texttt{N[\texttt{X}]}\)   \\
      \end{tabular}
    }
  \end{minipage}
  \begin{minipage}[t]{.3\textwidth}
    \scalebox{0.8}{
      \centering
      \begin{tabular}[t]{ll}
        \textit{istruzione} & \textit{semantica}                                                                   \\ \hline
        \texttt{ADD= X}     & \(\texttt{N}[0] \leftarrow \texttt{N}[0] + \texttt{X}\)                              \\
        \texttt{ADD* X}     & \(\texttt{N}[0] \leftarrow \texttt{N}[0] + \texttt{N}[\texttt{N}[\texttt{X}]]\)      \\
        \texttt{SUB= X}     & \(\texttt{N}[0] \leftarrow \texttt{N}[0] - \texttt{X}\)                              \\
        \texttt{SUB* X}     & \(\texttt{N}[0] \leftarrow \texttt{N}[0] - \texttt{N}[\texttt{N}[\texttt{X}]]\)      \\
        \texttt{MULT= X}    & \(\texttt{N}[0] \leftarrow \texttt{N}[0] \times \texttt{X}\)                         \\
        \texttt{MULT* X}    & \(\texttt{N}[0] \leftarrow \texttt{N}[0] \times \texttt{N}[\texttt{N}[\texttt{X}]]\) \\
        \texttt{DIV= X}     & \(\texttt{N}[0] \leftarrow \texttt{N}[0] \div \texttt{X}\)                           \\
        \texttt{DIV* X}     & \(\texttt{N}[0] \leftarrow \texttt{N}[0] \div \texttt{N}[\texttt{N}[\texttt{X}]]\)   \\
      \end{tabular}
    }
  \end{minipage}
  \begin{minipage}[t]{.3\textwidth}
    \scalebox{0.8}{
      \centering
      \begin{tabular}[t]{ll}
        \textit{istruzione} & \textit{semantica}                                                  \\ \hline
        \texttt{READ X}     & \(\texttt{N}[\texttt{X}] \leftarrow \texttt{IN}\)                   \\
        \texttt{READ* X}    & \(\texttt{N}[\texttt{N}[\texttt{X}]] \leftarrow \texttt{IN}\)       \\
        \texttt{WRITE X}    & \(\texttt{OUT} \leftarrow \texttt{N}[\texttt{X}]\)                  \\
        \texttt{WRITE= X}   & \(\texttt{OUT} \leftarrow \texttt{X}\)                              \\
        \texttt{WRITE* X}   & \(\texttt{OUT} \leftarrow \texttt{N}[\texttt{N}[\texttt{X}]]\)      \\
        \texttt{JUMP L}     & \(\texttt{PC} \leftarrow \texttt{L}\)                               \\
        \texttt{JZ L}       & \(\texttt{PC} \leftarrow \texttt{L} \text{ se } \texttt{N}[0] = 0\) \\
        \texttt{JGZ L}      & \(\texttt{PC} \leftarrow \texttt{L} \text{ se } \texttt{N}[0] > 0\) \\
        \texttt{JLZ L}      & \(\texttt{PC} \leftarrow \texttt{L} \text{ se } \texttt{N}[0] < 0\) \\
        \texttt{HALT}       & \texttt{-}                                                          \\
      \end{tabular}
    }
  \end{minipage}
  \bigskip
  \caption{Instruction set della macchina \RAM}
  \label{tab:instruction-set-macchina-RAM}
\end{table}

\subsubsection{Criterio di costo logaritmico}

L'approssimazione di costo tramite \(\Omega, \Theta\) funziona ma presenta molti limiti.

Infatti non si tiene conto di una limitatezza della memoria: una singola parola di memoria non può sempre contenere in un solo simboli tutti gli interi usati in un determinato algoritmo.
Poiché non si considera il numero di cifre necessarie a rappresentare un numero, il calcolo della complessità può risultare molto sbagliato.

Per effettuare un'approssimazione più simile alla realtà, è necessario tenere conto del numero di cifre necessarie a rappresentare un intero.
Così facendo risulterà che caricare e salvare interi in memoria non sarà più a costo costante (ma varierà in funzione della lunghezza del numero), con effetti analoghi sulle operazioni algebriche.

Alla luce di questi ragionamenti, sarà possibile calcolare la complessità per ogni categoria di operazione.

\begin{minipage}[htbp]{.99\textwidth}
  \bigskip
  \begin{itemize}
    \item \textbf{Copiare}, \textbf{spostare}, \textbf{scrivere}, \textbf{leggere} un intero \(i\) avrà complessità pari al suo numero di cifre in base \(b\):
          \begin{itemize}[label=\(\rightarrow\)]
            \item \(log_b(i) = \Theta(\log_2(i))\)
            \item con \(b = 2\) il costo corrisponde al numero di \texttt{bit} usati per rappresentare \(i\)
          \end{itemize}
    \item Il costo delle operazioni \textbf{aritmetiche} e \textbf{logiche} elementari dipende dall'operazione. Ponendo \(d = \log_2(i)\):
          \begin{itemize}[label=\(\rightarrow\)]
            \item \textbf{addizioni}, sottrazioni saranno \(\Theta(d)\)
            \item \textbf{divisioni} e moltiplicazioni con \textbf{metodo scolastico} \(\Theta(d^2)\)
            \item \textbf{divisioni} e moltiplicazioni con \textbf{algoritmi più efficienti} \(\Theta(n \log(n))\)
          \end{itemize}
    \item Operazioni di \textbf{salto}:
          \begin{itemize}[label=\(\rightarrow\)]
            \item il costo è \textbf{costante}, quindi \(\Theta(1)\)
          \end{itemize}
  \end{itemize}
  \bigskip
\end{minipage}

\subsubsection{Scelta del criterio di costo}

La scelta del criterio di costo (tra logaritmico o costante) dipende da una serie di fattori.
Infatti, se:

\begin{itemize}
  \item L'elaborazione \textbf{non altera l'ordine di grandezza} dei dati di ingresso
  \item La memoria allocata inizialmente \textbf{non varia durante l'esecuzione} del programma
        \begin{itemize}
          \item la memoria \textbf{non dipende} dai dati
          \item di conseguenza \textbf{una singola cella è elementare} e con essa le operazioni relative
          \item la dimensione di ogni singolo elemento in ingresso \textbf{non varia significativamente} nel tempo
        \end{itemize}
\end{itemize}

allora il criterio di costo costante è adeguato.
In caso contrario è indispensabile il criterio di costo logaritmico, l'unico in grado di approssimare con correttezza il costo della funzione.

\bigskip
La conseguenza immediata di questa differenza di complessità implica che con macchine diverse lo stesso algoritmo può dare luogo a complessità diverse.
Non è possibile identificare un modello \inlinequote{migliore} tra i vari formalismi studiati e non esiste un'analogo alla Tesi di Church (Sezione~\ref{sec:tesi-di-church}), però è possibile stabilire una relazione di maggiorazione a priori tra le complessità dei diversi modelli di calcolo.
Essa prende il nome di \textit{\nameref{sec:tesi-correlazione-polinomiale}}.

\subsection{Tesi di correlazione polinomiale}
\label{sec:tesi-correlazione-polinomiale}

La Tesi di correlazione polinomiale, in analogia con la Tesi di Church, prova una relazione tra le complessità di diverse modelli di automi.
Essa afferma che:

\indentquote{Se un problema è risolvibile mediante il modello \(\mathscr{M}_1\) con complessità spaziale o temporale \(C_1(n)\), allora è risolvibile da un qualsiasi altro modello Turing completo \(\mathscr{M}_2\) con complessità \(C_2(n) \leq \pi \left(C_1(n)\right),\text{ con } \pi \text{ polinomio }\)}

\subsubsection{Dimostrazione della Tesi di correlazione polinomiale}

L'obiettivo di questa sezione sarà dimostrare la Tesi di correlazione polinomiale \textit{(che d'ora in poi prenderà quindi la definizione di Teorema)}.

Essa sarà strutturata in due parti, che mostreranno rispettivamente la simulazione di una \TM tramite \RAM e viceversa.

\paragraph[Simulazione di una \TM a k nastri tramite \RAM]{Simulazione di una \TM a \(k\) nastri tramite \RAM}

Verrà ora dimostrato il funzionamento della simulazione delle azioni una \TM a \(k\) nastri tramite macchina \RAM.

Inizialmente, sarà necessario compiere questi passi per inizializzare la memoria della \RAM

\begin{itemize}
  \item La \TM viene mappata sulla \RAM:
        \begin{itemize}
          \item lo stato della \TM verrà posto sulla prima cella di memoria della \RAM
          \item verrà usata una cella di memoria della \RAM per ogni cella del nastro
          \item la memoria rimanente viene divisa in blocchi da \(4\) celle
        \end{itemize}
  \item I blocchi vengono riempiti con la seguente strategia:
        \begin{itemize}
          \item Il blocco \(0\) conterrà le posizioni delle \(k\) testine
          \item I blocchi \(n\)-esimi, \(n > 0\), conterranno l'\(n\)-esimo simbolo di ognuno dei \(k\) nastri
        \end{itemize}
  \item La \RAM emula la lettura di un carattere sotto la testina con un accesso indiretto, usando l'indice contenuto nel blocco \(0\)
\end{itemize}

Una volta inizializzata la memoria, sarà possibile simulare propriamente le azioni della \TM.
In particolare:

\begin{itemize}
  \item Lettura dei nastri
        \begin{itemize}
          \item lettura del blocco \(0\) e dello stato (\(\Theta(k)\) mosse)
          \item lettura dei valori sui nastri in corrispondenza delle testine (\(\Theta(k)\) accessi indiretti)
        \end{itemize}
  \item Scrittura dei nastri
        \begin{itemize}
          \item scrittura dello stato (\(\Theta(1)\))
          \item scrittura della celle dei nastri (\(\Theta(k)\) accessi indiretti)
          \item scrittura nel blocco \(0\) per aggiornare le posizioni delle \(k\) testine (\(\Theta(k)\) mosse)
        \end{itemize}
\end{itemize}

Per valutare la complessità, bisogna considerare che la \RAM emula una mossa della \TM con \(\Theta(k)\) mosse:

\begin{itemize}
  \item \(T_{RAM}(n) = \Theta\left(T_{\mathscr{M}}(n)\right)\) per il costo costante
  \item \(T_{RAM}(n) = \Theta\left(T_{\mathscr{M}}(n)\right) \log\left(T_{\mathscr{M}}(n)\right)\) per il costo logaritmico
\end{itemize}

\paragraph[Simulazione di una \RAM nastri tramite \TM a k nastri]{Simulazione di una \RAM tramite \TM a \(k\) nastri}

In questo paragrafo verrà dimostrata la simulazione di una \RAM tramite \TM a \(k\) nastri, omettendo le operazioni \texttt{MUL} e \texttt{DIV} per semplicità.

\begin{itemize}
  \item Un nastro di memoria della \TM viene inizializzato come in Figura~\ref{fig:inizializzazione-memoria-ram}
        \begin{itemize}
          \item le celle di memoria indicizzate da \(i\) sono state coinvolte da almeno una operazione di \texttt{STORE}
          \item il simbolo \(\$\) è usato come delimitatore tra indice della cella e contenuto della cella
          \item il simbolo \(\$\$\) è usato per separare tra di loro le celle di memoria
        \end{itemize}
  \item Un secondo nastro di memoria ospita il contenuto dell'accumulatore \(\texttt{N}[0]\)
        \begin{itemize}
          \item il valore viene memorizzato con codifica binaria
        \end{itemize}
  \item Un terzo nastro viene utilizzato come memoria temporanea
        \begin{itemize}
          \item viene usato per spostare i dati qualora sia necessario memorizzare \(\texttt{N}[i_j]\) laddove \(\texttt{N}[i_k],\ \texttt{N}[i_l],\ i_k < i_j < i_l\)
        \end{itemize}
\end{itemize}

\begin{figure}
  \bigskip
  \centering
  \tikzfig{image-24.tikz}
  \caption{Inizializzazione della memoria \RAM}
  \label{fig:inizializzazione-memoria-ram}
  \bigskip
\end{figure}

Una volta inizializzata la memoria, sarà possibile simulare propriamente le azioni della macchina \RAM.
In particolare:

\begin{itemize}
  \item L'operazione \texttt{LOAD x} avverrà come:
        \begin{enumerate}
          \item ricerca di \texttt{x} sul nastro principiale
          \item copia  di dati letti sulla cella corrispondente a \(\texttt{N}[0]\) usando il nastro di supporto
        \end{enumerate}
  \item L'operazione \texttt{STORE x} avverrà come:
        \begin{enumerate}
          \item ricerca di \texttt{x} sul nastro principiale
          \item se esso non viene trovato, è necessario creare dello spazio usando il nastro di servizio \textit{(se necessario) }
          \item valore di \(\texttt{N}[0]\) viene salvato in \(\texttt{N}[\texttt{x}]\)
        \end{enumerate}
  \item L'operazione \texttt{ADD x} avverrà come:
        \begin{itemize}
          \item ricerca di \texttt{x}
          \item copia di \(\texttt{N}[x]\) sul nastro di supporto
          \item calcolo della somma
          \item scrittura del valore risultante in \(\texttt{N}[0]\)
        \end{itemize}
\end{itemize}

In generale, la simulazione di una mossa della \RAM richiede alla \TM un numero di mosse \(n\) inferiore alla lunghezza del nastro principale per una costante opportuna.
In simboli:
\[ n \leq c \cdot \text{ lunghezza nastro principale } \]

\subsubsection{Lemma della Tesi di correlazione polinomiale}

In seguito all'enunciazione e alla dimostrazione del Teorema di correlazione polinomiale, risulta che:

\indentquote{Lo spazio occupato sul nastro principale è \(\bigO \left(T_\text{RAM}(n)\right)\)}

\paragraph{Dimostrazione del Lemma}

\begin{itemize}
  \item Ogni cella \(i_j\)-esima della \RAM occupa \(\log(i_j) + \log(\texttt{N}[i_j])\)
  \item Ogni cella della \RAM viene materializzata solo se la \RAM effettua una operazione di \texttt{STORE}
  \item L'operazione \texttt{STORE} ha un costo per la \RAM pari a \(\log(i_j) + \log(\texttt{N}[i_j])\)
  \item Per riempire \(r\) celle, la \RAM impiega \(\displaystyle \sum_{j=1}^{r} \log(i_j) + \log(\texttt{N}[i_j])\) unità di tempo, la stessa quantità di spazio occupata sul nastro
\end{itemize}

\subsubsection{Conclusioni sulla Tesi di correlazione}

A valle delle dimostrazioni, si può concludere che:

\begin{itemize}
  \item La \TM impiega al più \(\Theta(T_\text{RAM}(n))\) per simulare una mossa della \RAM
  \item Se la \RAM ha complessità \(T_\text{RAM}(n)\), essa effettua al più \(T_\text{RAM}(n)\) mosse (ogni mossa costa almeno \(1\))
  \item La simulazione completa della \RAM da parte della \TM costa al più \(\Theta\left((T_\text{RAM}(n))^2\right)\)
  \item Il legame tra \(T_\text{RAM}(n)\) e \(T_\text{TM}(n)\) è polinomiale
\end{itemize}

\bigskip
Inoltre, è possibile fare le seguenti osservazioni:

\begin{itemize}
  \item Per quanto grande possa essere il grado di un polinomio, sarà sempre inferiore ad una funzione esponenziale
        \begin{itemize}[label=\(\rightarrow\)]
          \item è sufficiente confrontare l'andamento delle funzioni \(n^k\) e \(2^n\)
        \end{itemize}
  \item Grazie al teorema di correlazione polinomiale è possibile parlare dei problemi risolvibili in tempo e/o spazio polinomiale astraendo dalla classe del calcolatore
  \item classe dei problemi trattabili in pratica \(\equiv\) classe dei problemi risolvibili in tempo polinomiale, detta \Pset
  \item I problemi di interesse pratico che sono in \Pset hanno anche grado del polinomio \inlinequote{accettabile}
\end{itemize}

\clearpage

\section{Algoritmi e Grafi}

L'obiettivo di questa sezione sarà introdurre i concetti di base dei grafi e degli algoritmi, per poi iniziare ad analizzare alcuni problemi di ordinamento.

\subsection{Grafi}

Un grafo e una coppia \(\langle V, E \rangle\) in cui \(V\) è un insieme finito di \textbf{nodi} (detti anche \textbf{vertici}) e \(E \subseteq V \times V\) è una relazione binaria \textit{(costituente un insieme finito)} su \(V\) di coppie di nodi (dette anche \textbf{archi}).

Se \(u\) e \(v\) sono nodi del grafo, la coppia \(\langle u, v \rangle\) è un arco.
L'arco è detto \textbf{orientato} se \(u\) e \(v\) sono coppie di nodi consecutivi, ovvero \(u = v + 1\) \textit{(Figura~\ref{fig:grafo-arco-orientato})}.
In caso contrario, è detto \textbf{non orientato} \textit{(Figura~\ref{fig:grafo-arco-non-orientato})}.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[t]{0.495\textwidth}
    \centering
    \begin{tikzpicture}[auto, on grid, node distance=3cm, >=Triangle]
      \node [state, minimum size=1cm](u) {\textit{u}};
      \node [state, minimum size=1cm](v) [right=of u] {\textit{v}};

      \path[->, thick]
      (u) edge [] node {} (v);

    \end{tikzpicture}
    \caption{Grafo con arco orientato}
    \label{fig:grafo-arco-orientato}
  \end{subfigure}
  \begin{subfigure}[t]{0.495\textwidth}
    \centering
    \begin{tikzpicture}[auto, on grid, node distance=3cm, >=Triangle]
      \node [state, minimum size=1cm](u) {\textit{u}};
      \node [state, minimum size=1cm](v) [right=of u] {\textit{v}};

      \path[-, thick]
      (u) edge [] node {} (v);

    \end{tikzpicture}
    \caption{Grafo con arco non orientato}
    \label{fig:grafo-arco-non-orientato}
  \end{subfigure}
\end{figure}

\subsubsection{Tipi di Grafo}

\begin{itemize}
  \item Se tutti gli archi di un grafo sono \textbf{orientati}, allora il grafo è detto \textbf{orientato}
        \begin{itemize}
          \item in caso contrario, è detto \textbf{non orientato}
        \end{itemize}
  \item n \textbf{cammino} è una sequenza di nodi \(v_0, v_1, \ldots, v_n\) tali che tra ogni coppia di nodi della sequenza \(\langle v_i, v_{i+1} \rangle\) esiste un arco
        \begin{itemize}
          \item i nodi \(v_0, v_1, \ldots, v_n\) appartengono tutti al cammino
          \item la lunghezza del cammino è data da \(n\) \textit{(il numero di vertici \(-1\))}
        \end{itemize}
  \item In un grafo \textbf{non orientato}, il cammino forma un \textbf{ciclo} se \(v_0 = v_n\)
        \begin{itemize}
          \item un grafo che non contiene cicli è detto \textbf{aciclico}
        \end{itemize}
  \item Un grafo \textbf{non orientato} è connesso se tra ogni coppia di vertici esiste un cammino.
\end{itemize}

\subsubsection{Alberi}

Un \textbf{albero} è un grafo connesso, aciclico, non orientato.
È detto \textbf{radicato} se un nodo viene indicato come \textbf{radice}.

Ogni nodo del grafico è \textbf{raggiungibile} dalla radice tramite un \textbf{cammino}.
Esso sarà \textbf{unico} in quanto il grafo è aciclico.
Gli ultimi nodi dei cammini dalla radice sono detti \textbf{foglie}.
Viene detta \textbf{altezza} dell'albero la distanza massima tra la radice e una sua foglia

\bigskip
Ogni \textbf{nodo} ha un \textbf{padre} \textit{(tranne la radice)} e uno o più \textbf{figli} \textit{(tranne le foglie)}.
Più precisamente vengono chiamati:

\begin{itemize}
  \item \textbf{Nodi interni}: tutti i nodi dei cammini tra la radice e le foglie
  \item \textbf{Profondità} di un nodo \(N\): la \textbf{distanza} di \(N\) dalla radice
  \item \textbf{Antenato} di un nodo \(N\):  ogni nodo che \textbf{precede} \(N\) sul cammino dalla radice a \(N\)
  \item \textbf{Padre} di un nodo \(N\): il nodo che \textbf{precede} immediatamente \(N\) lungo il cammino dalla radice a \(N\)
  \item \textbf{Figlio} di un nodo \(N\): ogni nodo che \textbf{succede} immediatamente \(N\) lungo il cammino dalla radice a \(N\)
  \item \textbf{Fratelli} di un nodo \(N\): ogni nodo che ha lo stesso padre di \(N\)
\end{itemize}

Infine un albero è detto \textbf{binario} se ogni nodo ha \textbf{al più} \(2\) figli.

\bigskip
Un esempio di albero è mostrato in Figura~\ref{fig:esempio-di-albero}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-26.tikz}
  \caption{Esempio di albero}
  \label{fig:esempio-di-albero}
  \bigskip
\end{figure}

\subsubsection{Heap}
\label{sec:heap}

Uno \textbf{heap binario} è un albero binario detto \textbf{quasi completo}.
Tutti i suoi livelli sono completi, tranne l'ultimo che potrebbe essere completo solo fino ad un certo punto da sinistra.
L'albero binario che deriva dall'interpretazione di un array come albero è quasi completo.

Un \textbf{max-heap} è uno heap tale che, per ogni nodo \(x\) dell'albero, il valore contenuto nel padre di \(x\) è \(\geq\) del contenuto di \(x\).
Considerando i singoli nodi, \(A\left(\left\lfloor\sfrac{i}{2}\right\rfloor\right) \geq A[i]\).

Nei \textbf{min-heap}, al contrario, il valore contenuto nel padre di \(x\) è \(\leq\) del contenuto di \(x\).

\bigskip
Un esempio di \textbf{max-heap} e la sua corrispondenza con un array è mostrata nella Figura~\ref{fig:esempio-di-max-heap}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.8]{image-27.tikz}
  \caption{Esempio di max heap}
  \label{fig:esempio-di-max-heap}
  \bigskip
\end{figure}

\subsection{Algoritmi}

\textit{Informalmente}, un \textbf{algoritmo} è una qualsiasi procedura ben definita che prende un valore \textit{(o un insieme di valori)} in input e restituisce un valore \textit{(o un insieme di valori)} in output, calcolato tramite una serie ben definita di operazioni.
L'algoritmo può essere visto anche come uno strumento per risolvere un particolare problema di computazione.

In particolare, si definiscono:

\begin{itemize}
  \item \textbf{Problema}: il compito da svolgere
        \begin{itemize}
          \item quali \textit{output} vogliamo ottenere a fronte di certi \textit{input}
          \item quali \textit{operazioni} devono essere effettuate (la funzione che deve svolgere)
        \end{itemize}
  \item \textbf{Algoritmo}: i passi da eseguire per risolvere un problema
        \begin{itemize}
          \item un algoritmo prende gli \textit{input} in ingresso ad un problema e li trasforma in opportuni \textit{output}
          \item è formato da una sequenza di \textit{operazioni concrete}
          \item è eseguibile da una \textbf{macchina}
          \item deve essere \textbf{corretto}
        \end{itemize}
\end{itemize}

Gli algoritmi possono essere descritti in diversi linguaggi, \textit{(come ad esempio, in \texttt{C}, \texttt{Java}, \texttt{Python}, \ldots)}.
La complessità di un algoritmo non varia significativamente in base al linguaggio in cui è implementato. Essa varierà solo di un fattore moltiplicativo.
All'interno del corso si userà lo \textbf{pseudocodice}, che assomiglia nella sintassi ad un linguaggio vero e proprio senza tuttavia esserlo.

\subsubsection{Pseudocodice}

La sintassi dello \textbf{pseudocodice} è la seguente:

\begin{itemize}
  \item \textbf{Assegnamento}: \texttt{i := j}
        \begin{itemize}
          \item assegnamento multiplo: \texttt{i := j := e}
          \item le variabili sono locali alla procedura
          \item è applicato da destra a sinistra
        \end{itemize}
  \item \textbf{Cicli} e \textbf{salti} condizionali: \texttt{while}, \texttt{for}, \texttt{if-then-else}
        \begin{itemize}
          \item la struttura a blocchi è determinata dall'indentazione
          \item non si usano le parentesi
        \end{itemize}
  \item \textbf{Commenti}: \texttt{// commento}
  \item \textbf{Array}: la notazione è analoga a quella del \texttt{C}
        \begin{itemize}
          \item il primo elemento può avere indice diverso da \(0\)
          \item \texttt{A[j]} è l'elemento \(j\) dell'array \texttt{A}
          \item \texttt{A[i..j]} è il sotto array che inizia dall'elemento \(i\) e termina all'elemento \(j\)
        \end{itemize}
  \item Gli \textbf{oggetti} sono strutture che raggruppano dati composti
        \begin{itemize}
          \item possiedono degli attributi, anche detti campi
          \item per accedere all'attributo \texttt{attr} dell'oggetto \texttt{x}, si usa \texttt{x.attr}
          \item gli \texttt{array} sono dati composti, quindi sono assimilabili ad oggetti
                \begin{itemize}
                  \item ogni array ha un attributo \texttt{length} che indica la lunghezza dell'array
                \end{itemize}
        \end{itemize}
  \item Una \textbf{variabile} che corrisponde ad un oggetto è un puntatore all'oggetto
        \begin{itemize}
          \item analogo a quanto avviene in \texttt{C} o in \texttt{Java}
          \item un puntatore che non fa riferimento ad un oggetto ha valore \texttt{NIL}
        \end{itemize}
  \item Il \textbf{passaggio dei parametri} avviene per valore
        \begin{itemize}
          \item la procedura invocata ottiene una copia dei parametri passati
          \item passando un oggetto come parametro alla procedura, si passa il puntatore all'oggetto
        \end{itemize}
\end{itemize}

\bigskip
Per la valutazione della complessità di algoritmi scritti in pseudocodice si userà il \textbf{criterio di costo costante}, poiché non verranno manipolati dati più grandi rispetto a quelli in ingresso.
Come conseguenza diretta, ogni istruzione \texttt{i} di pseudocodice verrà eseguita in un tempo costante \(c_\texttt{i}\)

Grazie a questa assunzione sarà possibile trascurare il modello di calcolo dello pseudocodice \textit{(che sarà la macchina \RAM)}.
La complessità temporale sarà studiata più di quella spaziale perché più rilevante nella realtà.

\subsubsection{Divide et Impera}
\label{sec:divide-et-impera}

La tecnica chiamata \textbf{Divide et Impera} è una tecnica algoritmica molto comune.
Essa si applica a problemi grossi e difficili da risolvere e si articola nei seguenti passi:

\begin{enumerate}
  \item \textbf{Divide} - divisione del problema in problemi più piccoli da risolvere
  \item \textbf{Impera} - risoluzione dei problemi più piccoli
  \item \textbf{Combina} - le soluzioni vengono combinate al fine di ottenere il risultato finale
\end{enumerate}

Dividendo a sufficienza i problemi, inevitabilmente si ottengono problemi risolubili senza che debbano essere divisi ulteriormente.
Questa è una tecnica di natura ricorsiva, in quanto per risolvere i problemi più piccoli si applica lo stesso algoritmo del problema più grosso.

\bigskip
In generale, un algoritmo \textbf{divide et impera} mostra le seguenti caratteristiche:

\begin{itemize}
  \item si divide il problema \(a\) in sotto problemi, ognuno di dimensione \(\sfrac{1}{b}\) di quello originale
  \item se il sotto problema ha dimensione \(n\) sufficientemente piccola \textit{(\(n < c\) con \(c\) costante)} allora esso può essere risolto in tempo costante \textit{(\(\Theta(1)\))}
  \item Indicando con \(D(n)\) il costo di divisione e \(C(n)\) il costo di ricombinazione, si può esprimere il costo totale \(T(n)\) come:
        \[ T(n) = \begin{cases} \Theta(1)                                 & \quad \text{ se } n < c   \\
              D(n) + aT\left(\sfrac{n}{b}\right) + C(n) & \quad \text{ altrimenti }
          \end{cases}\]
\end{itemize}

\subsubsection{Ricorrenze}

Una \textbf{ricorrenza} è una caratteristica delle funzioni ricorsive, ossia quelle funzioni con:

\begin{itemize}
  \item uno o più \textbf{casi base}
  \item \textbf{chiamata a sé stesse}, con parametri più piccoli
\end{itemize}

Esistono \(3\) tecniche principali per il risolvimento delle ricorrenze:

\begin{enumerate}
  \item Sostituzione
  \item Albero di ricorsione
  \item Teorema dell'Esperto (dall'Inglese \textit{Master Theorem})
\end{enumerate}

Tutte queste tecniche verranno analizzate più nel dettaglio nelle seguenti Sezioni.

\subsubsection{Metodo di sostituzione}

Il metodo di sostituzione si articola in \(2\) passi:

\begin{enumerate}
  \item \textbf{Ipotizzare} una soluzione
  \item \textbf{Trovare le costanti} e dimostrare che la soluzione è corretta tramite induzione
\end{enumerate}

Può essere usato per trovare il limite superiore o il limite inferiore di una ricorrenza.

Questo è un metodo potente, ma può essere applicato solo laddove è facile ipotizzare una soluzione.

\subsubsection{Metodo dell'albero di ricorsione}

Questo metodo fornisce un risultato approssimato la cui validità va provata con altre tecniche.

\begin{enumerate}
  \item Si disegna l'albero di ricorsione, tenendo conto che ad ogni livello la somma di ciascun nodo è pari alla dimensione della radice
  \item Si somma la complessità di ogni livello per trovare la complessità totale
\end{enumerate}

Questo metodo va usato insieme al metodo di sostituzione.
Il metodo dell'albero di ricorsione, infatti, semplifica di molto il primo passo del metodo di sostituzione, ossia l'ipotesi della soluzione.

Inoltre, questo metodo è molto comodo nei problemi di tipo \textit{Dividi et Impera}.

\subsubsection{Teorema dell'esperto}

Il \textbf{Teorema dell'esperto} fornisce un metodo \inlinequote{da manuale} per risolvere le ricorrenze nella forma \(T(n) = aT(\sfrac{n}{b}) + f(n)\), dove \(a > 1,\ b \geq 1\) sono costanti e \(f\) è una funzione asintoticamente positiva.
Il teorema richiede di memorizzare tre casi, ma permette di trovare una soluzione alla ricorrenza con relativa semplicità.

\bigskip
\textit{Formalmente}, siano \(a > 1,\ b \geq 1\) due costanti e \(f(n)\) una funzione dal comportamento asintotico positivo.
A partire da essi è possibile definire \(T(n),\ n \in \mathbb{N}\) tramite la ricorrenza:
\[ T(n) = aT(\sfrac{n}{b}) + f(n) \]
dove \(\sfrac{n}{b}\) è considerato arrotondato per difetto o per eccesso (quindi rispettivamente \(\lfloor\sfrac{n}{b}\rfloor\) o \(\lceil\sfrac{n}{b}\rceil\)).

\(T(n)\) può essere limitato asintoticamente come segue:

\begin{enumerate}[label=\arabic*), ref=(\arabic*)]
  \item\label{enum:teorema-esperto-1} Se \(f(n) = \bigO\left(n^{\log_b(a) - \epsilon}\right)\) per una costante \(\epsilon > 0\), allora \(T(n) = \Theta\left(n^{\log_b (a)}\right)\)
  \item\label{enum:teorema-esperto-2} Se \(f(n) = \Theta\left(n^{\log_b(a)}\right)\), allora \(T(n) = \Theta\left(n^{\log_b(a)} \log(n)\right)\)
  \item\label{enum:teorema-esperto-3} Se \(f(n) = \Omega\left(n^{\log_b(a) + \epsilon}\right)\) per una costante \(\epsilon > 0\) e se \(a \cdot f(\sfrac{n}{b}) \leq c \cdot f(n)\) per una costante \(c < 1\) \textit{(condizione di regolarità)} e dei valori di \(n\) sufficientemente grandi, allora \(T(n) = \Theta\left(f(n)\right)\)
\end{enumerate}

\bigskip
\textit{Informalmente}, il teorema porta a confrontare \(f(n)\) con la funzione \(n^{\log_b(a)}\).

Se, come nel caso \ref{enum:teorema-esperto-1}, la funzione \(n^{\log_b(a)}\) è la più grande, allora la soluzione è \(T(n) = \Theta\left(n^{\log_b (a)}\right)\).
Se, invece, è \(f(n)\) ad essere la più grande, come nel caso \ref{enum:teorema-esperto-3}, allora la soluzione è \(T(n) = \Theta\left(f(n)\right)\).
Infine, se come nel caso \ref{enum:teorema-esperto-2}, le due funzioni sono della stessa dimensione, moltiplicando per un fattore logaritmico risulta che la soluzione è \(T(n) = \Theta\left(n^{\log_b(a)} \log(n)\right) = \Theta\left(f(n) \log(n)\right)\).

\bigskip
Bisogna tuttavia tenere conto di alcuni dettagli tecnici.

Nel primo caso, \ref{enum:teorema-esperto-1}, non solo \(f(n)\) deve essere più piccola di \(n^{\log_b(a)}\), ma deve essere \textbf{polinomialmente più piccola}.
Essa sarà, asintoticamente, più piccola di un fattore \(n^\epsilon,\ \epsilon>0\).

Nel terzo caso, \ref{enum:teorema-esperto-3}, non solo \(f(n)\) deve essere più grande di \(n^{\log_b(a)}\), ma deve essere \textbf{polinomialmente più grande} e deve soddisfare la condizione di \inlinequote{regolarità} che impone \(a \cdot f(\sfrac{n}{b}) \leq c \cdot f(n)\).
Questa condizione è soddisfatta dalla maggior parte delle funzioni polinomialmente limitate che possono essere incontrate.

\bigskip
E importante realizzare che i tre casi del teorema \textbf{non coprono tutti i possibili comportamenti} asintotici di \(f(n)\).
C'è infatti un \inlinequote{vuoto} tra i casi \ref{enum:teorema-esperto-1} e \ref{enum:teorema-esperto-2} quando \(f(n)\) è più piccola di \(n^{\log_b(a)}\) ma non \textbf{polinomialmente} più piccola.
In modo analogo, la stessa problematica si presenta tra i casi \ref{enum:teorema-esperto-2} e \ref{enum:teorema-esperto-3} quando \(f(n)\) è più grande di \(n^{\log_b(a)}\), ma non \textbf{polinomialmente} più grande.
Se la funzione \(f(n)\) cade in uno di questi \inlinequote{vuoti}, o se la condizione di regolarità del caso \ref{enum:teorema-esperto-3} non è valida, il teorema del maestro \textbf{non è applicabile}.

\paragraph{Confronto polinomiale}

Per cercare di chiarire il concetto di \inlinequote{più piccolo} e \inlinequote{più grande}, verranno ora esposti degli \textit{esempi}.

\begin{itemize}
  \item \(n\) è polinomialmente \textbf{più piccolo} di \(n^2\)
  \item \(n \log (n)\) è polinomialmente \textbf{più grande} di \(n^{1/2}\)
  \item \(n \log (n)\) è più grande di \(n\) ma \textbf{non polinomialmente}
\end{itemize}

\paragraph{Casi particolari}

Il teorema dell'Esperto si semplifica notevolmente qualora \(f(n)\) è una funzione \(\Theta(n^k)\), con \(k\) costante:

\begin{enumerate}[label=\arabic*), ref=(\arabic*)]
  \item\label{enum:caso-particolare-teorema-esperto-1} Se \(k < \log_b(a)\), allora \(T(n) = \Theta\left(n^{\log_b(a)}\right)\)
  \item\label{enum:caso-particolare-teorema-esperto-2} Se \(k = \log_b(a)\), allora \(T(n) = \Theta\left(n^k \log(n)\right)\)
  \item\label{enum:caso-particolare-teorema-esperto-3} Se \(k > \log_b(a)\), allora \(T(n) = \Theta\left(n^k\right)\)
\end{enumerate}

Nel caso \ref{enum:caso-particolare-teorema-esperto-3} la condizione aggiuntiva di regolarità è automaticamente verificata.

\subsection{Algoritmi di supporto}

\subsubsection{Operazioni sugli heap}

\begin{center}
  \begin{minipage}{0.3\textwidth}
    \centering
    \begin{lstlisting}[style=pseudocode, numbers=none]
PARENT(i):
  return floor(i / 2)
\end{lstlisting}
  \end{minipage}
  \begin{minipage}{0.3\textwidth}
    \begin{lstlisting}[style=pseudocode, xleftmargin=20pt, xrightmargin=20pt, numbers=none]
LEFT(i):
  return 2 * i
\end{lstlisting}
  \end{minipage}
  \begin{minipage}{0.3\textwidth}
    \begin{lstlisting}[style=pseudocode, numbers=none]
RIGHT(i):
  return 2 * i + 1
\end{lstlisting}
  \end{minipage}
\end{center}

Ogni array \texttt{A} che rappresenta uno heap ha \(2\) attributi:

\begin{itemize}
  \item \texttt{A.length}, rappresentante il numero totale di elementi dell'array
  \item \texttt{A.heap-size}, rappresentante il numero totale di elementi dell'heap
        \begin{itemize}
          \item \texttt{A.heap-size} \(\leq\) \texttt{A.length} \(\forall \, A\)
          \item solo gli elementi fino a \texttt{A.heap-size} hanno le proprietà dello heap
          \item l'array potrebbe però contenere elementi dopo l'indice \texttt{A.heap-size}
        \end{itemize}
\end{itemize}

\subsubsection{Algoritmo \texttt{MAX-HEAPIFY}}
\label{sec:algoritmo-max-heapify}

L'algoritmo di \texttt{MAX-HEAPIFY} è una funzione che modifica un \textbf{array}, tale per cui i suoi figli sinistro e destro sono max-heap ma in cui \texttt{A[i]} \textit{(la radice del sotto albero)} potrebbe essere \textit{minore} dei suoi figli, in modo che tutto l'albero di radice \texttt{A[i]} sia un \textbf{max-heap}.

\begin{lstlisting}[style=pseudocode]
MAX-HEAPIFY(A, i)
  l := LEFT(i)
  r := RIGHT(i)
  if l < A.heap-size and A[l] > A[i]
    max := l
  else
    max := i
  if r < A.heap-size and A[r] > A[max]
    max := r
  if max != i then
    swap A[i], A[max]
    MAX-HEAPIFY(A, max)
\end{lstlisting}

\paragraph{Analisi dell'algoritmo \texttt{MAX-HEAPIFY}}

La complessità temporale di \texttt{MAX-HEAPIFY} è parti a
\[ T_{\texttt{MAX-HEAPIFY}} = \bigO(h) = \bigO\left(\log(n)\right) \]
dove \(h\) corrisponde all'altezza dell'albero, che essendo quasi completo è inferiore di \(\log(n)\).

\bigskip
Sarebbe stato possibile giungere alla stessa soluzione usando il teorema dell'esperto per la ricorrenza
\[ T(n) = T\left(\sfrac{2n}{3}\right) + \Theta(1) \]
che rappresenta il tempo di esecuzione di \texttt{MAX-HEAPIFY} nel caso pessimo.

In questo caso, infatti, l'ultimo livello dell'albero è pieno esattamente a metà.
L'algoritmo viene applicato ricorsivamente sul sotto albero sinistro (contenente \(\leq \sfrac{2n}{3}\), con \(n\) nodi totali).

\subsubsection{Algoritmo \texttt{BUILD-MAX-HEAP}}
\label{sec:algoritmo-build-max-heap}

L'algoritmo \texttt{BUILD-MAX-HEAP} permette di costruire un \textbf{max-heap} partendo da un \textbf{array}.
La costruzione avviene \textit{bottom-up}, partendo dalle foglie fino alla radice.

Funziona grazie a due proprietà dello heap:

\begin{itemize}
  \item Tutti gli elementi oltre l'indice \texttt{A.length / 2} sono delle foglie, gli altri sono dei nodi interni
  \item I sotto alberi fatti di solo foglie sono, presi singolarmente, max-heap in quanto costituiti da un solo elemento
\end{itemize}

\begin{lstlisting}[style=pseudocode]
BUILD-MAX-HEAP(A)
  A.heap-size := A.length
  for i := A.length / 2 downto 1 do
    MAX-HEAPIFY(A, i)
\end{lstlisting}

\paragraph{Analisi dell'algoritmo \texttt{BUILD-MAX-HEAP}}

Il costo dell'algoritmo \texttt{BUILD-MAX-HEAP}, a priori, potrebbe risultare pari a \(\bigO(\left(n \log(n)\right))\).
Tuttavia, questo limite non fornisce una precisione sufficiente.

Infatti, si osserva che:

\begin{itemize}
  \item L'altezza di un albero quasi completo di \(n\) nodi è \(\left\lfloor \log_2(n) \right\lfloor\)
  \item Definendo come \inlinequote{altezza di un nodo di uno heap} la lunghezza del cammino più lungo che porta ad una foglia, il costo dell'algoritmo invocato su un nodo di altezza \(h\) è \(\bigO(h)\)
  \item Il numero massimo di nodi di altezza \(h\) di uno heap è \(\left\lceil \dfrac{n}{2^{h+1}} \right\rceil\) volte ad ogni altezza \(h\)
\end{itemize}

Grazie a queste proprietà, il costo di computazione dell'algoritmo \texttt{BUILD-MAX-HEAP} è pari a:
\[ \displaystyle \sum_{n=0}^{\left\lfloor \log_2(n) \right\rfloor} \left[ \dfrac{n}{n^{h+1}} \right] \bigO(h) = \bigO \left(n \displaystyle \sum_{n=0}^{\left\lfloor \log_2(n) \right\rfloor} \dfrac{h}{2^h} \right) = \bigO (n)\]

Il passo finale della soluzione deriva dall'uguaglianza
\[ \sum_{n=0}^{\left\lfloor \log_2(n) \right\rfloor} \dfrac{h}{2^h} = \dfrac{\sfrac{1}{2}}{(1 - \sfrac{1}{2})^2} \]
come sommatoria di una serie geometrica.

\subsection{Problemi di ordinamento}

L'\textbf{ordinamento} degli elementi di una sequenza è un problema molto comune e fornisce un classico esempio di problema risolvibile tramite algoritmi.
Esistono parecchi algoritmi di ordinamento \textit{(insertion sort, selection sort, bubble sort, \ldots)}, ognuno con la propria complessità.

\bigskip
Nelle prossime Sezioni alcuni di essi verranno analizzati in dettaglio.

\subsubsection{\texttt{INSERTION-SORT}}

L'algoritmo \texttt{INSERTION-SORT} è uno dei più semplici algoritmi di ordinamento.

\textit{Idea} dell'algoritmo:
per ogni elemento \(x\) di una sequenza \(A\) di elementi, partendo dalla prima posizione, si inserisce l'elemento in una posizione successiva, fino a quando non viene trovata una posizione successiva in cui inserire \(x\).

L'array viene quindi ordinato senza sul posto, senza bisogno di array ausiliari.

\begin{minipage}{0.99\textwidth}
  \bigskip
  \centering
  \begin{minipage}{0.45\textwidth}
    \centering
    \begin{lstlisting}[style=pseudocode]
INSERTION-SORT(A)
  for j := 2 to A.length
    key := A[j]
    i := j - 1
    while i > 0 and A[i] > key
      A[i + 1] := A[i]
      i := i - 1
    A[i + 1] := key
    \end{lstlisting}
  \end{minipage}
  \begin{minipage}{0.45\textwidth}
    \centering
    \begin{tabular}{ccc}
      \textit{riga} & \textit{costo} & \textit{ripetizioni}     \\ \hline
      \(2\)         & \(c_1\)        & \(n\)                    \\
      \(3\)         & \(c_2\)        & \(n-1\)                  \\
      \(4\)         & \(c_3\)        & \(n-1\)                  \\
      \(5\)         & \(c_4\)        & \(\sum_{j=2}^n t_j\)     \\
      \(6\)         & \(c_5\)        & \(\sum_{j=2}^n (t_j-1)\) \\
      \(7\)         & \(c_6\)        & \(\sum_{j=2}^n (t_j-1)\) \\
      \(8\)         & \(c_7\)        & \(n-1\)                  \\
    \end{tabular}
  \end{minipage}
  \bigskip
\end{minipage}

\paragraph{Analisi dell'algoritmo \texttt{INSERTION-SORT}}

Il tempo di esecuzione di \texttt{INSERTION-SORT(A)} è:

\[ T(n) = c_1 \cdot n + c_2 \cdot (n-1) + c_3 \cdot (n-1) + c_4 \cdot \displaystyle \sum_{j=2}^n t_j + c_5 \cdot \displaystyle \sum_{h-2}^n (t_j - 1) + c_6 \cdot \displaystyle \sum_{h-2}^n (t_j - 1) + c_7 \cdot (n-1) \]

\begin{itemize}
  \item Caso \textbf{ottimo}:
        \begin{itemize}
          \item gli elementi sono già ordinati, \(t_2 = \ldots = t_n = 1\)
          \item \(T(n) = an + b \quad \Rightarrow \quad T(n) = \Theta(n)\)
          \item si può anche dire che \(T(n) = \Omega(n)\) perché il limite inferiore è funzione di \(\Theta(n)\)
        \end{itemize}
  \item Caso \textbf{pessimo}:
        \begin{itemize}
          \item gli elementi sono già ordinati ma in ordine decrescente \(t_2 = 2,\ t_3 = 3,\ \ldots\)
          \item \(T(n) = an^2 + bn + c \quad \Rightarrow \quad T(n) = \Theta(n^2)\)
          \item si può anche dire che \(T(n) = \bigO(n^2)\) perché il limite superiore è funzione di \(\Theta(n^2)\)
        \end{itemize}
\end{itemize}

\subsubsection{\texttt{MERGESORT}}

\textit{Idea} dell'algoritmo:

\begin{itemize}
  \item Se l'array da ordinare ha meno di \(2\) elementi, allora è ordinato per definizione
  \item Altrimenti:
        \begin{itemize}
          \item si dive l'array in \(2\) sotto array, ognuno con la metà degli elementi di quello originario
          \item si ordinano i \(2\) sotto array applicando di nuovo l'algoritmo
          \item si fondono \textit{(tramite l'algoritmo \texttt{MERGE})} i \(2\) sotto array ora ordinati
        \end{itemize}
\end{itemize}

Il \texttt{MERGE-SORT} è un algoritmo ricorsivo e adotta la tecnica \textbf{divide ed impera}.
Per completare l'algoritmo, infatti, è necessario definire un sotto algoritmo \texttt{MERGE} che combina le soluzioni dei problemi divisi.

\textit{Idea} dell'algoritmo:

\begin{enumerate}
  \item Si va all'inizio dei \(2\) sotto array
        \item\label{enum:begin-merge-loop} Si prende il minimo dei \(2\) elementi correnti
  \item Si inserisce il minimo alla fine dell'array da restituire
  \item Si avanza di uno nell'array da cui si è preso il minimo
  \item Si ripete il passo \ref{enum:begin-merge-loop}
\end{enumerate}

\begin{minipage}{0.99\textwidth}
  \begin{lstlisting}[style=pseudocode]
MERGE-SORT(A, p, R)
  if p < r
    q := floor((p + r) / 2)
    MERGE-SORT(A, p, q)
    MERGE-SORT(A, q+1, r)
    MERGE(A, p, q, r)
\end{lstlisting}

  \begin{minipage}{0.99\textwidth}
    \centering
    \begin{minipage}[t]{0.495\textwidth}
      \begin{lstlisting}[style=pseudocode]
      MERGE(A, p, q, r)
        n1 := q - p + 1
        n2 := r - q
        alloca L[1 .. n1+1]
        alloca R[1 .. n2+1]
        for i := 1 to n1
          L[i] := A[p+i-1]
        for j := 1 to n2
          R[j] := A[q+j]
        L[n1+1] := $\infty$
        R[n2+1] := $\infty$
        i := 1
        j := 1
        for k := p to r
          if L[i] <= R[j]
            A[k] := L[i]
            i := i + 1
          else
            A[k] := R[j]
            j := j + 1
          \end{lstlisting}
    \end{minipage}
    \begin{minipage}[t]{0.495\textwidth}
      \begin{tabular}[t]{ccc}
        \textit{riga} & \textit{costo}  & \textit{ripetizioni}        \\ \hline
        \(2\)         & \(c_1\)         & \(1\)                       \\
        \(3\)         & \(c_2\)         & \(1\)                       \\
        \(4\)         & \(\Theta(n)\)   & \(1\)                       \\
        \(5\)         & \(\Theta(n_1)\) & \(n_1\)                     \\
        \(8\)         & \(\Theta(n_2)\) & \(n_2\)                     \\
        \(10\)        & \(c_3\)         & 1                           \\
        \(11\)        & \(c_4\)         & 1                           \\
        \(12\)        & \(c_5\)         & 1                           \\
        \(13\)        & \(c_7\)         & 1                           \\
        \(14\)        & \(\Theta(n)\)   & \(\texttt{r} - \texttt{p}\) \\
        \(15\)        & \(c_8\)         & 1                           \\
        \(16\)        & \(c_9\)         & 1                           \\
        \(17\)        & \(c_10\)        & 1                           \\
        \(19\)        & \(c_11\)        & 1                           \\
        \(20\)        & \(c_12\)        & 1                           \\
      \end{tabular}
    \end{minipage}
  \end{minipage}
\end{minipage}

\paragraph{Analisi dell'algoritmo \texttt{MERGE}}

Nell'algoritmo \texttt{MERGE} prima vengono copiati gli elementi dei \(2\) sotto array \texttt{A[p .. q]} e \texttt{A[q+1 .. r]} in \(2\) array temporanei \(L\) e \(R\) e poi vengono fusi in \texttt{A[p .. r]}.
Per non dover controllare se \(L\) e \(R\) sono vuoti si usa un valore particolare \((\infty)\), più grande di ogni possibile valore, in ultima posizione negli array.

La dimensione dei dati in input è \(\texttt{n} = \texttt{r} - \texttt{p} + 1\) ed è composto da \(3\) cicli \textbf{\texttt{for}}:

\begin{itemize}
  \item \(2\) cicli di inizializzazione (linee \(10\) e \(11\)) per assegnare i valori a \(L\) e \(R\)
        \begin{enumerate}
          \item il primo è eseguito \(n_1\) volte, con \(\Theta(n_1) = \Theta(\texttt{q}-\texttt{p}+1) = \Theta(\sfrac{n}{2}) = \Theta(n)\)
          \item il secondo è eseguito \(n_2\) volte, con \(\Theta(n_2) = \Theta(\texttt{r}-\texttt{q}) = \Theta(\sfrac{n}{2}) = \Theta(n)\)
        \end{enumerate}
  \item[\(\rightarrow\)] in totale, \(T_\texttt{MERGE} = \Theta(n)\)
\end{itemize}

\paragraph{Analisi delle ricorrenze dell'algoritmo \texttt{MERGE-SORT}}

Siano \(a = b = c = 2\).
Di conseguenza, \(D(n) = \Theta(1),\ C(n) = \Theta(n)\).

La complessità totale sarà:

\[T(n) = \begin{cases}
    \Theta(1)                    & \quad \text{ se } n < 2   \\
    2T(\sfrac{n}{2}) + \Theta(n) & \quad \text{ altrimenti }
  \end{cases}\]

In realtà la complessità dovrebbe essere \(T(\lfloor \sfrac{n}{2} \rfloor) + T(\lfloor \sfrac{n}{2} \rfloor)\) ma l'approssimazione a \(2T(\sfrac{n}{2})\) è sufficiente perché non influisce sul comportamento asintotico della funzione.

\bigskip

Riscrivendo la ricorrenza dell'algoritmo senza fare uso delle notazioni \textit{Theta}, risulta che:

\[
  T(n) = \begin{cases}
    c                     & \quad \text{ se } n < 2  \\
    2T(\sfrac{n}{2}) + cn & \quad \text{ altrimenti}
  \end{cases}
\]

Per visualizzare le ricorsioni si può fare uso di un \textbf{albero di ricorsione}, scegliendo per semplicità il caso in cui \(n\) è una potenza di due.
Una rappresentazione dell'albero di ricorsione è mostrata in Figura~\ref{fig:albero-ricorsione-algoritmo-merge-sort}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-25.tikz}
  \caption{Albero di ricorsione per l'algoritmo \texttt{MERGE-SORT} su un array di dimensione \(n = 2^k\).}
  \label{fig:albero-ricorsione-algoritmo-merge-sort}
  \bigskip
\end{figure}

Sommando i costi dei vari livelli si ottiene:

\[ T(n) = cn \log(n) + cn \quad \Rightarrow \quad T_\texttt{MERGE-SORT} = \Theta(n \log(n)) \]

\bigskip
Analogamente sarebbe stato possibile applicare il Teorema dell'Esperto, giungendo alla stessa soluzione.
Infatti:

\[ T(n) = 2T(\sfrac{n}{2}) + \Theta(n)\ \]
dove \(a = b = 2,\ f(n) = n\) e di conseguenza \(n^{\log_b(a)} = n^1 = n\).

Si ricade quindi nel caso \ref{enum:teorema-esperto-2} del teorema dell'esperto:
\[ T_{\texttt{MERGE-SORT}} = \Theta(n \log(n)) \]

\subsubsection{\texttt{HEAPSORT}}
\label{sec:heap-sort}

L'algoritmo \texttt{HEAPSORT}, \textit{come suggerisce il nome}, sfrutta le proprietà degli heap \textit{(Sezione~\ref{sec:heap})} per ordinare gli elementi di un array.
Questo algoritmo farà uso degli algoritmi \textit{di supporto} \texttt{BUILD-MAX-HEAP} \textit{(Sezione~\ref{sec:algoritmo-build-max-heap})} e \texttt{MAX-HEAPIFY} \textit{(Sezione~\ref{sec:algoritmo-max-heapify})}.

\textit{Idea} dell'algoritmo:
Ad ogni ciclo viene preso l'elemento più grande \textit{(primo dell'array, in quanto max-heap)} in fondo alla parte di array ancora da ordinare \textit{(corrispondente allo heap).}
Lo heap viene decrementato di \(1\) e viene ricostruito il max-heap mettendo come radice l'ultima foglia a destra dell'ultimo livello e invocando \texttt{MAX-HEAPIFY}.

\begin{lstlisting}[style=pseudocode]
HEAPSORT(A)
  BUILD-MAX-HEAP(A)
  for i := A.length downto 2
    swap A[1], A[i]
    A.heap-size := A.heap-size - 1
    MAX-HEAPIFY(A, 1)
\end{lstlisting}

\paragraph{Analisi dell'algoritmo \texttt{HEAPSORT}}

La complessità dell'algoritmo \texttt{HEAPSORT} è
\[ T(n) = \bigO (n \log(n)) \]
perché:

\begin{itemize}
  \item \texttt{BUILD-MAX-HEAP} ha costo \(\bigO(n)\)
  \item \texttt{MAX-HEAPIFY} è invocato \(n-1\) volte, con costo \(\bigO\left(\log(n)\right)\)
\end{itemize}

\subsubsection{\texttt{QUICKSORT}}

L'algoritmo \texttt{QUICKSORT} è un algoritmo in stile \textit{\nameref{sec:divide-et-impera}}, che ordina sul posto.
Nel caso pessimo ha complessità \(\Theta(n^2)\), superiore all'\nameref{sec:heap-sort}, ma nel caso medio ha complessità \(\Theta\left(n \log(n)\right)\).

\textit{Idea} dell'algoritmo, dato un array \texttt{A[p..r]} da ordinare:

\begin{itemize}
  \item \textbf{Divide}: \texttt{A} viene diviso in \(2\) sotto array \texttt{A[p..q]} e \texttt{A[q+1..r]}
        \begin{itemize}
          \item \texttt{q} è il minimo elemento del sotto array \texttt{A[p..q]} (\texttt{A[n]} \(\leq\) \texttt{A[q]} \(\forall\) \texttt{n} \(\in [\texttt{p}, \texttt{q}]\))
          \item \texttt{q} è il massimo elemento del sotto array \texttt{A[q+1..r]} (\texttt{A[n]} \(\geq\) \texttt{A[q]} \(\forall\) \texttt{n} \(\in [\texttt{q}+1, \texttt{r}]\))
        \end{itemize}
  \item \textbf{Impera}: i sotto array \texttt{A[p...q-1]} e \texttt{A[q+1...r]} vengono ordinati utilizzando \texttt{QUICKSORT}
  \item \textbf{Combina}: l'array \texttt{A[p..r]} è già ordinato
\end{itemize}

La parte \inlinequote{complicata} da implementare riguarda la partizione: essa viene effettuata tramite l'algoritmo \texttt{PARTITION}:
viene scelto un elemento di come \textit{pivot}:

\begin{itemize}
  \item Tutti gli elementi \textbf{minori} di \texttt{pivot} vengono posti nel sotto array \textbf{sinistro}
  \item Tutti gli elementi \textbf{maggiori} di \texttt{pivot} vengono posti nel sotto array \textbf{destro}
\end{itemize}

\begin{center}
  \begin{minipage}[t]{0.495\textwidth}
    \begin{lstlisting}[style=pseudocode]
QUICKSORT(A, p, r):
  if p < r:
    q := PARTITION(A, p, r)
    QUICKSORT(A, p, q - 1)
    QUICKSORT(A, q + 1, r)
        \end{lstlisting}
  \end{minipage}
  \begin{minipage}[t]{0.495\textwidth}
    \begin{lstlisting}[style=pseudocode]
PARTITION(A, p, r):
    x := A[r]
    i := p - 1
    for j := p to r - 1
        if A[j] <= x
            i := i + 1
            swap A[i], A[j]
    return i + 1
  \end{lstlisting}
  \end{minipage}
\end{center}

\paragraph{Analisi di \texttt{QUICKSORT}}

La complessità di \texttt{PARTITION} e \(\Theta(n),\ n = r - p + 1\).

\bigskip
Il tempo di esecuzione di \texttt{QUICKSORT} dipende da come viene partizionato l'array:

\begin{itemize}
  \item Se ogni volta uno dei \(2\) sotto array è vuoto mentre l'altro contiene \(n-1\) elementi si ha il caso \textbf{pessimo}:
        \begin{itemize}
          \item la ricorrenza è \(T(n) = T(n - 1) + \Theta(n)\)
          \item risolvendo per sostituzione o tramite il Teorema dell'esperto dimostra che la soluzione è \(\Theta(n^2)\)
          \item questo caso si può verificare quando l'array è già ordinato
        \end{itemize}
  \item Se ogni volta i due sotto array hanno una dimensione pari a \(\sfrac{n}{2}\) si ha il caso \textbf{ottimo}:
        \begin{itemize}
          \item la ricorrenza è \(T(n) = 2T(\sfrac{n}{2}) + \Theta(n)\)
          \item risolvendo si dimostra che la soluzione è \(\Theta(n \log(n))\)
        \end{itemize}
\end{itemize}

Se la proporzione di divisione fosse diversa da \(\sfrac{1}{2}\) e \(\sfrac{1}{2}\), la complessità rimarrebbe la stessa.

\bigskip
Il caso \textbf{medio} va analizzato in modo diverso:

\clearpage

\section{Conclusioni sparse delle precedenti Sezioni}

L'obiettivo di questa Sezione sarà ricapitolare delle conclusioni tratte nelle precedenti sezioni, con particolare riguardo agli esercizi ed alle applicazioni pratiche.

\subsection{Ripasso di matematica}

\begin{itemize}
  \item La formula \(\log_a(b)\) indica il numero reale \(c\) che realizza l'uguaglianza \(a^c = b\), \(a, b > 0,\ a \neq 1\)
  \item Formula di cambio di base per i logaritmi \(\log_a(b) = \dfrac{\log_c(b)}{\log_(a)}\) con \(a, b, c > 0,\ c \neq 1\)
  \item Formula di Gauss \(\displaystyle \sum_{i=1}^k i = \dfrac{k(k+1)}{2}\)
  \item Sommatoria di una serie geometrica \(\displaystyle \sum_{n=0}^{+\infty} q^n = \dfrac{1}{1-q}\) con \(|q| < 1\)
\end{itemize}

\subsection{Scala di potenza delle classi di automi}

Le classi di automi possono essere rappresentati in una scala, in ordine dal \textbf{più} al \textbf{meno} potente.
Ciò avviene nella Figura~\ref{fig:scala-potenza-automi}.

Si noti che \NFA e \FSA hanno potenza equivalente in quanto esiste un algoritmo per convertire i primi nei secondi.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-19.tikz}
  \caption{Scala di potenza degli automi}
  \label{fig:scala-potenza-automi}
  \bigskip
\end{figure}

\subsection{Chiusura degli automi rispetto alle operazioni}

La chiusura degli automi rispetto alle operazioni è mostrata all'interno della Tabella~\ref{tab:chisura-automi-rispetto-operazioni}.

\begin{table}[htbp]
  \bigskip
  \centering
  \scalebox{0.8}{
    \begin{tabular}{c|ccccccc}
      \textit{classe di automi} & \textit{unione \(\cup\)} & \textit{intersezione \(\cap\)} & \textit{complemento \(A^c\)} & \textit{differenza \(\backslash\)} & \textit{stella di Kleene \(A^\ast\)} & \textit{concatenazione \(\cdot\)} \\ \hline
      \FSA, \NFA                & \colorcmark              & \colorcmark                    & \colorcmark                  & \colorcmark                        & \colorcmark                          & \colorcmark                       \\
      \PDA                      & \colorxmark              & \colorxmark                    & \colorcmark                  & \colorxmark                        & \colorxmark                          & \colorxmark                       \\
      \NPDA                     & \colorcmark              & \colorxmark                    & \colorxmark                  & \colorxmark                        & \colorcmark                          & \colorcmark                       \\
      \TM, \NTM                 & \colorcmark              & \colorcmark                    & \colorxmark                  & \colorxmark                        & \colorcmark                          & \colorcmark                       \\
    \end{tabular}}
  \bigskip
  \caption{Chiusura degli automi rispetto alle operazioni}
  \label{tab:chisura-automi-rispetto-operazioni}
\end{table}

\subsection{Automi e grammatiche di Chomsky}

All'interno della Tabella~\ref{tab:automi-grammatiche-chomsky} è mostrata la relazione tra grammatiche secondo la caratterizzazione di Chomsky e corrispondente formalismo a potenza minima che la riconosce.

\begin{table}[htbp]
  \bigskip
  \centering
  \begin{tabular}{c|c|c}
    \textit{grammatica} & \textit{formalismo}               & \textit{note}                                                             \\ \hline
    \(0\)               & \TM                               & \textit{grammatiche \textbf{generali} e \textbf{dipendenti} dal contesto} \\
    \(1\)               & \textit{linear bounded automaton} & \textit{non trattati nel corso}                                           \\
    \(2\)               & \NPDA                             & \textit{grammatiche \textbf{non dipendenti} dal contesto}                 \\
    \(3\)               & \FSA, \NFA                        & \textit{grammatiche \textbf{regolari}}                                    \\
  \end{tabular}
  \bigskip
  \caption{Relazione tra grammatiche e formalismi}
  \label{tab:automi-grammatiche-chomsky}
\end{table}

\subsection{Pattern tipici delle grammatiche}

Si riconoscono pattern tipici per generare delle strutture note all'interno di grammatiche.
Essi sono mostrati in Tabella~\ref{tab:pattern-tipici}.

\begin{table}[htbp]
  \bigskip
  \centering
  \begin{tabular}{c|c|c}
    \textit{linguaggio}                           & \textit{pattern}                                                                               & \textit{note}                \\ \hline
    \(a^n b^n, n \geq 0\)                         & \(S \rightarrow a S b \, | \, \epsilon\)                                                       & \textit{stella di Kleene}    \\
    \(a^n b^n, n \geq 1\)                         & \(S \rightarrow a S b \, | \, ab\)                                                             & \textit{più di Kleene}       \\
    \(w w^r,\ w = \left(a \, | \, b\right)^\ast\) & \(S \rightarrow a S a \, | \, a B b \, | \, a \, | \, b \, | \, \epsilon \)                    & \textit{stringhe palindrome} \\
    \((a b^+)^\ast\)                              & \(\begin{cases} S \rightarrow ah \, | \, \epsilon \\ h \rightarrow bh \, | \, bS \end{cases}\) &                              \\
  \end{tabular}
  \bigskip
  \caption{Pattern tipici}
  \label{tab:pattern-tipici}
\end{table}

\subsubsection[Sintetizzazione di grammatiche di tipo 0]{Sintetizzazione di grammatiche di tipo \(0\)}

\begin{minipage}{0.99\textwidth}
  \bigskip
  \textit{Idea generale}: simulare i nastri di una \TM tramite le regole della grammatica.
  In dettaglio:
  \begin{itemize}
    \item I nastri memorizzano i caratteri non terminali
    \item Altri caratteri non terminali simulano le testine
    \item I simboli nei nastri non mossi attraverso regole di \inlinequote{swap}
  \end{itemize}
  \bigskip
\end{minipage}

\subsection{Linee guida sulla decidibilità}

\subsubsection{Casi immediati}

\begin{itemize}
  \item C'è una domanda di tipo \textbf{booleano} la cui risposta non dipende da alcun parametro esterno?
        \begin{itemize}[label=\(\Rightarrow\)]
          \item La domanda è \textbf{chiusa} e il problema è \textbf{decidibile}
        \end{itemize}
  \item La funzione in questione consiste di un \textbf{numero finito di casi}, tutti singolarmente decidibili e calcolabili?
        \begin{itemize}[label=\(\Rightarrow\)]
          \item La funzione è \textbf{calcolabile} e il problema è \textbf{decidibile}
        \end{itemize}
\end{itemize}

\subsubsection{Caso del programmatore}

È possibile scrivere un programma in un generico linguaggio (sia esso \texttt{C}, \texttt{Java}, \texttt{\ldots}) che risolve il problema dato?

\begin{itemize}[label=\(\Rightarrow\)]
  \item Se \textbf{sì}, la funzione è \textbf{calcolabile} e il problema è \textbf{decidibile}
\end{itemize}

Ciò implica che sia possibile scrivere un programma che per ogni possibile ingresso sia in grado di calcolare il valore corretto dell'uscita.
Se per qualche valore dell'ingresso l'uscita non è definita, è sufficiente far entrare il programma in un loop infinito.

\subsubsection{Riduzioni}

\begin{itemize}
  \item Esiste un problema \textbf{indecidibile} che è un caso particolare del problema in analisi
        \begin{itemize}[label=\(\Rightarrow\)]
          \item Il problema in analisi è \textbf{indecidibile}
        \end{itemize}
  \item Esiste un problema \textbf{decidibile} che è un caso particolare del problema in analisi
        \begin{itemize}[label=\(\Rightarrow\)]
          \item Il problema in analisi è \textbf{indecidibile}
        \end{itemize}
\end{itemize}

\subsubsection{Applicazione del teorema di Rice}

Il teorema di Rice può essere applicato nei casi in cui si deve verificare se un programma \textit{(o analogamente una \TM o un algoritmo)}:

\begin{itemize}
  \item Ha una data \textbf{proprietà} relativa alla funzione da esso calcolata
  \item Calcola una \textbf{funzione} tra quelle di un insieme dato
\end{itemize}

Infatti, se:

\begin{itemize}
  \item L'insieme di funzioni identificato \textbf{non è banale}
        \begin{itemize}[label=\(\Rightarrow\)]
          \item Il problema in analisi è \textbf{indecidibile}
        \end{itemize}
  \item L'insieme di funzioni identificato \textbf{è banale}
        \begin{itemize}[label=\(\Rightarrow\)]
          \item Il problema in analisi è \textbf{decidibile}
        \end{itemize}
\end{itemize}

Si ricordi che un insieme di funzioni è banale se è \textbf{vuoto} o se è l'insieme di \textbf{tutte le funzioni computabili}.

\subsubsection[Ricorsività di un insieme S di numeri naturali]{Ricorsività di un insieme \(S\) di numeri naturali}

\begin{itemize}
  \item \(S\) è \textbf{finito}
        \begin{itemize}[label=\(\Rightarrow\)]
          \item \(S\) è \textbf{ricorsivo}
        \end{itemize}
  \item \(S\) è \textbf{infinito}
        \begin{itemize}
          \item La funzione caratteristica di \(S\) è \textbf{computabile}
                \begin{itemize}[label=\(\Rightarrow\)]
                  \item \(S\) è \textbf{ricorsivo}
                \end{itemize}
          \item \(S\) può essere espresso come insieme di indici di \TM con una \textbf{proprietà comune relativa alla funzione che calcolano}
                \begin{itemize}[label=\(\Rightarrow\)]
                  \item \textbf{Si può} usare il teorema di Rice
                \end{itemize}
          \item \(S\) può essere espresso come insieme di indici di \TM con una \textbf{proprietà comune non relativa alla funzione che calcolano}
                \begin{itemize}[label=\(\Rightarrow\)]
                  \item \textbf{Non si può} usare il teorema di Rice
                \end{itemize}
        \end{itemize}
\end{itemize}

\subsection{Complessità}

\begin{itemize}
  \item Tutti i logaritmi sono nello stesso \(\Theta\) indipendentemente dalla base
        \begin{itemize}
          \item la formula di cambio base comporta un fattore moltiplicativo
          \item grazie a questa proprietà si può non indicare la base del logaritmo quando si valuta l'andamento asintotico
        \end{itemize}
  \item Contrariamente a quanto avviene nei logaritmi, negli esponenziali è importante specificare la base
\end{itemize}

\end{document}