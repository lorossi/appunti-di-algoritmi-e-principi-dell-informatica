\documentclass[italian, 10pt]{article}
\usepackage{notestemplate}

% document specific commands
\input{shortfor.tex}

% math operators
\DeclareMathOperator{\succop}{succ}
\DeclareMathOperator{\Succop}{Succ}
\DeclareMathOperator{\lastop}{last}
\DeclareMathOperator{\firstop}{first}
\DeclareMathOperator{\modop}{\ mod}

% various text
\newcommand{\vero}{\textcolor{ForestGreen}{\texttt{vero}}\xspace} % green check
\newcommand{\falso}{\textcolor{BrickRed}{\texttt{falso}}\xspace} % red
\newcommand{\blank}{\texttt{b}} % similar to ƀ

% don't ask
\usepackage{mathrsfs}
\newcommand{\flower}{\ding{97}\xspace}

% TODO una volta finiti gli appunti, ri numerare le immagini in modo che sia tutto in ordine

\begin{document}

\makecover{Algoritmi e Principi dell'Informatica}{AA 2021/2022}

\section*{Introduzione}

Questi appunti si riferiscono al corso di \textit{Algoritmi e principi dell'Informatica}, tenuto nell'anno accademico \textit{2021/2022} dal professor \textit{Marco Martinenghi}.
La versione più aggiornata può essere trovata sulla repo di GitHub: \href{https://github.com/lorossi/appunti-di-algoritmi-e-principi-dell-informatica}{github.com/lorossi/appunti-di-algoritmi-e-principi-dell-informatica}.
\bigskip

Il contenuto di questo documento è tratto parzialmente dalle slide mostrate in aula e dai libri indicati nel manifesto degli studi, (\textit{Informatica Teorica. Mandrioli, Spoletini} e \textit{Introduzione agli algoritmi e strutture dati. Cormen, Leiserson, Rivest, Stein}).
Spesso, le illustrazioni e le tabelle saranno uguali \textit{(a meno di palesi errori da parte mia)}.

\bigskip
Alcuni argomenti, paragrafi e sezioni potrebbero talora seguire le lezioni, talora seguire i libri perché mi sono preso la libertà di riordinare il corso a piacimento, in modo che seguisse il mio schema mentale.

\bigskip
Il lettore si senta libero di avvisarmi qualora trovasse un errore!
Vi ringrazio in anticipo.

\signature

\clearpage

\section{Linguaggi Formali}

Con \textbf{linguaggi formali} si intende un'insieme di \textbf{stringhe} costruito su un \textbf{alfabeto}, secondo uno specifico insieme di regole.

\textit{Formalmente}:
\begin{itemize}
  \item \textbf{Alfabeto}, o \textit{vocabolario}
        \begin{itemize}
          \item Insieme \textbf{finito} di simboli di base
        \end{itemize}
  \item \textbf{Stringa} su un alfabeto \(A\)
        \begin{itemize}
          \item Sequenza \textbf{finita} di simboli dell'alfabeto \(A\)
                \begin{itemize}[label=\(\rightarrow\)]
                  \item c'è un ordine tra gli elementi
                  \item non c'è un limite superiore alla lunghezza
                \end{itemize}
          \item Sono consentite le \textbf{ripetizioni}
        \end{itemize}
\end{itemize}

\subsection{Stringhe}

Una \textbf{stringa}, è caratterizzata dalla sua \textbf{lunghezza}:

\begin{itemize}
  \item Equivale al numero di simboli che contiene
  \item La lunghezza della generica stringa \(x\) si indica con \(|x|\)
\end{itemize}

La stringa \textbf{vuota} è una stringa:

\begin{itemize}
  \item Indicata come \(\epsilon\)
  \item Per \textbf{definizione}, \(|\epsilon| = 0\)
  \item Un insieme formato dalla stringa vuota non corrisponde all'insieme vuoto
        \begin{itemize}[label=\(\rightarrow\)]
          \item \textit{in simboli:} \(\{\epsilon\} \neq \emptyset\)
        \end{itemize}
\end{itemize}

La stringa vuota è definita su qualsiasi alfabeto.

\subsubsection{Confronto di stringhe}

Date due stringhe

\begin{gather*}
  x = x_1 x_2 \ldots x_n \\
  y = y_1 y_2 \ldots y_m
\end{gather*}

esse si dicono \textbf{uguali} se e solo se sono valide le seguenti due proposizioni:

\begin{enumerate}
  \item Le due stringhe hanno la stessa \textbf{lunghezza} \[ |x| = |y| \Leftrightarrow n = m \]
  \item Gli elementi in posizioni corrispondenti sono \textbf{uguali} \[ x_i = y_i \ \forall \, 1 \leq i \leq n \]
\end{enumerate}

\subsubsection{Concatenazione di stringhe}

Date due stringhe \(x\) e \(y\), la loro \textbf{concatenazione} (detta anche \textit{prodotto}) è una stringa \(xy\) (\textit{oppure} \(x \cdot y \)) dove \(x\) è seguita da \(y\).

\bigskip
\textbf{Proprietà} della concatenazione:

\begin{enumerate}
  \item Una stringa \(x\) concatenata con \(\epsilon\) è ancora \(x\) \textit{(non viene alterata)}
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(\{\epsilon\} \cdot x = x\)
        \end{itemize}
  \item La concatenazione è \textbf{associativa} e \textbf{non commutativa}
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(a \cdot (x \cdot y) = (a \cdot x) \cdot y\)
          \item \(x \cdot y \neq x \cdot y\)
        \end{itemize}
  \item Le ripetizioni di un carattere all'interno di una stringa vengono abbreviate tramite elevazione a potenza
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(xx \rightarrow x^2\)
          \item \(yyy \rightarrow y^3\)
          \item \(yyyyxx \rightarrow y^4 x^2\)
        \end{itemize}
\end{enumerate}

\subsubsection{Sottostringhe}

Una stringa \(x\) è una \textbf{sottostringa} (detta anche \textit{fattore}) di una stringa \(s\) se esistono due stringhe \(y, z\)

\begin{gather*}
  y = y_1 y_2 \ldots y_m \\
  z = z_1 z_2 \ldots z_m
\end{gather*}

tali che:

\[ s = y x z \]

\bigskip
\textbf{Proprietà} delle sottostringhe:

\begin{itemize}
  \item sia \(y\) che \(z\) possono essere \(\epsilon\)
        \begin{itemize}
          \item se \(y = \epsilon \Rightarrow x\) è detta \textbf{prefisso} e \(s\) inizia con i caratteri di \(x\)
          \item se \(z = \epsilon \Rightarrow x\) è detta \textbf{suffisso} e \(s\) finisce con i caratteri di \(x\)
        \end{itemize}
  \item Se \(y = \epsilon, z = \epsilon\) allora \(x\) è \textit{uguale} a \(s\)
\end{itemize}

\subsection{Stella di Kleene}
\label{sec:stella-di-Kleene}

La \textbf{stella di Kleene} è un operatore unario che si applica a un insieme di simboli o a un insieme di stringhe.
Si indica con il simbolo \(\ast\) e si pronuncia \inlinequote{A star}.

Se \(A\) è un alfabeto, allora \(A^\ast\) è l'insieme di tutte le stringhe su simboli di \(A\), inclusa la stringa vuota \(\epsilon\) \textit{a patto che essa faccia parte dell'alfabeto}.
Non è imposto un limite superiore alla lunghezza delle stringhe prodotte.

\subsubsection{Definizione formale della Stella di Kleene}

È possibile definire la stella di Kleene tramite una trattazione più algebrica:

\begin{itemize}
  \item Un \textbf{semigruppo} è una coppia \(\langle S, \circ \rangle\) in cui:
        \begin{itemize}
          \item \(S\) è un \textbf{insieme} ed è \textbf{chiuso} rispetto a \(\circ\)
          \item \(\circ\) è un'operazione \textbf{associativa} su \(S\)
                \begin{itemize}[label=\(\rightarrow\)]
                  \item le operazioni possono essere associate a piacere
                  \item è \textit{distributiva} e \textit{commutativa}
                \end{itemize}
        \end{itemize}
  \item Un \textbf{monoide} è un semigruppo tale per cui:
        \begin{itemize}
          \item \( \exists \, u \, \forall \, x \, | \, x \circ u = u \circ x = x \)
          \item \(u\) è \textit{l'elemento neutro} rispetto all'operazione \(\circ\)
        \end{itemize}
  \item Un \textbf{gruppo} è un monoide che tale per cui:
        \begin{itemize}
          \item \( \forall \, x \, \exists \, x^{-1} \, | \, x \circ x^{-1} = x^{-1} \circ x = u\)
          \item \(x^{-1}\) è \textit{l'elemento inverso} di \(x\) rispetto all'operazione \(\circ\)
        \end{itemize}
\end{itemize}

Tra i \(3\) insiemi è valida la relazione:

\[ semigruppo \subseteq monoide \subseteq gruppo \]

\bigskip
A valle di queste definizioni, è possibile inoltre affermare che:

\begin{itemize}
  \item Dato un semigruppo \(\langle S, \circ \rangle\) e un sottoinsieme \(X\) di \(S\):
        \begin{itemize}
          \item \(X^+\) \textit{(detto \textquote{più di Kleene})} denota il sottoinsieme di \(S\) generato da \(X\), cioè tutte le sequenza della forma
                \[ x_1 \circ \ldots \circ x_n \quad x_i \in X, n \geq 1 \]
          \item Per un monoide \(\langle S, \circ \rangle\) con unità \(u\):
                \begin{itemize}
                  \item \(X^\ast = X^+ \cup \{u\}\)
                  \item \(X^\ast\) è detto il \textbf{monoide libero} generato da \(X\)
                \end{itemize}

        \end{itemize}
\end{itemize}

\subsection{Linguaggi}

È detto \textbf{linguaggio} un qualsiasi insieme di stringhe definite su un alfabeto.
Sono linguaggi:

\begin{itemize}
  \item Italiano, Inglese, Francese, \ldots
  \item \texttt{C}, \texttt{Java}, \texttt{Pascal}, \ldots
  \item Linguaggi grafici, Musica, Multimedia, \ldots
\end{itemize}

\textit{Inoltre:}

\begin{itemize}
  \item Una lingua come l'Italiano è \textit{infinita}, perché è possibile scrivere frasi di lunghezza infinita
  \item Analogamente, un linguaggio come il \texttt{C} è un insieme \textit{potenzialmente infinito} poiché l'insieme di programmi corretti è infinito
\end{itemize}

\textit{Formalmente}, un linguaggio \(L\) definito su un alfabeto \(A\) è un \textit{sottoinsieme} di \(A^\ast\).

I linguaggi formali, contrariamente a quanto possa sembrare, \textit{non sono solo rappresentazioni matematiche astratte}.
Essi infatti sono metodi utili a \textit{rappresentare} o \textit{comunicare} una informazione, quindi non solo stringhe senza significato.
Usando le operazioni descritte nella Sezione~\ref{sec:operazioni-linguaggi}, in base ai vari contesti, è possibile interpretare un linguaggio in modi consoni.
Anche i \textit{calcoli} possono essere rappresentati tramite linguaggi formali.

Esistono molti tipi di linguaggi, tra i quali si riconoscono soprattutto:

\begin{itemize}
  \item Linguaggi \textit{naturali}
  \item Linguaggi \textit{di programmazione}
  \item Linguaggi \textit{logici}
\end{itemize}

Il significato di queste definizioni verrà affrontato in seguito.

\subsubsection{Operazioni sui linguaggi}
\label{sec:operazioni-linguaggi}

Poiché un linguaggio non è altro che un insieme di stringhe, su di loro si applicano le operazioni insiemistiche.
Esse sono:

\begin{enumerate}
  \item \textbf{Unione} - \(\cup\)
  \item \textbf{Intersezione} - \(\cap\)
  \item \textbf{Differenza} - \(\backslash\) \textit{oppure} \(-\)
  \item \textbf{Complemento} - \(L^c\)
  \item \textbf{Concatenazione} - \(\cdot\)
  \item \textbf{Potenza n-esima} - \(L^n\)
  \item \textbf{Chiusura di Kleene} - \(L^\ast\)
\end{enumerate}

Le operazioni sui linguaggi creano nuove classi di linguaggi.
Esempi e le loro rispettive proprietà sono descritte nelle Sezioni seguenti (\ref{par:unione-di-linguaggi}~-~\ref{par:potenze-di-linguaggi}).

\paragraph{Unione}
\label{par:unione-di-linguaggi}

Siano \(L_1, L_2\) due linguaggi:

\[ L_1 \cup L_2 = \{w \, | \, w \in L_1 \lor w \in L_2 \} \]

\bigskip
\textit{Esempio}: siano \(L_1, L_2\) due linguaggi:

\[ L_1 = \{ \epsilon, a, b, c, bc, ca\} \qquad L_2 = \{ ba, bb, bc, ca, cb, cc\} \]

La loro unione sarà:

\[ L_1 \cup L_2 = \{\epsilon, a, b,c , ba, bb, bc, ca, cb, cc\} \]

\paragraph{Intersezione}

Siano \(L_1, L_2\) due linguaggi:

\[ L_1 \cup L_2 = \{w \, | \, w \in L_1 \land w \in L_2 \} \]

\bigskip
\textit{Esempio}: siano \(L_1, L_2\) due linguaggi:

\[ L_1 = \{ \epsilon, a, b, c, bc, ca\} \qquad L_2 = \{ ba, bb, bc, ca, cb, cc\} \]

La loro intersezione sarà:

\[ L_1 \cap L_2 = \{bc, ca\} \]

\paragraph{Differenza}

Siano \(L_1, L_2\) due linguaggi:

\[ L_1 \, \backslash \, L_2 = L_1 - L_2 = \left\{ w \, | \, w \in L_1, w \notin L_2 \right\} \]

È un'operazione che viene generalmente usata quando \(L_2 \subseteq L_1\).

\bigskip
\textit{Esempio:} siano \(L_1, L_2\) due linguaggi:

\[L_1 = \{ ba, bb, bc, ca, cb, cc\} \qquad L_2 = \{ bc, ca \} \]

La loro differenza sarà:

\[ L_1 \backslash L_2 = \{ba, bb, cb, cc\} \]

\paragraph{Complemento}

Sia \(L_1\) un linguaggio:

\[ \neg L_1 = \left\{ w \, | \, w \notin L_1 \right\} \]

Sia \(L\) un linguaggio definito su un alfabeto \(A\).
Allora la sua operazione di complementazione sarà definita come:

\[ L^c = A^\ast \backslash L \]

\paragraph{Concatenazione}

Siano \(L_1, L_2\) due linguaggi:

\[ L_1 \cdot L_2 = \{ w z \, | \, w \in L_1, z \in L_2 \} \]

\begin{itemize}
  \item L'operazione non è commutativa: \( L_1 \cdot L_2 \neq L_2 \cdot L_1 \)
  \item Si fa scorrere ogni elemento di \(L_1\) associandolo a ogni elemento di \(L_2\)
  \item La concatenazione di un linguaggio con un linguaggio vuoto dà origine al linguaggio stesso, mentre la concatenazione di un linguaggio con un insieme vuoto dà un insieme vuoto:
        \[ L \cdot \{\epsilon\} = L \quad L \cdot \emptyset = \emptyset \]
  \item Il numero delle stringhe in \(L_1 \cdot L_2\) sarà pari al prodotto del numero di stringhe in ciascun linguaggio:
        \[ | L_1 \cdot L_2 | = |L_1| \cdot |L_2| \]

\end{itemize}

\bigskip
\textit{Esempio:} siano \(L_1, L_2\) due linguaggi:

\[ L_1 = \{ \epsilon, a, b, c, bc, ca\} \qquad L_2 = \{ ba, bb, bc, ca, cb, cc \} \]

La loro concatenazione sarà:

\begin{align*}
  L_1 \cdot L_2 = \{ \  & ba, bb, bc, ca, cb, cc, aba, abb, abc, aca, acb, acc, bba, bbb, bbc, bca,    \\
                        & bcb, bcc, cba, cbb, cbc, cca, ccb, ccc, bcba, bcbb, bcbc, bcca, bccb, bccc , \\
                        & caba, cabb, cabc, caca, cacb, cacc \ \}
\end{align*}

\paragraph{Potenze}
\label{par:potenze-di-linguaggi}

Sia \(L\) un linguaggio. La sua potenza \(L^n\) sarà ottenuta \textit{concatenando} \(L\) con se stesso per \(n\) volte.

\[ \displaystyle L^n = \underbrace{L \cdot L \cdot \ldots \cdot L}_{n \text{ volte}} \]

\bigskip
\textit{Definizione induttiva:}
\begin{enumerate}
  \item \(L^0 = \{\epsilon\}\) \label{enum:potenze-caso-base}
  \item \(L^i = L^{i-1} \cdot L\) \label{enum:potenze-passo-induttivo}
\end{enumerate}

Si noti che il punto~\ref{enum:potenze-caso-base} prende il nome di \textbf{caso base}, mentre il punto~\ref{enum:potenze-passo-induttivo} si chiama \textbf{passo induttivo}.

Il \textit{passo induttivo} implica che almeno una parte del problema sia stato risolto (tramite \textbf{ipotesi induttiva}).
In questo caso, l'\textit{ipotesi induttiva} è data dalla relazione che riguarda \(L^{i-1}\).

\bigskip
\textit{Esempi:}
\begin{itemize}
  \item \(L^2 = L \cdot L\)
  \item \(L^3 = L \cdot L \cdot L\)
  \item \(L^4 = L \cdot L \cdot L \cdot L\)
\end{itemize}

\paragraph{Chiusura di Kleene}
\label{sec:proprieta-Kleene}

Sia \(L\) un linguaggio.
Allora l'operatore \textit{stella di Kleene} su \(L\) è definito come:

\[ L^\ast = \bigcup_{n=0}^{\infty} L^n \]

\bigskip
Analogamente, è possibile definire l'operatore \textit{più di Kleene}:

\[ L^+ = \bigcup_{n=1}^{\infty} L^n \]

\bigskip
\textit{Proprietà:}
\begin{enumerate}
  \item \(L^\ast = L^+ \cup L^0 = L^+ \cup \{\epsilon\}\)
  \item \(L^+ = L \cdot L^\ast\)
  \item \(L^+ = L^\ast \Leftrightarrow \epsilon \in L\)
\end{enumerate}

Poiché la concatenazione \textbf{non è commutativa}, la chiusura di Kleene \textbf{non è riflessiva}.

\clearpage

\section{Automi a stati finiti - \FSA}
\label{sec:automi-a-stati-finiti}

Un \textbf{automa a stati finiti} (o \FSA, dall'Inglese \textit{Finite State Automaton}) è il più semplice modello di astrazione.

Esso rappresenta un sistema che ammette un \textit{insieme finito} di stati (e di conseguenza un numero limitato di configurazioni).
In seguito ad un determinato ingresso (a sua volta formato da un insieme finito di valori), potrà avvenire una transizione tra due stati distinti.

Gli \FSA rappresentano il più semplice modello di computazione: molti dispositivi possono essere modellati come tali, seppure con alcune limitazioni.

Per poter usare gli \FSA per riconoscere linguaggi, è importante identificare:

\begin{enumerate}
  \item Le condizioni \textit{iniziali} del sistema
  \item Gli stati \textit{finali} del sistema
\end{enumerate}

Affinché sia possibile costruire un modello adatto, è necessario riconoscere gli elementi al suo interno:

\begin{itemize}
  \item Gli \textbf{stati}, tra i quali si identificano:
        \begin{itemize}
          \item Stato \textbf{iniziale}
          \item Stati \textbf{finali}
        \end{itemize}
  \item Le \textbf{transizioni}
  \item L'\textbf{ingresso}
\end{itemize}

\subsection{Stati, Transizioni e Ingressi}

Gli stati sono rappresentati come dei cerchi con all'interno la loro etichetta di riferimento.
La loro rappresentazione è illustrata in Figura~\ref{fig:stati-iniziale-finale-FSA}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, node distance=2cm, >=Triangle]
    \node [state, initial left, initial text=, minimum size=1.5cm](initial) {\footnotesize \texttt{iniziale}};
    \node [state, minimum size=1.5cm](generic)  [right=of initial] {\footnotesize \texttt{stato}};
    \node [state, minimum size=1.5cm, accepting] [right=of generic] {\footnotesize \texttt{finale}};
  \end{tikzpicture}
  \caption{Stati iniziali e finali di un \FSA}
  \label{fig:stati-iniziale-finale-FSA}
  \bigskip
\end{figure}

\bigskip
Un \FSA è definito su un alfabeto.
I simboli dell'alfabeto rappresentano l'ingresso del sistema.

\bigskip
Quando il sistema riceve un ingresso, cambia il proprio stato interno.
Il passaggio tra stati diversi avviene tramite \textbf{transizioni}.
Una \textbf{transizione} può avere un'etichetta che denomina l'azione che viene intrapresa nel momento della sua attivazione.

Una transizione è rappresentata mediante frecce, come in Figura~\ref{fig:transizioni-FSA}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, on grid, node distance=4cm, >=Triangle]
    \node [state, minimum size=1.5cm](on) {\footnotesize \texttt{ON}};
    \node [state, minimum size=1.5cm](off) [right=of on] {\footnotesize \texttt{OFF}};

    \path[->, thick]
    (on) edge [loop left] node {} ()
    (on) edge [] node {} (off);

  \end{tikzpicture}

  \caption{Transizioni in un \FSA}
  \label{fig:transizioni-FSA}
  \bigskip
\end{figure}

\subsection{Definizione formale \FSA}
\label{sec:definizione-formale-FSA}

Formalmente, un \FSA è una tupla di \(5\) elementi \(\langle Q, A, \delta, q_0, F \rangle\) dove:

\begin{itemize}
  \item \(Q\) è un insieme di \textbf{stati}, finito
  \item \(A\) è l'\textbf{alfabeto} di \textbf{ingresso}
  \item \(\delta\) è la \textbf{funzione} di \textbf{transizione}:
        \begin{itemize}
          \item \(\delta: Q \times A \rightarrow Q\)
          \item la funzione di transizione è detta \textbf{parziale} se non tutte le transizione da tutti i possibili stati per tutti i possibili elementi dell'alfabeto sono definite
          \item un \FSA con una funzione di transizione totale è detto \textbf{completo}
        \end{itemize}
  \item \(q_0 \in Q\) è lo \textbf{stato iniziale}
  \item \(F \subseteq Q\) è l'insieme di \textbf{stati finali}
\end{itemize}

Si noti che nonostante sia definire \textbf{un solo} stato iniziale, un \FSA ammette \textbf{più} stati finali.
Inoltre, uno stato può essere contemporaneamente iniziale e finale.

\subsubsection{\FSA con transizione totale}

Come già enunciato, una funzione di transizione completa implica che essa sia definita per tutti i possibili stati per ogni possibile elemento dell'alfabeto di ingresso.
Il \FSA in Figura~\ref{fig:esempio-funzione-transizione-totale-FSA} ha una funzione di transizione \textit{totale}, mentre quello in Figura~\ref{fig:esempio-funzione-transizione-parziale-FSA} presenta una funzione di transizione \textit{parziale}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{subfigure}[t]{0.62\textwidth}
    \centering
    \scalebox{0.9}{
      \begin{tikzpicture}[auto, on grid, node distance=4cm, >=Triangle]
        \node [state](on) {\texttt{ON}};
        \node [state](off) [right=of on] {\texttt{OFF}};

        \path[->, thick]
        (on) edge [loop left] node {\texttt{switch on}} ()
        (off) edge [loop right] node {\texttt{switch off}} ()
        (on) edge [bend left] node {\texttt{switch off}} (off)
        (off) edge [bend left] node {\texttt{switch on}} (on);
      \end{tikzpicture}
    }
    \caption{\FSA con funzione di transizione totale}
    \label{fig:esempio-funzione-transizione-totale-FSA}
  \end{subfigure}
  \begin{subfigure}[t]{0.37\textwidth}
    \centering
    \scalebox{0.9}{
      \begin{tikzpicture}[auto, on grid, node distance=4cm, >=Triangle]
        \node [state](on) {\texttt{ON}};
        \node [state](off) [right=of on] {\texttt{OFF}};

        \path[->, thick]
        (on) edge [bend left] node {\texttt{switch off}} (off)
        (off) edge [bend left] node {\texttt{switch on}} (on);
      \end{tikzpicture}
    }
    \caption{\FSA con funzione di transizione parziale}
    \label{fig:esempio-funzione-transizione-parziale-FSA}
  \end{subfigure}
  \bigskip
\end{figure}

Il \FSA in Figura~\ref{fig:esempio-funzione-transizione-parziale-FSA}, al contrario di quello in Figura~\ref{fig:esempio-funzione-transizione-totale-FSA}, non presenta le transizioni in corrispondenza dell'azione di \texttt{switch off} sullo stato \texttt{off} e \texttt{switch on} sullo stato \texttt{on}.

\subsubsection{Sequenza di mosse}
\label{sec:sequenza-di-mosse}

Una \textbf{sequenza di mosse} inizia da uno stato iniziale ed è di \textbf{accettazione} se raggiunge uno degli stati \textit{finali}.

\bigskip
\textit{Formalmente:}
\begin{itemize}
  \item \textbf{Sequenza di mosse}:
        \begin{itemize}
          \item \(\delta^\ast : Q \times A^\ast \rightarrow Q\)
          \item Il \textbf{dominio} è definito come il prodotto cartesiano tra stati e stella di Kleene di sequenze di simboli
          \item Il \textbf{codominio} coincide con l'insieme degli stati
        \end{itemize}
  \item \(\delta^\ast\) è definita induttivamente a partire da \(\delta\):
        \begin{enumerate}
          \item \(\delta^\ast(q, \epsilon) = q\)
          \item \(\delta^\ast(q, yi) = \delta\left(\delta^\ast(q, y), i\right)\)
        \end{enumerate}
  \item Stato \textbf{iniziale} \(q_0 \in Q\)
  \item Stati \textbf{finali} \( F \subseteq Q \)
\end{itemize}

\subsubsection{Condizione di accettazione di un \FSA}

Il linguaggio relativo al \FSA è costituito dalle stringhe \(x\) che appartengono a \(\delta^\ast\).
\textit{Formalmente}:

\[ \forall \, x \in L \Leftrightarrow \delta^\ast (q_0, x) \in F \]

I linguaggi riconosciuti dagli \FSA, come verrà analizzato più in dettaglio in seguito \textit{(per esempio nella Sezione~\ref{sec:gerarchia-di-Chomsky})}, prendono il nome di \textbf{regolari}.

\subsection{Trasduttori a stati finiti - \FST}
\label{sec:trasduttori-a-stat-finiti}

\textit{Idea}: usare gli automi come traduttori di linguaggi.
Nasce il \textbf{FST} \textit{(dall'Inglese finite state transducer)}, cioè un \FSA che lavora su due nastri.
È una specie di macchina traduttrice.

La rappresentazione più semplificata di un \textit{FST} è rappresentata in Figura~\ref{fig:diagramma-FST}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-1.tikz}
  \caption{Diagramma semplificato di un FST}
  \label{fig:diagramma-FST}
  \bigskip
\end{figure}

Un \FST è composto da:

\begin{enumerate}
  \item Una stringa di \textbf{ingresso} \(x\)
  \item Una stringa di \textbf{uscita} \(y\)
  \item Una funzione \(\tau: L_1 \rightarrow L_2 \) tale che \(y = \tau(x)\)
\end{enumerate}

\subsubsection{Definizione formale \FST}

Un trasduttore a stati finiti (\FST) è una tupla di \(7\) elementi \(\langle Q, A, \delta, q_0, F, O, \eta \rangle\):

\begin{itemize}
  \item \(Q\) è un insieme finito di \textbf{stati}
  \item \(A\) è l'\textbf{alfabeto} di \textbf{ingresso}
  \item \(\delta\) è la \textbf{funzione} di \textbf{transizione}:
        \begin{itemize}
          \item \(\delta: Q \times A \rightarrow Q\)
        \end{itemize}
  \item \(q_0 \in Q\) è lo \textbf{stato iniziale}
  \item \(F \subseteq Q\) è l'insieme di \textbf{stati finali}
  \item \(O\) è l'\textbf{alfabeto} di \textbf{uscita}
  \item \(\eta\) è la \textbf{funzione} di \textbf{uscita}:
        \begin{itemize}
          \item \(\eta: Q \times I \rightarrow O^\ast\)
          \item il \textbf{dominio} è il prodotto cartesiano degli stati per l'alfabeto di ingresso
          \item il \textbf{codominio} è la stella di Kleene dell'alfabeto di uscita
        \end{itemize}
\end{itemize}

La definizione di \(Q, A, \delta, q_0, F\) è analoga a quella avvenuta nella Sezione~\ref{sec:definizione-formale-FSA} riguardante gli \FSA.

La condizione di accettazione resta la stessa degli accettori.
Di conseguenza, \textbf{la traduzione avviene solo su stringhe accettate}.

\subsubsection{Traduzione di una stringa}

Analogamente a quanto già illustrato nella definizione della sequenza di mosse \(\delta\) (Sezione~\ref{sec:sequenza-di-mosse}), \(\eta^\ast\) verrà definito induttivamente.
Infatti, ricordando che \(\eta^\ast : Q \times I^\ast \rightarrow O^\ast \):

\begin{enumerate}
  \item \(\eta^\ast(q, \epsilon) = \epsilon\)
  \item \(\eta^\ast(q, y \cdot i) = \eta^\ast(q, y) \cdot \eta\left(\delta^\ast(q, y), i\right)\)
\end{enumerate}

sarà quindi valida la relazione:

\[\forall \, x, \tau(x) = \eta^\ast (q_0, x) \Leftrightarrow \delta^\ast(q_0, x) \in F\]

\subsubsection{Pumping lemma}
\label{sec:pumping-lemma}

Si consideri il \FSA in Figura~\ref{fig:pumping-lemma-FSA}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, on grid, node distance=2cm, >=Triangle]
    \node [state, initial, initial text = ](q0) {\(q_0\)};
    \node [state](q1) [right=of q0] {\(q_1\)};
    \node [state](q2) [right=of q1] {\(q_2\)};
    \node [state, accepting](q9) [right=of q2] {\(q_9\)};
    \node [state](q5) [below=of q0] {\(q_5\)};
    \node [state](q4) [right=of q5] {\(q_4\)};
    \node [state](q3) [right=of q4] {\(q_3\)};
    \node [state, accepting](q8) [right=of q3] {\(q_8\)};
    \node [state](q6) [below=of q4] {\(q_6\)};
    \node [state](q7) [right=of q6] {\(q_7\)};

    \path[->]
    (q0) edge [] node {b} (q1)
    (q2) edge [] node {b} (q9)
    (q1) edge [] node {b} (q7)
    (q9) edge [] node {b} (q7)
    (q9) edge [] node {a} (q8)
    (q4) edge [] node {a} (q5);

    \path[->, thick, draw=red, fill=red]
    (q1) edge [] node {a} (q2)
    (q2) edge [] node {a} (q3)
    (q3) edge [] node {b} (q6)
    (q6) edge [] node {a} (q4)
    (q4) edge [] node {b} (q1);

  \end{tikzpicture}
  \caption{Pumping lemma}
  \label{fig:pumping-lemma-FSA}
  \bigskip
\end{figure}

Se si ammette la possibilità che il ciclo \textcolor{red}{\(q_1 \rightarrow q_2 \rightarrow q_3 \rightarrow q_6 \rightarrow q_4 \rightarrow q_1 \rightarrow \ldots\)} venga attraversato una volta, allora è possibile che esso venga attraversato un numero indefinito \textit{(potenzialmente infinito)} di volte.

Più formalmente:

\begin{itemize}
  \item Se \(x \in L, |x| \geq |Q| \), allora esistono uno stato \(q \in Q\) a una stringa \(w \in I^+\) tali che:
        \begin{itemize}
          \item \(x = ywz\)
          \item \(\delta^\ast (q, w) = q \)
        \end{itemize}
  \item Perciò varrà anche \(\forall \, n \geq 0 \ y w^n z \in L\)
\end{itemize}

Questo fenomeno prende il nome di \textbf{pumping lemma}, e porta a due possibili \textit{conseguenze}:

\begin{enumerate}
  \item \(L = \emptyset\)
        \begin{itemize}
          \item \(\exists \, x \in L \Leftrightarrow \exists \, y \in L, \ |y| < |Q|\)
          \item È sufficiente rimuovere tutti i \textit{cicli} dall'\FSA che accetta \(x\)
        \end{itemize}
  \item \(L = \infty\)
        \begin{itemize}
          \item Il \FSA deve verificare in modo analogo se \(\exists \, x \in L, \ |Q| \leq |x| < 2 \cdot |Q| \)
          \item \textit{La lunghezza della stringa \(x\) è tale da compiere meno di due cicli}
        \end{itemize}
\end{enumerate}

\bigskip
Nella pratica, ciò porta alle seguenti \textit{implicazioni}:

\begin{itemize}
  \item Un linguaggio di programmazione che ammette \(0\) programmi corretti \textbf{non è rilevante}
  \item Analogamente, un linguaggio di programmazione che ammette \textit{un numero finito} di programmi corretti \textbf{non è rilevante}
  \item[\(\Rightarrow\)] Il \textit{pumping lemma}, \textbf{quindi, non unicamente è un aspetto negativo} poiché permette di creare linguaggi \textbf{infiniti}
\end{itemize}

\paragraph{Conseguenza negativa del \textit{pumping lemma}}
\label{par:conseguenza-negativa-pumping-lemma}

Si consideri il linguaggio
\[ L = \left\{ a^n b^n \, | \, n > 0 \right\} \]
e si supponga che sia riconosciuto da un qualsiasi \FSA.

\bigskip
Si consideri ora la stringa
\[ x = a^m b^m, \  m > |Q| \]
e si applichi il \textit{pumping lemma}.
Come conseguenza dello stesso, poiché la lunghezza della stringa è superiore al numero di stati, dovrebbe esistere una costante \(k\) per la quale, se \(x \in L, |x| \geq k\), la stessa \(x\) può essere scritta come \(ywz\) con \(1 \leq |w| \leq k, yw^iz \in L \, \forall \, i \geq 0\).

La dimostrazione avviene \textit{per assurdo}.
Scomponendo \(x\), si possono identificare \(3\) possibili forme di essa che verranno accettate:

\begin{itemize}
  \item \(x = ywz, w = a^r, r > 0\), quindi anche \(a^{s + k \cdot r} b^n \, \forall \, k \geq 0\) \textbf{dovrebbe essere accettato}
  \item \(x = ywz, w = b^r, r > 0\), quindi anche \(a^n b^{s + k \cdot r} \, \forall \, k \geq 0\) \textbf{dovrebbe essere accettato}
  \item \(x = ywz, w = a^r b^s, r > 0, s > 0\), quindi anche \(a^{n - r} (a^r b^s)^k b^{n-s} \, \forall \, k \geq 0\) \textbf{dovrebbe essere accettato}
\end{itemize}

Tutte e tre queste forma, tuttavia, violano la forma di \(x\) indicata nell'ipotesi.
Quindi si può affermare che esistono linguaggi non riconoscibili tramite \FSA.

Più precisamente, esistono dei \textit{linguaggi infiniti} che non possono essere riconosciuti dagli \FSA.
Questa affermazione può essere giustificata informalmente affermando che \inlinequote{gli \FSA non possono contare}.

La famiglia dei linguaggi che sono riconosciuti dagli \FSA prende il nome di \textbf{regolare}.

\paragraph{Provare che un linguaggio sia infinito}

Per poter provare che:

\begin{itemize}
  \item Il linguaggio riconosciuto dal \FSA sia infinito, bisogna provare le infinite stringhe che appartengono al linguaggio su cui esso è definito
  \item Il linguaggio riconosciuto dal \FSA sia vuoto, bisogna provare le infinite stringhe che appartengono al linguaggio su cui esso è definito
\end{itemize}

Non è possibile effettuare un test con un numero infinito di stringhe in un tempo finito.
Quindi, per effettuare una ricerca esaustiva e trovare una risposta a questa domanda, si provano le stringhe strettamente inferiori al numero di stati:

\begin{itemize}[label=\(\Rightarrow\)]
  \item Se una stringa \textbf{viene accettata}, allora \textbf{tutte} le stringhe verranno accettate
  \item Se una stringa \textbf{non viene accettata}, allora \textbf{nessuna} stringa verrà accettata
\end{itemize}

Il numero di queste stringhe, e quindi il numero di test da effettuare, è limitato.

\paragraph{Provare la presenza di un ciclo}

Se un automa accetta un certo linguaggio, allora una stringa di lunghezza superiore al numero di stati verrà accettata se e solo se è presente un ciclo all'interno del corrispondente \FSA.

\subsection{Operazioni sugli \FSA}

Prima di poter definire le operazioni sugli \FSA, è necessario definire il concetto di \textbf{chiusura}:

Chiusura in \textit{matematica}: un insieme \(S\) è \textbf{chiuso} rispetto a una operazione \(OP\) se, quando \(OP\) è applicata agli elementi di \(S\), il risultato è ancora un elemento di \(S\)

Leggermente diverso è il caso dei \textit{linguaggi}, che verrà esaminato più nel seguente paragrafo.

\subsubsection{Chiusura per i linguaggi}

Siano:

\begin{itemize}
  \item \(\mathcal{L} = \{L_i\}\) una \textit{famiglia} di \textbf{linguaggi}
        \begin{itemize}
          \item \(\mathcal{L}\) è chiuso rispetto all'operazione \(OP\) se e solo se, \(\forall \, L_1, L_2 \in \mathcal{L}, \ L_1 \ OP \ L_2 \in \mathcal{L} \)
        \end{itemize}
  \item \(\mathcal{R}\) la \textit{famiglia} dei \textbf{linguaggi regolari}
        \begin{itemize}
          \item \(\mathcal{R}\) è chiuso rispetto alle operazioni insiemistiche, alla concatenazione e all'operatore \(\ast\) \textit{(stella di Kleene)}
        \end{itemize}
\end{itemize}

\paragraph{Intersezione}
\label{par:intersezione-FSA}
% pagina 108 libro, slide 32 1-02.FSA

Siano \(A_1\) e \(A_2\) due generici \FSA, caratterizzati dalle loro espressioni

\begin{align*}
  A_1 & = \langle Q_1, I_1, \delta_1, q_{01}, F_1 \rangle \\
  A_2 & = \langle Q_2, I_2, \delta_2, q_{02}, F_2 \rangle
\end{align*}

Si supponga che \( I_2  = I_1 = I \) e che \(Q_1 \cap Q_2 = \emptyset \).
Entrambe le supposizioni non comportano alcuna perdita di generalità perché:

\begin{enumerate}
  \item Se \(I_1\) ed \(I_2\), fossero diversi, posto che la funzione di transizione non deve essere necessariamente uguale, i due atomi possono essere considerati definiti su \(I_1 \cup I_2\)
  \item Se \(Q_1\) e \(Q_2\) non fossero disgiunti, i nomi degli stati comuni possono essere banalmente cambiati
\end{enumerate}

\FSA che riconosce entrambi i linguaggi (e quindi \(I_1 \cap I_2\)) è costruito come:

\begin{gather*}
  C = \langle A_1, A_2 \rangle = \langle Q_1 \times Q_2, I, \delta, \langle q_{01}, q_{02} \rangle, F_1 \times F_2 \rangle \\
  \delta(\langle q_1, q_2 \rangle, i)  = \langle \delta_1 (q_1, i), \delta_2(q_2, i) \rangle
\end{gather*}

\bigskip
La dimostrazione che
\[ L \left( \langle A_1, A_2 \rangle \right) = L\left(A_1\right) \cap L\left(A_2\right) \]
può avvenire un ragionamento per induzione alla funzione \(\delta\).

\bigskip
Intuitivamente, l'\textit{esecuzione parallela} di due \FSA può essere simulata \textit{accoppiandoli} tramite prodotto cartesiano.
L'intersezione di due \FSA può essere vuota, non riconoscendo alcun linguaggio (ricordando che \(\emptyset \neq \{\epsilon\}\)).

\bigskip
\textit{Esempio:} siano \(A\) e \(B\) due \FSA rappresentati come segue:

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, on grid, node distance=2cm, >=Triangle]
    \node [state, initial, initial text = A](q0) {\(q_0\)};
    \node [state](q1) [right=of q0] {\(q_1\)};
    \node [state](q2) [right=of q1] {\(q_2\)};
    \node [state, accepting](q3) [right=of q2] {\(q_3\)};

    \node [state, initial, initial text = B](p0) [below=of q0] {\(p_0\)};
    \node [state](p1) [right=of p0] {\(p_1\)};
    \node [state](p2) [right=of p1] {\(p_2\)};
    \node [state, accepting](p3) [right=of p2] {\(p_3\)};

    \path[->, thick]
    (q0) edge [] node {} (q1)
    (q1) edge [] node {} (q2)
    (q2) edge [] node {} (q3);

    \path[->, thick]
    (p0) edge [] node {} (p1)
    (p1) edge [] node {} (p2)
    (p2) edge [] node {} (p3);

  \end{tikzpicture}
  \caption{Intersezione tra FSA}
  \label{fig:intersezione-FSA-1}
  \bigskip
\end{figure}

La loro \textit{intersezione} \(C = A \cap B\) può essere rappresentata come segue:

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, on grid, node distance=2cm, >=Triangle]
    \node [state, initial, initial text = \(C\)](q0) {\(q_0 p_0\)};
    \node [state](q1) [right=of q0] {\(q_1 p_1\)};
    \node [state](q2) [right=of q1] {\(q_2 p_2\)};
    \node [state, accepting](q3) [right=of q2] {\(q_3 p_3\)};

    \path[->, thick]
    (q0) edge [] node {} (q1)
    (q1) edge [] node {} (q2)
    (q2) edge [] node {} (q3);

  \end{tikzpicture}
  \caption{Intersezione tra FSA}
  \label{fig:intersezione-FSA-2}
  \bigskip
\end{figure}

\paragraph{Unione}
\label{par:FSA-unione}
% pagina 111 libro, slide 35 1-02.FSA
Analogamente a quanto avviene nell'intersezione (Sezione~\ref{par:intersezione-FSA}), con le stesse ipotesi, l'unione di due \FSA:

\begin{align*}
  A_1 & = \langle Q_1, I_1, \delta_1, q_{01}, F_1 \rangle \\
  A_2 & = \langle Q_2, I_2, \delta_2, q_{02}, F_2 \rangle
\end{align*}

La loro unione sarà data da:

\begin{gather*}
  C = \langle A_1, A_2 \rangle = \langle Q_1 \times Q_2, I, \delta, \langle q_{01}, q_{02} \rangle, F_1 \times Q_2 \cup Q_1 \times F_2 \rangle \\
  \delta(\langle q_1, q_2 \rangle, i) = \langle \delta_1 (q_1, i), \delta_2(q_2, i) \rangle
\end{gather*}

\paragraph{Complemento}
% pagina 109 libro, slide 36 1-02.FSA
Si consideri il \FSA:

\[ A = \langle Q, I, \delta, q_0, F \rangle \]

Per poterlo complementare, è prima necessario costruire un \FSA \(A^\prime\) aggiungendo un nuovo stato \(\overline{q}\) ad \(A\) in modo che la funzione di transizione di \(A^\prime\) conduca a \(\overline{q}\) ogni qualvolta che è indefinita in \(A\).
Si imponga inoltre che l'automa, una volta in \(\overline{q}\), ci rimanga per qualsiasi simbolo di ingresso.

In questo modo, la funzione di transizione di \(A\) (e quindi la sua corrispondente \(\delta^\ast)\) è totale.

\textit{Proprietà del complemento:}

\begin{itemize}
  \item Se l'intera stringa di ingresso viene scandita, allora per complementare il risultato basta \textit{scambiare il} \texttt{si} \textit{con il} \texttt{no}
  \item Se la fine della stringa non viene raggiunta, allora lo scambio di \(F\) con \(Q - F\) non funziona
  \item Nel caso degli \FSA il \inlinequote{trucco} consiste nel completare la funzione di transizione \(\delta\)
  \item In generale, \textit{non è possibile considerare equivalenti la risposta negativa a una domanda e la risposta positiva della domanda opposta}
\end{itemize}

Grazie a questa operazione, è possibile definire l'unione di due \FSA (Sezione~\ref{par:FSA-unione}) tramite le leggi di \textit{De Morgan}:

\[ L(A_1) \cup L(A_2) = \overline{\overline{L(A_1)} \cap \overline{L(A_2)}} = \left(L(A_1)^C \cup L(A_2)^C\right)^C \]

con il medesimo risultato.

\clearpage

\section{Pushdown automaton - \PDA}

Come mostrato prima nell'applicazione del \textit{pumping lemma} al linguaggio \( L = \left\{ a^n b^n \, | \, n > 0 \right\}\) (Sezione~\ref{par:conseguenza-negativa-pumping-lemma}), dimostra l'impossibilità di riconoscere alcuni linguaggi tramite \FSA, a causa della loro inabilità nel contare una quantità di simboli sconosciuta a priori.

A causa di questa mancanza, essi sono inadatti al riconoscimento di molti linguaggi di interesse pratico.
Per ovviare a questo limite, vengono introdotti i \PDA (in Italiano \textit{automi a pila} or \textit{AP}), il cui diagramma semplificato è mostrato in Figura~\ref{fig:diagramma-PDA}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-2.tikz}
  \caption{Diagramma semplificato di un PDA}
  \label{fig:diagramma-PDA}
  \bigskip
\end{figure}

La \textbf{pila} è una forma di memoria in cui:

\begin{itemize}
  \item I nuovi simboli sono \textbf{inseriti in cima}
  \item La pila viene \textbf{letta dalla cima}
  \item Un simbolo letto viene \textbf{estratto dalla cima}
        \begin{itemize}
          \item[\(\rightarrow\)] si attua una politica \LIFO
        \end{itemize}
  \item L'ultimo elemento in basso della pila è occupato da un particolare simbolo \(Z_0\)
\end{itemize}

I \PDA differiscono dagli automi a stati finiti in due modi:

\begin{enumerate}
  \item Possono usare la cima della pila per decidere quale transizione effettuare
  \item Possono manipolare la pila durante una transizione
\end{enumerate}

\subsection{Definizione formale \PDA}
\label{sec:definizione-formale-PDA}

Formalmente, un \PDA é una tupla di \(7\) elementi \( \langle Q, A, \Gamma, \delta, q_0, Z_0, F \rangle \), dove:

\begin{itemize}
  \item \(Q\) è un \textbf{insieme di stati}, finito
  \item \(A\) è l'\textbf{alfabeto} di ingresso
  \item \(\Gamma\) è l'\textbf{alfabeto} di pila
  \item \(\delta\) è la funzione di \textbf{transizione}:
        \begin{itemize}
          \item \(\delta: Q \times \left(I \, \cup \, \{\epsilon\} \right) \times \Gamma \rightarrow Q \times \Gamma^\ast\)
          \item Il \textbf{dominio} è definito come il prodotto cartesiano tra stati, alfabeto di ingresso unito all'elemento nullo e alfabeto di pila
          \item Il \textbf{codominio} è definito come il prodotto cartesiano tra stati e \textit{stella di Kleene} dell'alfabeto di pila
        \end{itemize}
  \item \(Z_0 \in \Gamma\) è il \textbf{simbolo iniziale} (il primo in \textit{basso}) di pila
  \item \(q_0 \in Q\) è lo \textbf{stato iniziale}
  \item \(F \subseteq Q\) è l'insieme di \textbf{stati finali}, finito
\end{itemize}

Si noti che la definizione di \(Q, A, \delta, q_0, F\) sono analoghi a quanto illustrato in Sezione~\ref{sec:definizione-formale-FSA} riguardo gli \FSA.

\subsection{Mosse di un \PDA}

In base a:

\begin{itemize}
  \item Il simbolo \textbf{letto} dall'ingresso \textit{(opzionalmente)}
  \item Il simbolo \textbf{letto} dalla cima della pila
  \item Lo \textbf{stato} del dispositivo di controllo
\end{itemize}

il \PDA:

\begin{itemize}
  \item Cambia il proprio \textbf{stato}
  \item Sposta in avanti la \textbf{stato}
  \item Sostituisce il simbolo letto dalla \textbf{pila} con una stringa \(\alpha\) \textit{eventualmente vuota}
\end{itemize}

Sono anche ammesse delle delle mosse \textit{spontanee} (dette \(\epsilon\)-mosse) che avvengono anche senza che venga letto il simbolo dalla stringa, ignorandolo.
Per queste mosse la funzione di transizione è definita come:
\[ \delta(q, \epsilon, A) = \langle p, \alpha \rangle \]
Se una \(\epsilon\)-mossa è stata definita per una coppia \(\langle \overline{Q}, \overline{A} \rangle\) \textit{(stato e simbolo in cima alla pila)}, non è possibile definire un'altra mossa non-\(\epsilon\) per lo stesso simbolo \(\overline{A}\) dallo stesso stato \(\overline{Q}\).

\textbf{Gli \FSA non hanno \(\epsilon\)-mosse}, in quanto esse sono una caratteristica dei \PDA.
Essi non implicano che il carattere in cima alla stringa sia \(\epsilon\) ma che la stringa non viene letta.

\subsection{Accettazione di una stringa}

Alla fine della computazione, la stringa di ingresso \(x\) è \textit{riconosciuta} (o \textit{accettata}) se valgono entrambe le condizioni:

\begin{enumerate}
  \item Il \PDA la legge \textbf{completamente}
  \item Il \PDA si trova in uno stato di accettazione (\textbf{finale}) quando la fine di \(x\) è stata raggiunta
\end{enumerate}

Non ci sono condizioni sulla pila sia vuota affinché la stringa \(x\) sia accettata.
In particolare, \textbf{non è necessario che la pila sia vuota} alla fine della computazione \textit{(anche se è una condizione che si può verificare)}.

\subsection{Configurazione di un \PDA}
\label{sec:configurazione-PDA}
% slide 26 - 1-03PDA.pdf

Una \textbf{configurazione} è una generalizzazione della nozione di stato.
Essa mostra:

\begin{itemize}
  \item Lo \textbf{stato corrente} del dispositivo di controllo
  \item La \textbf{porzione di stringa} di ingresso a destra dalla testina
        \begin{itemize}
          \item indica la parte di essa ancora non letta
          \item ciò chè è già stato letto non è rilevante poiché è già stato consumato
        \end{itemize}
  \item Il \textbf{contenuto} della pila
\end{itemize}

Può essere vista come una \textit{istantanea} del \PDA, mostrando nel tempo il suo stato interno.

\bigskip
\textit{Formalmente}, la configurazione \(c\) è una tripla \(\langle q, x, \gamma \rangle\):

\begin{itemize}
  \item \(q \in Q\) è lo \textbf{stato corrente} del dispositivo di controllo
  \item \(x \in I^\ast\) è la \textbf{porzione non letta} della stringa di ingresso
  \item \(\gamma \in \Gamma^\ast\) è la \textbf{stringa di simboli} nella pila.
\end{itemize}

\subsection{Transizioni di un \PDA}
% slide 28 1-03PDA.pdf

Le \textbf{transizioni} tra configurazioni dipendono dalla funzione di transizione e si indicano con il simbolo \(\vdash\).
Una sequenza di transizioni è indicata col simbolo \(\vdash^\ast\).

Esse illustrano come commutare tra una \textit{istantanea} e la sua successiva.
Per un dato \PDA \(A\), la transizione \(\vdash_A\) nello spazio di tutte le possibili configurazioni di \(A\) è definita da
\[ c = \langle q, x, \gamma \rangle \, \vdash_A c^\prime  = \langle q^\prime , x^\prime , \gamma^\prime  \rangle \]
solo se vale una delle due condizioni:

\begin{enumerate}
  \item La funzione di transizione è definita per un simbolo di ingresso
        \begin{align*}
          x = ay           & \mapsto \ x{'} = y                    \\
          \gamma = A \beta & \mapsto \gamma^\prime  = \alpha \beta \\
          \delta(q, a, A)  & =  \langle q^\prime , \alpha \rangle
        \end{align*}
  \item La funzione di transizione è definita per una \(\epsilon\)-mossa
        \begin{align*}
          x                      & \mapsto x{'},                         \\
          \gamma       = A \beta & \mapsto \gamma^\prime  = \alpha \beta \\
          \delta(q, \epsilon, A) & = \langle q^\prime , \alpha \rangle
        \end{align*}
\end{enumerate}

\subsubsection{Nota sulle transizioni}

Si consideri un generico \PDA con le caratteristiche già indicate, sia \(\delta\) la sua funzione di transizione.

Si indica con il simbolo \(\perp\) il risultato di una transizione che
\textbf{non è definita} per i parametri indicati.

Allora:

\begin{itemize}
  \item Una \(\epsilon\)-mossa è una mossa \textbf{spontanea}:
        \begin{itemize}
          \item se \(\delta(q, \epsilon, A) \neq \bot \) \textit{(\(\delta\) non è indefinita)} e \(A\) è il simbolo in cima alla pila, la transizione \textbf{può sempre essere eseguita}
        \end{itemize}
  \item Se \(\delta(q, \epsilon, A) \neq \bot\), allora \(\forall \, i \in I, \delta(q, i, A) = \bot\)
        \begin{itemize}
          \item se questa proprietà non fosse soddisfatta, entrambe le transizioni \textbf{sarebbero consentite}
          \item questa condizione \textbf{garantisce il determinismo} poiché non sarebbe possibile definire unicamente quale delle più transizioni scegliere partendo dallo stato
          \item più avanti verrà mostrato il funzionamento degli automi in assenza di determinismo (Sezione~\ref{sec:non-determinismo})
        \end{itemize}
\end{itemize}

\subsubsection{Notazione grafica delle transizioni di un \PDA}

Il diagramma mostrato in Figura~\ref{fig:notazione-grafica-PDA-push} mostra una transizione di un \PDA da uno stato \(q_0\) a uno stato \(p_0\).

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, on grid, node distance=4cm, >=Triangle]
    \node [state, initial, initial text=](q) {\(q_0\)};
    \node [state](p) [right= of q] {\(p_0\)};

    \path [->, thick]
    (q) edge [] node {\(a, B \, | \, CB \)} (p);

  \end{tikzpicture}
  \caption{Notazione grafica dei \PDA, \textit{push}}
  \label{fig:notazione-grafica-PDA-push}
  \bigskip
\end{figure}

La notazione implica che se:

\begin{enumerate}
  \item Il \PDA si trova nello \textbf{stato} \(q_0\)
  \item Il \textbf{carattere} \(a\) viene letto dalla stringa di input
  \item Il \textbf{carattere} \(B\) è in cima alla pila
\end{enumerate}

allora succederanno i seguenti eventi:

\begin{enumerate}
  \item Il carattere \(C\) viene \textbf{aggiunto} in cima alla pila \textit{(operazione di push)}
  \item Lo stato attuale diventa \(p_0\)
\end{enumerate}

\bigskip

Il diagramma mostrato in Figura~\ref{fig:notazione-grafica-PDA-pop} mostra una transizione di un \PDA da uno stato \(q_1\) a uno stato \(p_1\).

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, on grid, node distance=4cm, >=Triangle]
    \node [state, initial, initial text=](q) {\(q_1\)};
    \node [state](p) [right= of q] {\(p_1\)};

    \path [->, thick]
    (q) edge [] node {\(a, B \, | \, \epsilon\)} (p);

  \end{tikzpicture}
  \caption{Notazione grafica dei \PDA, \textit{pop}}
  \label{fig:notazione-grafica-PDA-pop}
  \bigskip
\end{figure}

La notazione implica che se:

\begin{enumerate}
  \item Il \PDA si trova nello \textbf{stato} \(q_1\)
  \item Il \textbf{carattere} \(a\) viene letto dalla stringa di input
  \item Il \textbf{carattere} \(B\) è in cima alla pila
\end{enumerate}

allora succederanno i seguenti eventi:

\begin{enumerate}
  \item Il carattere \(B\) viene \textbf{rimosso} dalla pila \textit{(operazione di pop)}
  \item Lo stato attuale diventa \(p_1\)
\end{enumerate}

\subsection{Condizione di accettazione di un \PDA}

Sia \(\vdash^\ast\) la \textit{chiusura riflessiva} e \textit{transitiva} della relazione \(\vdash\).
La condizione di accettazione è:
\[ \forall \, x \in I^\ast \, | \, x \in L \Leftrightarrow c_0 = \langle q_0, x, Z_0 \rangle \vdash^\ast c_F = \langle q, \epsilon, \gamma \rangle, \ q \in F \]

in cui:
\begin{enumerate}
  \item La configurazione iniziale parte dallo \textbf{stato iniziale} con la \textbf{stringa ancora interamente da leggere} e la \textbf{pila vuota}
  \item Viene applicato un certo numero di \textbf{transizioni}
  \item La configurazione finale ha la \textbf{stringa completamente letta} e lo \textbf{stato attuale è nell'insieme degli stati finali} mentre non ci sono condizioni sullo stato della pila
\end{enumerate}

\textit{Informalmente}, una stringa è accettata da un \PDA se esiste un cammino coerente con \(x\) su di esso che va dallo stato iniziale allo stato finale.
La stringa deve essere letta in tutta la sua interezza.

\subsection{\PDA vs \FSA}

Alcuni linguaggi, a causa del \textit{pumping lemma}, non possono essere riconosciuti da un \FSA.
Essi tuttavia possono essere riconosciuti da un \PDA e prendono il nome di \textbf{linguaggi regolari}.

I \PDA sono quindi \textbf{più espressivi} degli \FSA.

Infatti, dato un \FSA \(A = \langle Q, I, \delta, q_0, F \rangle\) è immediato costruire un \PDA \(A^\prime  = \langle Q^\prime , i^\prime , \Gamma^\prime , q_0^\prime , Z_0^\prime , F^\prime  \rangle \) tale che \(L(A) = L(A^\prime )\).

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.7]{image-3.tikz}
  \caption{Linguaggi riconosciuti da \PDA e \FSA}
  \label{fig:PDA-vs-FSA}
  \bigskip
\end{figure}

\textit{Informalmente}, i \PDA, rispetto agli \FSA, hanno la capacità di invertire le stringhe di lunghezza indefinita e contare, proprio grazie alla loro memoria a pila.

\subsection{\PDA ciclici e aciclici}

A differenza degli \FSA, i \PDA potrebbero non fermarsi dopo un numero finito di mosse: sono possibili \textbf{cicli} di \(\epsilon\)-mosse.
Tuttavia, i \PDA ciclici non aggiungono potere espressivo alla classe dei \PDA e questa loro caratteristica potrà essere rimossa.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, on grid, node distance = 4cm, >=Triangle]
    \node [state, initial, initial text = \(a Z_0 \, \vert \, A Z_0\)](q) {\(q\)};

    \path [->, thick]
    (q) edge [loop above] node {\(\epsilon, A \, | \, AA \)} ();

  \end{tikzpicture}

  \caption{\PDA ciclico}
  \label{fig:PDA-ciclo}
  \bigskip
\end{figure}

Formalmente, si consideri un \PDA \(A\).
La notazione
\[ \langle q, x, \alpha \rangle \vdash^\ast_d \langle q^\prime , y, \beta \rangle\]
indica che:

\begin{enumerate}
  \item Il \PDA evolve dalla \textit{configurazione iniziale} \(\langle q, x, \alpha \rangle\) alla configurazione \(\langle q^\prime , y, \beta \rangle\) tramite operatore \(\vdash^\ast\)
  \item Per \(\beta = Z \beta^\prime\), \(\delta(q^\prime , \epsilon, Z) = \bot\) (è quindi \textit{indefinita})
        \begin{itemize}
          \item \(\vdash^\ast_d\) è una sequenza di mosse che porta a una configurazione da cui \textbf{non è possibile procedere} con una \(\epsilon\)-mossa
          \item per evolvere da questa configurazione è necessario leggere un simbolo in ingresso
        \end{itemize}
\end{enumerate}

Ciò detto, un \PDA è \textbf{aciclico} se e solo se
\[ \forall \, x \in I^\ast \ \exists \, (q, y) \, | \, \langle q_0, x, Z_0 \rangle \vdash^\ast_d  \langle q, \epsilon, \gamma \rangle \]
e che quindi:

\begin{enumerate}
  \item Legge sempre l'intera stringa di ingresso e, al termine di essa, si ferma
  \item Non può ripetere un ciclo indefinitamente con \(\epsilon\)-mosse
  \item Si ferma dopo un numero finito di mosse
\end{enumerate}

Ogni \PDA ciclico può (e deve) essere trasformato nel proprio equivalente aciclico.
Un \PDA che presenta cicli, infatti, potrebbe non raggiungere mai la fine della stringa e il suo corrispondente stato di accettazione, rimanendo per sempre in un ciclo di \(\epsilon\)-mosse.

\subsection{Operazioni sui \PDA}

Nelle successive Sezioni (\ref{sec:complemento-PDA}-\ref{sec:differenza-PDA}) ci si occuperà di trattare la chiusura dei linguaggi riconosciuti da \PDA rispetto a varie operazioni.

\subsubsection{Complemento}
\label{sec:complemento-PDA}

La classe dei linguaggi riconosciuti da \PDA \textbf{è chiusa} rispetto alla complementazione.
Il complemento può essere costruito:

\begin{itemize}
  \item Eliminando i cicli
  \item Aggiungendo stati di errore
  \item Scambiando stati finali con non finali
  \item Occupandosi di \(\epsilon\)-mosse che collegano stati finali e non finali:
        \begin{itemize}
          \item Un \PDA potrebbe imporre l'accettazione solo alla fine di una sequenza \textit{(finita)} di \(\epsilon\)-mosse
          \item Se così fosse, scambiare gli stati finali e iniziali non porterebbe a una complementazione corretta
        \end{itemize}
\end{itemize}

\subsubsection{Unione}
\label{sec:chiusura-PDA-unione}
La classe dei linguaggi riconosciuti dai \PDA \textbf{non è chiusa} rispetto all'unione.

\bigskip

\textit{Per esempio}, non esiste alcun \PDA che riconosca \(\left\{ a^n b^n \, | \, n \geq 1 \right\} \cup \left\{ a^n b^{2n} \, | \, n \geq 1 \right\}\).
Tuttavia:

\begin{itemize}
  \item \(A = \left\{ a^n b^n \, | \, n \geq 1 \right\}\) è riconoscibile via \PDA
  \item \(B = \left\{ a^n b^{2n} \, | \, n \geq 1 \right\}\) è riconoscibile via \PDA
\end{itemize}

\textit{Esempio intuitivo della dimostrazione:}

\begin{itemize}
  \item Se si procede riconoscendo una stringa di \(A\) ma trovando una stringa di \(B\), pur sapendo che rimangono \(n\) caratteri \(b\) da leggere si ha perso l'informazione sul valore stesso di \(n\)
  \item Analogamente, riconoscendo una stringa di \(B\) ma trovando una stringa di \(A\), pur sapendo che sono rimasti almeno \(n\) simboli nella pila non sarà possibile verificare se essi sono effettivamente \(n\)
\end{itemize}

\subsubsection{Intersezione}
\label{sec:intersezione-PDA}

La classe dei linguaggi riconosciuti da \PDA \textbf{non è chiusa} rispetto all'intersezione.

Applicando la legge di \textit{De Morgan} si verifica che:
\[ \cup B = \overline{\left(\overline{A} \cap \overline{B} \right)} =\left( A^C \cap B^C\right)^C \]
Poiché i linguaggi dei \PDA sono chiusi rispetto al complemento, se fossero chiusi rispetto all'intersezione dovrebbero essere chiusi anche rispetto all'unione, in contrario a quanto affermato nella Sezione~\ref{sec:differenza-PDA}

\subsubsection{Differenza}
\label{sec:differenza-PDA}

La classe dei linguaggi riconosciuti da \PDA \textbf{non è chiusa} rispetto alla differenza.

Applicando le leggi insiemistiche, si verifica che:
\[ A \cap B = A - \overline{B} = A - B^C \]
Poiché i linguaggi dei \PDA sono chiusi rispetto al complemento, se fossero chiusi rispetto alla differenza dovrebbero essere chiusi anche rispetto all'intersezione, in contrario a quanto affermato nella Sezione~\ref{sec:intersezione-PDA}

\subsection{Trasduttori a pila - \PDT}

\subsubsection{Definizione formale \PDT}

Un \textbf{trasduttore a pila} (\textit{pushdown transducer} o \PDT) è una tupla \(\langle Q, I, \Gamma. \delta, q_0, Z_0, F, O, \eta \rangle\), dove:

\begin{itemize}
  \item \(Q\) è un \textbf{insieme di stati}, finito
  \item \(I\) è l'\textbf{alfabeto di ingresso}
  \item \(\Gamma\) è l'\textbf{alfabeto di pila}
        \begin{itemize}
          \item \(Z_0 \in \Gamma\) è il simbolo iniziale (il primo in \textit{basso}) di pila
        \end{itemize}
  \item \(\delta\) è la \textbf{funzione di transizione}
  \item \(q_0 \in Q\) è lo \textbf{stato iniziale}
  \item \(F \subseteq Q\) è l'\textbf{insieme di stati finali}
  \item \(O\) è l'\textbf{alfabeto di uscita}
  \item \(\eta\) è la \textbf{funzione di uscita}:
        \begin{itemize}
          \item \(\eta: Q \times \left(I \cup \{ \epsilon \}\right) \times \Gamma \rightarrow O^\ast\)
          \item \textbf{dominio}: prodotto cartesiano dell'insieme di stati, alfabeto di ingresso (compreso \(\epsilon\)) e alfabeto di pila
          \item \textbf{codominio}: stella di Kleene dell'alfabeto di uscita
        \end{itemize}
\end{itemize}

Si noti che:

\begin{itemize}
  \item \(Q, I, \Gamma, \delta, q_0, Z_0, F\) sono definiti in modo analogo ai \PDA nella sezione~\ref{sec:definizione-formale-PDA}.
  \item \(\eta\) è definita solo dove è definita \(\delta\)
  \item La pila può essere necessaria perché richiesta dal linguaggio da riconoscere o perché richiesta dalla traduzione
\end{itemize}

L'illustrazione della transizione tra due stati con \(\delta(q, I, A) = \langle p, \alpha \rangle\) e \(\eta(q, I, A) = w\) è mostrata in Figura~\ref{fig:transizione-stati-PDT}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, on grid, node distance = 4cm, >=Triangle]
    \node [state](q) {\(q\)};
    \node [state](p) [right=of q] {\(p\)};

    \path[thick, ->]
    (q) edge [] node {\(i, A / \alpha, w\)} (p);

  \end{tikzpicture}

  \caption{Transizione tra due stati in un \PDT}
  \label{fig:transizione-stati-PDT}
  \bigskip
\end{figure}

I \PDT sono molto utilizzati per sopperire a due grosse lacune dei \FST: \textit{contare} i caratteri e \textit{invertire} le stringhe.

\subsubsection{Configurazione di un \PDT}

Una configurazione \(c\) di un \PDT è una tupla \(\langle q, x, \gamma, z \rangle\) in cui:

\begin{itemize}
  \item \(q \in Q\) è lo \textbf{stato corrente} del dispositivo di controllo
  \item \(x \in I^\ast\) è la \textbf{porzione non letta} della stringa d'ingresso
  \item \(\gamma \in \Gamma\) è la stringa di \textbf{simboli nella pila}
  \item \(z\) è la stringa già scritta sul \textbf{nastro di uscita}
\end{itemize}

\subsubsection{Transizioni di un \PDT}
% da slide 45 - 1-03PDA.pdf

Per un dato \PDT \(T\), la \textbf{relazione binaria di transizione} \(\vdash_T\) nello spazio di tutte le possibili configurazioni di \(A\) è definita da:
\[ \langle q, x, \gamma, z \rangle \vdash_a c^\prime = \langle q^\prime, x^\prime, \gamma^\prime, z^\prime \rangle, \quad z^\prime = z \overline{z} \]

se e solo se vale una delle due condizioni:

\begin{gather}
  x = ay, \ x^\prime = y, \ \gamma^\prime = A \beta, \ \gamma^\prime = \alpha \beta, \ \delta(q, a, A) = \langle q^\prime, \alpha \rangle, \ \overline{z} = \eta(q, a, A) \label{eq:transizione-PDT-0} \\
  x = x^\prime, \ \gamma = A \beta, \ \gamma^\prime = \alpha\beta, \ \delta(q, \epsilon, A) = \langle q^\prime, \alpha \rangle, \ \overline{z} = \eta(q, \epsilon, A) \label{eq:transizione-PDT-1}
\end{gather}

Ovverosia:

\begin{itemize}
  \item La condizione \ref{eq:transizione-PDT-0} descrive il passaggio da una configurazione all'altra quando viene letto un simbolo in ingresso
  \item La condizione \ref{eq:transizione-PDT-1} prende in considerazione il caso di \(\epsilon\)-mosse \textit{(quando la testina in ingresso rimane ferma)}
\end{itemize}

\subsubsection{Condizione di accettazione di un \PDT}
% da slide 46 - 1-03PDA.pdf

La condizione di accettazione di un \PDT può essere descritta come:
\[ \forall \, x \in I^\ast, \ x \in L \Leftrightarrow c_0 = \langle q_0, x, Z_0, \epsilon \rangle \vdash^\ast c_F = \langle q, \epsilon, \gamma, z \rangle, \ q \in F\]

La traduzione di \(x\) è definita se e solo se la stringa \(x\) è accettata

\clearpage

\section{Macchine di Turing - \TM}

Come visto in precedenza (Sezione~\ref{sec:chiusura-PDA-unione}), l'unione di alcuni linguaggi riconosciuti dai \PDA non può essere riconosciuta dai \PDA perché la classe di questi ultimi non è chiusa rispetto all'unione.

\bigskip

Si consideri il linguaggio \(L = \{a^n b^n c^n \, | \, n > 0\}\):

\begin{itemize}
  \item La pila può essere usata per contare le \(a\)
  \item I simboli sulla pila possono essere usati per controllare che il numero di \(b\) sia uguale al numero di \(a\)
        \begin{itemize}
          \item[\xmark] Il numero di \(c\), di conseguenza, \textbf{non può essere contato}
        \end{itemize}
\end{itemize}

Questa limitazione è dovuta alla \textbf{pila}.
Infatti essa è una memoria \textbf{distruttiva}, perché una volta che il simbolo viene letto, esso è distrutto.
Questa proprietà può essere dimostrata formalmente attraverso una generalizzazione del \textit{pumping lemma} (visto in Sezione~\ref{sec:pumping-lemma}).

Per risolvere questo problema, sono state introdotte le \textbf{Macchine di Turing} (in Inglese \textit{Turing Machine} o \TM), che fanno uso di \textbf{nastri di memoria}.
Essi, infatti, non sono di natura distruttiva e possono venire letti più volte.

\bigskip
Un diagramma semplificato di una \TM è mostrato in Figura~\ref{fig:diagramma-TM}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.8]{image-4.tikz}
  \caption{Diagramma semplificato di una TM}
  \label{fig:diagramma-TM}
  \bigskip
\end{figure}

\textit{Informalmente}, una \TM è costituita da:

\begin{itemize}
  \item \textbf{Stati} e \textbf{alfabeto} definiti come negli \FSA (Sezione~\ref{sec:definizione-formale-FSA})
  \item Un nastro di \textbf{ingresso}, con la testina \(T_I\)
  \item Un nastro di \textbf{uscita}, con la testina \(T_F\)
  \item Un \textbf{dispositivo di controllo}
  \item Un \textbf{alfabeto di memoria}
  \item \(k\) nastri di \textbf{memoria}:
        \begin{itemize}
          \item ogni nastro presenta una \textbf{testina di lettura}, \(T_0, \ldots, T_k\)
          \item rappresentati come \textbf{sequenze infinite}
          \item gli spazi \textbf{vuoti} sono occupati da simboli speciali detti \texttt{blank} (anche \blank, \(-\))
          \item inizialmente, ogni nastro di memoria è \textbf{vuoto}
          \item in ogni momento, i nastri contengono \textbf{un numero finito} di simboli non-\texttt{blank}
        \end{itemize}
\end{itemize}

Un passo di \textbf{computazione} di una \TM è basato su:

\begin{itemize}
  \item Un \textbf{simbolo letto} dal nastro di ingresso
  \item \(K\) simboli, uno per ogni nastro di \textbf{memoria}
  \item \textbf{Stato} del dispositivo di controllo
\end{itemize}

e può eseguire le seguenti operazioni \textbf{(mosse)}:

\begin{enumerate}
  \item \textbf{Cambio di stato} del dispositivo di controllo
  \item \textbf{Sovrascrittura} di un simbolo in nelle celle sottostanti alle testine dei nastri di memoria
  \item \textbf{Spostamento} delle \(k+1\) testine:
        \begin{itemize}[label=\(\rightarrow\)]
          \item i \(k\) nastri di memoria possono spostarsi a \textit{sinistra} o \textit{destra}
          \item il nastro di uscita può spostarsi solo a \textit{destra}
        \end{itemize}
  \item Nessuna operazione, fermandosi \textbf{definitivamente}
\end{enumerate}

La direzione di spostamento di ogni testina deve essere specificata esplicitamente, indicando:

\begin{itemize}
  \item[\(\rightarrow\)] \(R\) per \textit{destra di una posizione}
  \item[\(\leftarrow\)] \(L\) per \textit{sinistra di una posizione}
  \item[\(\downarrow\)] \(S\) per \textit{nessuno spostamento}
\end{itemize}

\subsection{Definizione formale \TM}
\label{sec:definizione-formale-TM}

Formalmente, una \TM con \(k\) nastri è una tupla di \(9\) elementi \(\langle Q, I, \Gamma, O, \delta, \eta, q_0, Z_0, F \rangle\) dove:

\begin{itemize}
  \item \(Q\) è un insieme di \textbf{stati}, finito
  \item \(I\) è l'\textbf{alfabeto} di \textbf{ingresso}
  \item \(O\) è l'\textbf{alfabeto} di \textbf{uscita}
  \item \(\Gamma\) è l'\textbf{alfabeto} di \textbf{memoria}
  \item \(\delta\) è la \textbf{funzione} di \textbf{transizione}:
        \begin{itemize}
          \item  \(\delta: \left(Q-F\right) \times I \times \Gamma^k \rightarrow Q \times \Gamma^k \times \left\{R, L, S \right\}^{k+1}\)
          \item \textbf{dominio}: prodotto cartesiano di tutti gli stati meno quelli finali, alfabeto di ingresso e alfabeto di memoria \textit{elevato al numero di nastri}
          \item \textbf{codominio}: prodotto cartesiano di tutti gli stati, dell'alfabeto di memoria elevato al numero di nastri e dei possibili movimenti dei nastri \textit{elevati a \(k+1\)}
                \begin{itemize}[label=\(\rightarrow\)]
                  \item \(k+1\) perché si riferisce ai \(k\) nastri di memoria e il nastro di uscita
                \end{itemize}
          \item può essere \textbf{parziale}, come avviene nei \FSA (Sezione~\ref{sec:definizione-formale-FSA})
          \item è definita in modo tale che non ci siano transizioni uscenti da uno stato finale
        \end{itemize}
  \item \(\eta\) è la \textbf{funzione} di \textbf{uscita}:
        \begin{itemize}
          \item \(\eta : (Q - F) \times I \times \Gamma^k \rightarrow O \times \{R, S\}\)
          \item \textbf{dominio}: coincide con quello della funzione \(\delta\)
          \item \textbf{codominio}: prodotto cartesiano dell'alfabeto di uscita con i possibili movimenti del nastro di uscita
          \item può essere \textbf{parziale}
        \end{itemize}
  \item \(q_0 \in Q\) è lo stato \textbf{iniziale}
  \item \(Z_0 \in \Gamma\) è il \textbf{simbolo iniziale} di \textbf{memoria}
  \item \(F \subseteq Q\) è l'\textbf{insieme di stati finali}
\end{itemize}

Se il valore di \(k\) non è indicato, la \TM viene semplicemente chiamata \TM \textit{multinastro (con un numero arbitrario di nastri)}.

La presenza di \(O\) e \(\eta\) non è obbligatoria.
Essi infatti sono definiti se è previsto un \textit{nastro di uscita}, come per esempio nelle \TM \textit{traduttrici} (Sezione~\ref{sec:TM-traduttrice}).

\bigskip
Si consideri una transizione in cui:

\begin{itemize}
  \item \(q_0, q_1\) sono due \textbf{stati}
  \item \(i\) è il \textbf{simbolo} di \textbf{ingresso}
  \item \(A_j\) è il \textbf{simbolo} letto dal \(j\)-esimo nastro di memoria
  \item \(A_j^\prime\) è il \textbf{simbolo} che rimpiazza \(A_j\)
  \item \(M_0\) è la \textbf{direzione} della testina del nastro di ingresso \(T_i\)
  \item \(M_j, \  \forall \, 1 \leq j \leq k\) è la \textbf{direzione} della testina del \(j\)-esimo nastro di memoria
\end{itemize}

La notazione grafica di questa situazione è mostrata in Figura~\ref{fig:esempio-transizione-TM}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, on grid, node distance = 10cm, >=Triangle]
    \node [state](q0) {\(q_0\)};
    \node [state](q1) [right=of q0] {\(q_1\)};

    \path[->, thick]
    (q0) edge [] node {\( i, \langle A_1, A_2, \ldots, A_k \rangle \, | \, \langle  A_1^\prime , A_2^\prime , \ldots, A_k^\prime  \rangle, \langle  M_0 M_1 \ldots, M_k \rangle\)} (q1);
  \end{tikzpicture}

  \caption{Transizione di una TM}
  \label{fig:esempio-transizione-TM}
  \bigskip
\end{figure}

\subsection{Configurazione di una \TM}

Analogamente a quanto visto \PDA (Sezione~\ref{sec:configurazione-PDA}), anche le \TM ammettono la definizione di \textbf{configurazione}.
Essa dovrà includere:

\begin{itemize}
  \item Lo \textbf{stato} del dispositivo di controllo
  \item La \textbf{stringa} sul nastro di ingresso e la \textbf{posizione della testina}
  \item la \textbf{stringa} e la \textbf{posizione della testina} per ogni nastro di memoria
\end{itemize}

\bigskip
\textit{Formalmente:} una configurazione \(c\) di una \TM con \(k\) nastri di memoria è la tupla di \(k+3\) elementi:
\[ c = \langle x \uparrow iy, \alpha_1 \uparrow A_1 \beta_1, \ldots, \alpha_k \uparrow A_k \beta_k, u \uparrow o \rangle \]

\begin{itemize}
  \item \(q \in Q\) è lo \textbf{stato corrente} del dispositivo di controllo
  \item \(x, y \in I^\ast, i \in I\) rappresentano il \textbf{contenuto} del nastro di ingresso
  \item \(\alpha_r, \beta_r \in \Gamma^\ast, \ A_r \in \Gamma \  \forall \, r \, 1 \leq r \leq k\) rappresentano il contenuto dei \textbf{nastri di memoria}
        \begin{itemize}
          \item le testine di ogni nastro sono posizionate sulla cella che memorizza il primo simbolo della stringa che segue il simbolo \(\uparrow\)
          \item \(\uparrow \, \notin \left\{ I \cup \Gamma \cup O \right\}\)
        \end{itemize}
  \item \(uo\) è il contenuto del \textbf{nastro di uscita} (se definito, come nelle \TM traduttrici viste in Sezione~\ref{sec:TM-traduttrice})
        \begin{itemize}
          \item \(u \in O^\ast\) è la stringa già scritta sul nastro di uscita
          \item \(o \in O\) è l'ultimo carattere scritto
        \end{itemize}
\end{itemize}

\subsubsection{Configurazione iniziale}

La \textbf{configurazione iniziale} \(c_0\) di \(M\) è del tipo: \(c_0 = \langle q_0, \uparrow iy, \uparrow Z_0, \ldots, \uparrow Z_0, \uparrow Z_0, \uparrow \blank \rangle\)
dove:

\begin{itemize}
  \item \(q = q_0\), lo stato attuale è quello iniziale
  \item \(x = \epsilon\), nessun carattere della stringa è ancora stata letta
  \item \(A_r = Z_0, \alpha_r = \beta_r = \epsilon \ \forall r\), tutti i nastri di memoria sono vuoti
  \item \(u = \epsilon, o = \blank\) il nastro di uscita è vuoto \textit{(se definito)}
\end{itemize}

Inoltre, tutte le testine sono all'inizio del corrispondente nastro.

\subsection{Transizioni di una \TM}

La relazione di \textbf{transizione} \(\vdash_M\) (detta anche \textit{mossa} o \textit{passo computazionale}) fra due configurazioni \(c\) e \(c^\prime\) di una \TM multinastro \(M\) è definita nel modo seguente:

Siano \(c, c^\prime\) le due configurazioni tra le quali si esegue la transizione:
\begin{align*}
   & c = \langle q, x \uparrow iy, \alpha_1 \uparrow A \beta_1, \ldots, \alpha_k \uparrow A \beta_k, u \uparrow o \rangle                                                                                                 \\
   & c = \langle q^\prime , x^\prime  \uparrow i^\prime y^\prime , \alpha_1^\prime  \uparrow A^\prime  \beta_1^\prime , \ldots, \alpha_k^\prime  \uparrow A^\prime  \beta_k^\prime , u^\prime  \uparrow o^\prime  \rangle
\end{align*}

\smallskip

Sia \(\delta\) la funzione di transizione, definita come:
\[ \delta(q, i, A_1, \ldots, A_k) = \langle p, C_1, \ldots, C_k, N, N_1, \ldots N_k \rangle \]
con:
\begin{gather*}
  p \in Q , \  N \in \{R, L, S\} , \  C_r \in \Gamma , \  N_r \in \{R, L, S\} \ \forall \, 1 \leq r \leq k \\
  x = \overline{xi} , \  y = \overline{jy} , \  \alpha_r = \overline{\alpha_r A_r} , \  \beta_r = \overline{B_r \beta_r}
\end{gather*}

\smallskip
Sia \(\eta\) la funzione di uscita, definita come:
\[ \eta (q, i, A_1, \ldots, A_k) = \langle v, M \rangle\]
con:

\[ v \in O \quad M \in \{R, S\}\]

\bigskip
Allora la transizione \(c \vdash_M c^\prime\) (da \(c\) a \(c^\prime\)) se e solo se:
\begin{gather}
  p = q^\prime \label{eq:transizione-TM-0}
\end{gather}

\bigskip
Se vale \textbf{una delle condizioni mutuamente esclusive}:
\begin{gather}
  x = x^\prime, \ i = i^\prime, \ y = y^\prime, \ N = S \label{eq:transizione-TM-1}                        \\
  x^\prime = xi, \ i^\prime = \overline{j}, \ y^\prime = \overline{y}, \ N = R \label{eq:transizione-TM-2} \\
  x^\prime = \overline{x}, \ i^\prime = i, \ y^\prime = iy, \ N = L \label{eq:transizione-TM-3}
\end{gather}

\bigskip
Inoltre, per \(1 \leq r \leq k\) deve valere anche \textbf{uno dei casi mutuamente esclusivi}:
\begin{gather}
  \alpha_r^\prime = \alpha_r, \ A_r^\prime = C_r, \ \beta_r^\prime = \beta_r, \ N_r = S \label{eq:transizione-TM-4}     \\
  \alpha_r^\prime = \alpha_r C_r, \ A_r^\prime = B_r, \ \beta_r^\prime = \overline{\beta_r}, \ N_r = R \label{eq:transizione-TM-5}     \\
  \alpha_r^\prime = \overline{\alpha_r}, \ A_r^\prime = \overline{A_r}, \ \beta_r^\prime = C_r \beta_r, \ N_r = L \label{eq:transizione-TM-6}
\end{gather}

\bigskip
Infine anche \textbf{una delle due condizioni mutualmente esclusive}:
\begin{gather}
  u^\prime = u, \ o^\prime = v, \ M = S \label{eq:transizione-TM-7} \\
  u^\prime = uv, \ o^\prime = \blank, \ M = R \label{eq:transizione-TM-8}
\end{gather}

\subsubsection{Singificato delle condizioni di transizione della \TM}

\begin{itemize}
  \item La condizione~\ref{eq:transizione-TM-0} vincola lo stato di \(c^\prime\) a essere quello di arrivo della transizione
  \item Le condizioni dal \ref{eq:transizione-TM-1} al \ref{eq:transizione-TM-3} definiscono l'evoluzione del nastro di ingresso nel passaggio da \(c\) a \(c^\prime\):
        \begin{itemize}
          \item Se la testina rimane ferma (condizione~\ref{eq:transizione-TM-1}, in cui \(N=S\)), le tre parti in cui essa divide il nastro \textit{(parte sinistra, destra e simbolo corrente)} rimangono invariate tra le due configurazioni
          \item Se la testina si muove a destra (condizione~\ref{eq:transizione-TM-2}, in cui \(N=R\)), la parte a sinistra della testina in \(c^\prime\) conterrà anche il simbolo corrente di \(c\), il simbolo corrente di \(c^\prime\) sara il primo simbolo della parte destra in \(c\) e la rimanente parte destra di \(c\) sarà la parte destra di \(c'\)
          \item Se la testina si muove a sinistra (condizione~\ref{eq:transizione-TM-3}, in cui \(N=L\)), laa parte a sinistra della testina in \(c^\prime\) conterrà tutti i simboli della parte sinistra in \(c\), tranne l'ultimo che diverrà il simbolo corrente di \(c^\prime\) e la parte destra di \(c^\prime\) sarà composta dal simbolo corrente in \(c\) seguito dalla sua parte destra
        \end{itemize}
  \item Le condizioni dal \ref{eq:transizione-TM-4} alla \ref{eq:transizione-TM-6} specificano l'evoluzione dei nastri di memoria in analogia con i precedenti casi. In particolare:
        \begin{itemize}
          \item La testina rimane ferma, condizione~\ref{eq:transizione-TM-4}
          \item La testina si muove a destra, condizione~\ref{eq:transizione-TM-5}
          \item La testina si muove a sinistra, condizione~\ref{eq:transizione-TM-6}
        \end{itemize}
  \item Le condizioni \ref{eq:transizione-TM-7} e \ref{eq:transizione-TM-8} specificano l'evoluzione del nastro di uscita
        \begin{itemize}
          \item Non è specificato il comportamento per \(N=L\) perché la testina del nastro di uscita si muove solo a destra
          \item Se la \TM non ha un nastro di uscita, queste condizioni non vanno considerate
        \end{itemize}
\end{itemize}

\subsection{Condizioni di accettazione di una \TM}

Sia \(M\) una \TM multinastro\
Una stringa \(x \in I^\ast\) è \textbf{accettata} da \(M\) se e solo se:
\[ c_0 = \langle q_0, \uparrow x, \uparrow Z_0, \ldots, \uparrow Z_0 \rangle \vdash^\ast_M \langle q, x^\prime \uparrow iy, \alpha_1 \uparrow A_1 \beta_1, \ldots, \alpha_k \uparrow A_k \beta_k \rangle \]
dove:
\begin{itemize}
  \item \(c_F\) è detta \textbf{configurazione finale}
  \item \(\vdash^\ast_M\) è la chiusura riflessiva e transitiva della relazione \(\vdash_M\)
  \item \(q \in F\), quindi lo stato \(q\) fa parte dell'insieme di stati finali
  \item Le testine possono trovarsi in un qualsiasi punto dei rispettivi nastri di memoria
\end{itemize}

\bigskip
Il linguaggio \textit{accettato} da \(M\) è definito come:
\[ L(M) = \{x \, | \, x \in I^\ast \text{ e } x \text{ è accettato da } M\} \]

\textit{Intuitivamente}, il linguaggio riconosciuto da una \TM \(M\) è composto da tutte e sole le stringhe che permettono di andare dallo stato iniziale a uno degli stati finali.
Poiché il nastro di ingresso è in grado di muoversi in entrambe le direzioni, non è richiesto che al termine dell'esecuzione la testina si trovi al termine della stringa di ingresso.

Una volta che \(M\) raggiunge uno stato finale, per definizione della funzione di transizione \(\delta\), non potrà più lasciarla e la computazione termina.

\subsection{Operazioni sulle \TM}

È possibile dimostrare che le \TM \textbf{sono chiuse} rispetto a:
\begin{itemize}
  \item \textbf{Intersezione}
  \item \textbf{Unione}
  \item \textbf{Concatenazione}
  \item \textbf{Stella di Kleene}
\end{itemize}

Perché una \TM può facilmente simulare due altre \TM (siano esse in \textit{serie} o \textit{parallelo}), si dimostra la chiusura rispetto alle prime due operazioni.
Di conseguenza, si dimostra la chiusura rispetto alle altre.

\bigskip
Analogamente, si dimostra che \textbf{non sono chiuse} rispetto a:
\begin{itemize}
  \item \textbf{Complemento}
  \item \textbf{Differenza},  conseguenza della non chiusura rispetto al complemento
\end{itemize}

La (non) chiusura rispetto a queste due operazioni è dovuta alla presenza di \textit{cicli} all'interno delle \TM.

Infatti, se essi non fossero presenti in una \TM sarebbe sufficiente definire l'insieme di stati di arresto e partizionarli in stati di di \textit{accettazione} e \textit{non accettazione}.
I problemi sorgono qualora una computazione non dovesse terminare (come visto in Sezione~\ref{sec:TM-non-deterministiche}).

\subsection{Proprietà delle \TM}

Di seguito sono enunciate alcune proprietà delle \TM:
\begin{enumerate}
  \item \label{enum:proprieta-1-tm} Ogni \TM è equivalente ad un'opportuna \TM dotata solo di \textbf{due stati non finali} e di \textbf{uno finale}
        \begin{itemize}
          \item può essere necessario accrescere l'alfabeto
          \item di conseguenza, sono sufficienti \(3\) stati per implementare qualsiasi algoritmo
        \end{itemize}
  \item \label{enum:proprieta-2-tm} Ogni \TM è equivalente ad un'opportuna \TM avente un alfabeto formato solo da \textbf{due simboli distinti}
        \begin{itemize}
          \item può essere necessario accrescere il numero di stati
          \item di conseguenza, sono sufficienti \(2\) simboli \textit{(più il simbolo \texttt{blank})} per implementare qualsiasi algoritmo
        \end{itemize}
\end{enumerate}

Le due proprietà impongono una scelta: è possibile ridurre gli stati di una \TM agendo sull'alfabeto (\ref{enum:proprieta-1-tm}) o viceversa ridurre la dimensione dell'alfabeto agendo sul numero di stati (\ref{enum:proprieta-2-tm}).
La scelta si riduce quindi ad un problema progettuale.

\subsection{\TM traduttrice}
\label{sec:TM-traduttrice}

Quando si definiscono le \TM con il nastro di uscita, esse diventano dei \textit{trasduttori}, ossia possono essere usate per tradurre stringhe di \(I^\ast\) in stringhe di \(O^\ast\).
In questo caso, le \TM vengono viste come le tuple di \(9\) elementi viste in Sezione~\ref{sec:definizione-formale-TM}.

\subsubsection{Traduzione tramite \TM}

Una \TM multinastro \(M\) definisce una \textbf{traduzione} \(\tau_M: I^\ast \rightarrow O^\ast\) secondo la regola seguente:
\[ \tau_M(x) = y \textit{ se e solo se } \langle q_0, \uparrow x, \uparrow Z_0, \ldots, \uparrow Z_0, \uparrow \blank \rangle \vdash^\ast_M \langle q, x^\prime \uparrow iy, \alpha_1 \uparrow A_1 \beta_1, \ldots, \alpha_k \uparrow A_k \beta_k, y \uparrow o \rangle \]
con \(q \in F\).

In generale, una \TM \(M\) definisce una \textbf{traduzione parziale} \(\tau_M: I^\ast \rightarrow O^\ast\).
Infatti, \(\tau_M\) è indefinita se:
\begin{enumerate}
  \item \(M\) raggiunge una configurazione di arresto il cui stato non appartiene a \(F\)
  \item \(M\) non si ferma mai quando opera su \(x\)
\end{enumerate}

\bigskip
\textit{Intuitivamente}, una stringa \(x\) viene tradotta in una stringa \(y\) da una \TM \(M\) se esiste un cammino che parte da una configurazione \textbf{iniziale} con \(x\) sul nastro di ingresso e termina in una configurazione \textbf{finale} con \(y\) sul nastro di uscita.

\subsection{Confronto di \TM con altre macchine}

Si vuole ora confrontare la classe di macchine computazionali appartenente alle \TM con altri tipi di macchine.

\subsubsection{\TM vs \PDA}

Come già visto, i linguaggi \(a^n b^n c^n\) e \(a^n b^n \cup a^n b^{2n}\) non possono essere riconosciuti da un \PDA (Sezione~\ref{par:conseguenza-negativa-pumping-lemma}) mentre tramite \TM il riconoscimento funziona.
Ogni linguaggio riconoscibile da un \PDA può essere riconosciuto da una \TM: si può sempre costruire una \TM che usa un nastro di memoria come se fosse una pila.

I linguaggi accettati dalle \TM sono detti \textbf{ricorsivamente enumerabili}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.6]{image-5.tikz}
  \caption{Relazione tra linguaggi}
  \label{fig:relazione-linguaggi}
  \bigskip
\end{figure}

\subsubsection{\TM vs macchine di \textit{Von Neumann}}

Le \TM possono simulare una macchina di \textit{Vonn Neumann} (in Inglese \VNM), un modello astratto di computer.
La principale differenza avviene nell'accesso alla memoria: mentre nelle \TM è \textit{sequenziale}, nelle \VNM è \textit{diretto}.

Tuttavia, il metodo di accesso alla memoria non influenza il potere espressivo di una macchina: non cambia la classe di problemi risolvibili con essa, ma può cambiarne la complessità.
Infatti si tramite \TM si possono anche calcolare funzioni ed eseguire algoritmi \textit{sebbene implementare questo tipo di operazioni possa essere estremamente complicato}.

\bigskip
In conclusione, la \TM è un modello più astratto di computer con accesso \textbf{sequenziale} al suo spazio di memoria.

\subsection{Memoria delle \TM}

Esistono \TM con diversi modelli di memoria:

\begin{itemize}
  \item A nastro \textbf{singolo} (Figura~\ref{fig:TM-a-nastro-singolo}):
        \begin{itemize}
          \item Normalmente è \textbf{illimitato} in entrambe le direzioni
          \item Serve da contemporaneamente da \textbf{ingresso, uscita e memoria}
          \item È il modello più simile alla macchina originalmente ideata da \textit{Alan Turing}
        \end{itemize}
  \item A nastro \textbf{bidimensionale} (Figura~\ref{fig:TM-bidimensionale}):
        \begin{itemize}
          \item È presente \textbf{una testina per ogni dimensione}
          \item Può essere generalizzato a un numero \textit{arbitrario} di dimensioni
        \end{itemize}
\end{itemize}

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{minipage}[b]{.45\textwidth}
    \centering
    \tikzfig[0.5]{image-6.tikz}
    \caption{\TM a nastro singolo}
    \label{fig:TM-a-nastro-singolo}
  \end{minipage}
  \begin{minipage}[b]{.45\textwidth}
    \centering
    \tikzfig[0.5]{image-7.tikz}
    \caption{\TM bidimensionale}
    \label{fig:TM-bidimensionale}
  \end{minipage}
  \bigskip
\end{figure}

Entrambi i modelli di \TM sono \textbf{equivalenti}, poiché riconoscono la stessa classe di linguaggi.
A tal proposito, si consulti la Figura~\ref{fig:configurazione-MT-nastro-singolo}, che mostra la configurazione della \TM a nastro singolo \(M^\prime\):

\begin{itemize}
  \item \(c(T_x)^\prime\) rappresenta il contenuto non vuoto del nastro \(T_x\) di \(M\) alla sinistra della testina di \(T_x\)
  \item \(c(T_x)^{\prime\prime}\) rappresenta il contenuto dello stesso nastro alla destra della testina, compreso il carattere sotto di essa
  \item \(\ast\) e \(\$\) sono simboli che non appartengono a \(I \cup \Gamma \cup O\) e vengono usati per marcare i limiti fra il contenuto dei diversi nastri e posizioni della testina (rispettivamente)
  \item \(\overline{q}\) è un'opportuna codifica dello stato \(M\)
\end{itemize}

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.8]{image-8.tikz}
  \caption{Equivalenza tra \TM multinastro e \TM a nastro singolo}
  \label{fig:configurazione-MT-nastro-singolo}
  \bigskip
\end{figure}

\clearpage

\section{Modelli operazionali non deterministici}
\label{sec:non-determinismo}

Tutti i modelli considerati fino ad ora (\FSA, \PDA, \TM, \ldots) sono \textbf{deterministici}: una volta fissato uno \textit{stato iniziale} e un \textit{ingresso}, la loro evoluzione è \textit{univocamente determinata}.
In certe situazioni, però, il modello che si desidera modellare non può essere descritto in modo così deterministico, in quanto l'osservatore non possiede informazioni sufficientemente accurate da consentirgli di prevederne l'esatta evoluzione per ogni configurazione.

Per esempio, si consideri la serie di istruzioni:

\begin{center}
  \begin{lstlisting}[style=pseudocode, numbers=none]
  if x >= y then:
    max := x

  if y >= x then:
    max := y
\end{lstlisting}
\end{center}

Essa definisce il massimo tra due numeri \texttt{x} e \texttt{y}.
Tale notazione rappresenta una specifica non deterministica: non viene illustrato il comportamento per \texttt{x == y} e l'applicazione di entrambe le regole risulta in un output valido.

Normalmente, nei linguaggi di programmazione, si usa una precedenza di tipo lessicale: viene applicata la prima condizione scelta.

Un'esempio analogo può essere fatto sfruttando gli algoritmi di \textit{ricerca binaria}.
Essi infatti compiono una \textit{simulazione} di algoritmi non deterministici:

\begin{center}
  \begin{lstlisting}[style=pseudocode, numbers=none]
while true:
  if elemento-cercato in radice:
    end

  search in left-subtree
  search in right-subtree
\end{lstlisting}
\end{center}

Siccome non è possibile determinare a priori quale dei due cammini \textit{(sinistro o destro)} sia migliore, la scelta della priorità nei cammini è spesso \textbf{arbitraria}.
Alternativamente, non viene fatta una scelta del cammino da intraprendere ma entrambi vengono esplorati in \textbf{parallelo}.

In entrambi i casi, grazie al non determinismo, è possibile implementare l'algoritmo senza la necessità di usare \textit{backtracking}, che sarebbe invece obbligatorio nel caso si cercasse di usare un sistema deterministico.

\bigskip
Il \textbf{non determinismo} (\ND per brevità) è quindi un modello di computazione.
Viene spesso sfruttato nei linguaggi di programmazione per consentire la computazione parallela.

Il \ND viene applicato a vari modelli computazionali, inclusi tutti quelli visti fin'ora.

\subsection{\FSA non deterministici}
\label{sec:FSA-non-deterministici}

Un \FSA non deterministico (\NFA) è una tupla di \(5\) elementi \(\langle Q, I, \delta, q_0, F \rangle\):

\begin{itemize}
  \item \(Q, I, q_0, F\) sono definiti come in un \FSA (Sezione~\ref{sec:definizione-formale-FSA})
  \item \(\delta\) è la \textbf{funzione di transizione}:
        \begin{itemize}
          \item \(\delta: Q \times I \rightarrow \wp(Q)\)
                \begin{itemize}[label=\(\rightarrow\)]
                  \item \(\wp(Q)\) rappresenta l'insieme delle parti di \(Q\)
                  \item gli elementi di \(\wp(Q)\) sono insiemi di stati
                \end{itemize}
          \item \(\delta^\ast\) è definito induttivamente a partire da \(\delta\):
                \begin{enumerate}
                  \item \(\delta^\ast(q, \epsilon) = \left\{q\right\}\)
                  \item \(\displaystyle \delta^\ast(q, y \cdot i) = \bigcup_{q^\prime \in \delta^\ast (q, y)} \delta (q^\prime, i)\)
                        \begin{itemize}[label=\(\rightarrow\)]
                          \item \(i\) è l'ultimo carattere della stringa di ingresso
                        \end{itemize}
                \end{enumerate}
        \end{itemize}
\end{itemize}

\bigskip
Nel caso di \FSA accettori con \ND, si dice che \(x \in I^\ast\) è \textbf{accettata} da un \NFA \(\langle Q, I, \delta, q_0, F \rangle\) se e solo se \(\delta^\ast (q_0, x) \cap F \neq \emptyset\).

\textit{Informalmente}: tra le varie possibili esecuzioni (a parità di ingresso) dell'\NFA, è sufficiente che una di esse vada a buon fine \textit{(la stella di Kleene delle transizioni \(\delta^\ast\) ha almeno uno stato che fa parte dell'insieme di stati finali)}.

Questa definizione prende il nome di \textbf{non determinismo esistenziale}, che si oppone al non determinismo \textbf{universale}: \(\delta^\ast(q_0, x) \subseteq F\).

\bigskip

Gli \NFA non sono più potenti degli \FSA, ma presentano due grossi vantaggi:
\begin{itemize}
  \item Può essere più semplice progettare un \NFA
  \item Il numero di stati di un \NFA può essere esponenzialmente più minore dell'analogo \FSA
\end{itemize}
Per maggiori dettagli, si consulti la Sezione~\ref{sec:DFA-vs-NFA}

\subsubsection{Esempio di un \NFA}

In Figura~\ref{fig:esempio-NFA} è mostrato un esempio di \NFA.
In esso, infatti, \(\delta(q_0, a) = \left\{q_1, q_2\right\}\) e quindi la funzione di transizione \textbf{non è univocamente definita}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, on grid, node distance=3cm, >=Triangle]
    \node [state](q0) {\(q_0\)};
    \node [state](q1) [below left=of q0] {\(q_1\)};
    \node [state](q2) [below right=of q0] {\(q_2\)};

    \path[->, thick]
    (q0) edge [bend right] node [above=0.2cm] {\(a\)} (q1)
    (q0) edge [bend left] node [above=0.2cm] {\(a\)} (q2);

  \end{tikzpicture}

  \caption{Esempio di un \NFA}
  \label{fig:esempio-NFA}
  \bigskip
\end{figure}

\subsubsection{\DFA vs \NFA}
\label{sec:DFA-vs-NFA}

Si osservi il \NFA in Figura~\ref{fig:DFA-e-NFA}.
Partendo da \(q_0\) e leggendo \texttt{ab}, l'automa raggiunge uno stato che appartiene all'insieme \(\left\{q_3, q_4, q_5\right\}\).
Negli \NFA viene ancora chiamato \textbf{stato} l'insieme dei possibile stati in cui esso può trovarsi durante l'esecuzione.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, on grid, node distance=3cm, >=Triangle]
    \node [state, initial, initial where=above, initial text=](q0) {\(q_0\)};
    \node [state](q1) [below left=of q0] {\(q_1\)};
    \node [state](q2) [below right=of q0] {\(q_2\)};
    \node [state](q3) [draw=red, text=red, below left=of q1] {\(q_3\)};
    \node [state](q3) [draw=red, text=red, below left=of q1] {\(q_3\)};
    \node [state](q4) [draw=red, text=red, below left=of q2, below right=of q1] {\(q_4\)};
    \node [state](q5) [draw=red, text=red, below right=of q2] {\(q_5\)};

    \path[->, thick]
    (q0) edge [] node [above=0.2cm] {\(a\)} (q1)
    (q0) edge [] node [above=0.2cm] {\(a\)} (q2)
    (q1) edge [] node [above=0.2cm] {\(b\)} (q3)
    (q1) edge [] node [above=0.2cm] {\(b\)} (q4)
    (q2) edge [] node [above=0.2cm] {\(b\)} (q4)
    (q2) edge [] node [above=0.2cm] {\(b\)} (q5);

  \end{tikzpicture}

  \caption{\DFA e \NFA}
  \label{fig:DFA-e-NFA}
  \bigskip
\end{figure}

\bigskip
\textit{Formalmente}, \NFA e \DFA \textbf{hanno lo stesso potere espressivo.}
Dato un \NFA, \(A\), è possibile costruire un \DFA \(A_D\) che accetti il medesimo linguaggio.

\bigskip
Sia \(A = \langle Q, I, \delta, q_0, F \rangle, \ \delta: Q \times I \rightarrow \wp(Q)\).
Si definisca \(A_D = \langle Q_D, I, \delta_D, q_{0D}, F_D \rangle\) con:

\begin{itemize}
  \item \(Q_D = \wp(Q)\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item l'insieme dei suoi stati è uguale all'insieme delle parti degli stati di \(A\)
        \end{itemize}
  \item \(\displaystyle \delta_D(q_D, i) = \bigcup_{q \in q_D} \delta(q, i) \)
        \begin{itemize}[label=\(\rightarrow\)]
          \item se un insieme di stati è raggiungibile a partire da uno stato del \NFA, allora tale relazione viene preservata nel \DFA sfruttando la costruzione degli stati come insiemi di stati del \NFA
        \end{itemize}
  \item \(F_D = \left\{q_D \, | \, q_D \in Q_D \land F \cap q_D \neq \emptyset \right\}\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item l'insieme dei suoi stati finali è dato dall'insieme di stati finali di \(A\) raggiungibili senza \ND
        \end{itemize}
  \item \(q_{0d} = \left\{q_0\right\}\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item il suo stato iniziale è uguale allo stato iniziale di \(A\)
        \end{itemize}
\end{itemize}

Gli \NFA non sono quindi più potenti dei corrispondenti \DFA ma sono di dimensione ridotta.
Il precedente teorema, infatti, produce un insieme di stati di cardinalità \(2^n\) partendo da \(n\) stati di partenza del \NFA.
Inoltre, la formalizzazione più naturale di un problema è quella descritta mediante un \NFA.

\subsubsection{Conversione da \NFA a \DFA}
\label{sec:conversione-nfa-dfa}

\textit{Operativamente}, per convertire un \FSA in un \DFA bisogna:

\begin{enumerate}
  \item Creare l'insieme vuoto di stati del \DFA \(Q^\prime\)
  \item Creare la tabella di transizione del \DFA \(T^\prime\)
  \item Aggiungere lo stato iniziale del del \NFA a \(Q^\prime\)
  \item Aggiungere le transizioni dello stato iniziale alla tabella di transizione \(T^\prime\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item se lo stato di partenza porta a più stati per un certo input, allora essi vanno trattati come un singolo stato del \DFA
        \end{itemize}
  \item \label{enum:loop-in-NFA-to-DFA} Se un nuovo stato è presente in \(T^\prime\):
        \begin{itemize}[label=\(\rightarrow\)]
          \item Aggiungere il nuovo stato a \(Q^\prime\)
          \item Aggiungere le sue transizioni a \(T^\prime\)
        \end{itemize}
  \item Ripetere il passo~\ref{enum:loop-in-NFA-to-DFA} finché non vengono più aggiunti nuovi stati a \(T^\prime\)
  \item \(T^\prime\) è ora la tabella di transizione completa del \DFA ricercato
\end{enumerate}

\subsection{\PDA non deterministici}
\label{sec:definzione-NPDA}

Un \PDA non deterministico (\NPDA) è una tupla di \(7\) elementi \(\langle Q, I, \Gamma, \delta, q_0, Z_0, F \rangle\):

\begin{itemize}
  \item \(Q, I, \gamma, q_0, Z_0, F\) sono definiti come in un \PDA (Sezione~\ref{sec:definizione-formale-PDA})
  \item \(\delta\) è la \textbf{funzione di transizione}:
        \begin{itemize}
          \item \(\delta: Q \times \left(I \cup \left\{\epsilon\right\}\right) \times \Gamma \rightarrow \wp_F (Q \times \Gamma^\ast)\)
          \item \(\wp_F\) indica l'insieme delle parti \textbf{finito} dell'insieme \(Q\)
                \begin{itemize}
                  \item[\(\rightarrow\)] \(\Gamma^\ast\) è un insieme \textbf{infinito}
                \end{itemize}
        \end{itemize}
\end{itemize}

Inoltre, la relazione \(\vdash\) su \(Q \times I^\ast \times \Gamma^\ast\) è definita da \(\langle q, x, \gamma \rangle \vdash \langle q^\prime, x^\prime, \gamma^\prime \rangle\) se e sono se è valida una delle due condizioni mutualmente esclusive:

\begin{gather}
  x = ay,\quad x^\prime = y,\quad \gamma = a \beta,\quad \gamma^\prime = \alpha\beta,\quad \langle q^\prime, \alpha \rangle \in \delta(q, a, A) \\
  x = x^\prime,\quad \gamma = a \beta,\quad \gamma^\prime = \alpha\beta,\quad \langle q^\prime, \alpha \rangle \in \delta(q, \epsilon, A)
\end{gather}

\bigskip
La stringa \(x \in I^\ast\) è accettata dall'automa se e solo se:
\[ \langle q_0, x, Z_0 \rangle \vdash^\ast \langle q, \epsilon, \gamma \rangle, \quad q \in F, \gamma \in \Gamma^\ast \]
\textit{Informalmente}, una stringa è accettata da un \NPDA se esiste un cammino coerente con \(x\) che va dallo stato iniziale a uno stato finale quando essa viene letta \textbf{interamente}.

\bigskip
I \PDA, tuttavia, sono intrinsecamente non deterministici.
Nella loro definizione era infatti stato aggiunto il vincolo per cui:
\[ \delta(q, \epsilon, A) \neq \bot \Rightarrow \delta(q, I, A)=\bot, \ \forall i \in I \]
\textit{Informalmente}, se una transizione da uno stato è  una \(\epsilon\)-mossa, allora la stessa transizione non può essere definita per nessun altro ingresso.

Rimuovere questo vincolo, infatti, priverebbe i \PDA del loro non determinismo.
Analogamente è possibile avere non determinismo cambiando la funzione di transizione di \PDA, modificando al contempo la transizioni tra configurazioni e la condizione di accettazione.

\subsubsection{Chiusura delle \NPDA}

Come per gli altri formalismi con \ND, è possibile dimostrare che un \NPDA può riconoscere ogni linguaggio riconoscibile tramite \DPDA.
Tuttavia, siccome gli \NPDA sono più potenti dei \DPDA, la classe di linguaggi riconosciuti dai primi è più ampia di quella riconosciuta dai secondi e di conseguenza non è scontato che valgano le stesse proprietà di chiusura.

Gli \NPDA, infatti, sono chiusi rispetto all'unione.
È possibile costruire un \NPDA che è collegato con due \(\epsilon\)-mosse agli stati iniziali di due \DPDA, così mostrando una chiusura rispetto all'unione (come mostrato nella Figura~\ref{fig:intersezione-PDA-tramite-NPDA}).
Tuttavia, non rimanendo chiusi rispetto all'intersezione, non possono \textit{(e non sono)} essere chiusi rispetto al complemento.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{tikzpicture}[auto, node distance=3cm, >=Triangle]
    \node [state, initial above, initial text=, minimum size=1cm](initial) {\(q_F\)};
    \node [state, minimum size=1cm](0)  [below left=of initial] {\(q_0\)};
    \node [state, minimum size=1cm](1)  [below right=of initial] {\(q_1\)};

    \path[->, thick]
    (initial) edge [bend right] node [above=0.2cm] {\(\epsilon\)} (0)
    (initial) edge [bend left] node [above=0.2cm] {\(\epsilon\)} (1);
  \end{tikzpicture}

  \caption{Intersezione di \PDA tramite \NPDA}
  \label{fig:intersezione-PDA-tramite-NPDA}
  \bigskip
\end{figure}

\subsubsection{Complemento e \ND}

Se una macchina è deterministica e la sua computazione termina, il \textbf{complemento} può essere ottenuto:
\begin{enumerate}
  \item \textbf{Completando} la macchina
  \item \textbf{Scambiando gli stati} di accettazione con quelli di non accettazione
\end{enumerate}

Tuttavia, il non determinismo \textit{(analogamente alla non terminazione)} rende questo approccio inapplicabile.
Come nei \DPDA, le computazioni nei \NPDA possono sempre essere fatte terminare.

Tuttavia, si possono avere due computazioni del tipo:
\begin{align*}
  c_0 = \langle q_0, x, Z_0 \rangle & \vdash^\ast c_1 = \langle q_1, \epsilon, \gamma \rangle \\
  c_0 = \langle q_0, x, Z_0 \rangle & \vdash^\ast c_2 = \langle q_2, \epsilon, \gamma \rangle
\end{align*}
Con:
\[ q_1 \in F \quad q_2 \notin F \]

Con questa configurazione, scambiando stati di accettazione e non, \(x\) verrebbe comunque accettato (e di conseguenza il complemento non sarebbe valido).

\subsubsection{Conseguenze della chiusura delle \NPDA}

Grazie a questa proprietà degli \NPDA è possibile riconoscere il linguaggio generato dall'unione di linguaggi \(\left\{a^n b^n \, | \, n \geq 1\right\} \cup \left\{a^n b^{2n} \, | \, n \geq 1\right\}\).

\subsubsection{Linguaggi riconosciuti dalle \NPDA}

I linguaggi riconosciuti da \NPDA prendono il nome di \textbf{linguaggi non contestuali} (o \textbf{context-free}).

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.8]{image-10.tikz}
  \caption{Linguaggi non contestuali}
  \label{fig:linguaggi-non-contestuali}
  \bigskip
\end{figure}

\subsection{\TM non deterministiche}
\label{sec:TM-non-deterministiche}

Un \TM deterministico (\NTM) è un tupla di \(9\) elementi \(\langle Q, I, \Gamma, O, \delta, \eta, q_0, Z_0, F \rangle\):

\begin{itemize}
  \item \(Q, I, \Gamma, O, q_0, Z_0, F\) sono definiti come in una \TM (Sezione~\ref{sec:definizione-formale-TM})
  \item \(\delta\) è la \textbf{funzione di transizione}:
        \begin{itemize}
          \item \(\delta: (Q - F) \times I \times \Gamma^k \rightarrow \wp\left(Q \times \Gamma^k \times \left\{R, L, S\right\}^{k+1} \times \left\{R, S \right\} \right)\)
        \end{itemize}
  \item \(\eta\) è la \textbf{funzione di uscita}:
        \begin{itemize}
          \item \(\eta : (Q - F) \times I \times \Gamma^k \rightarrow \wp\left(O \times \left\{R, S\right\}\right)\)
        \end{itemize}
\end{itemize}

\bigskip
Contrariamente a quanto avvenuto per la definizione delle \NPDA (Sezione~\ref{sec:definzione-NPDA}), non è necessario specificare che nella definizione \(\delta\) l'insieme delle parti sia finito.
Ciò è dovuto al fatto che l'insieme \(Q \times \Gamma^k \times \left\{R, L, S\right\}^{k+1} \times \left\{R, S \right\}\) è finito, poiché costruito dal prodotto cartesiano di insiemi finiti.

\subsubsection{Albero di computazione di una \NTM}

Per come è definita la funzione di transizione di \(M\), se si considera una computazione di \(M\) su una stringa in ingresso, essa è ben descritta da un albero di configurazioni in cui è inserita ogni configurazione raggiungibile dalla configurazione iniziale.
Una parola viene accettata se esiste almeno un cammino che termina in una configurazione finale.
Un esempio di albero di configurazione per una \NTM è mostrato in Figura~\ref{fig:albero-configurazioni-NTM}, dove:

\begin{itemize}
  \item I cerchi indicano configurazioni di accettazione
  \item I rettangoli indicano configurazioni di \texttt{halt} ma non di accettazione
  \item Le linee tratteggiate configurazione che non terminano
\end{itemize}

% TODO sistema questa

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.8]{image-9.tikz}
  \caption{Albero delle computazioni di una \NTM}
  \label{fig:albero-configurazioni-NTM}
  \bigskip
\end{figure}

\subsubsection{Accettazione di una stringa da un \NTM}

Una stringa \(x \in I^\ast\) è accettata da una \NTM se e solo se esiste una computazione che termina in uno stato di accettazione.
Il problema dell'accettazione di una stringa si pu\`o ridurre quindi alla visita di un albero di computazione.

Per cercare la via verso uno stato di accettazione, si riconoscono due tipi di approccio:

\begin{itemize}
  \item Visita in \textit{"profondità"}, detta \textbf{depth first}
        \begin{itemize}
          \item Un percorso viene seguito fino al suo termine
          \item Quando viene raggiunta la fine, un'altro ramo viene selezionato ed esplorato tramite \textit{backtracking}
        \end{itemize}
  \item Visita in \textit{"ampiezza"}, detta \textbf{breadth first}
        \begin{itemize}
          \item Si crea una coda di nodi da visitare
          \item Ogni volta che un nuovo nodo viene trovato, i suoi successori si aggiungono alla fine della coda
        \end{itemize}
\end{itemize}

Si noti tuttavia che se parola non viene accettata, una \TM può entrare in uno ciclo infinito e quindi non terminare mai la sua computazione.
Questo comportamento può essere causato dalla presenza di rami di lunghezza infinita all'interno dell'albero delle configurazioni.
Una ricerca di tipo \textbf{depth first} non può funzionare per questo tipo di problema, perché finirebbe con molta probabilità in un ciclo infinito.

Una macchina deterministica può simulare l'attraversamento di un albero tramite una ricerca \textbf{breadth first}.

\subsubsection{\DTM vs \NTM}

È possibile costruire una \DTM che visita un'albero di computazione costruito da una \NTM livello dopo livello, implementando una ricerca di tipo \textbf{breadth first}.

Quindi, data una \NTM, è possibile costruire:

\begin{itemize}
  \item Una \DTM analoga che determina se la \NTM riconosce una stringa \(x\)
  \item La sua equivalente \DTM
\end{itemize}

Infine, può essere dimostrato che il \ND \textbf{non aggiunge potere espressivo} alle \TM.

\subsubsection{\NPDA vs \NTM}
% slide 31 - 1-06ND.pdf

Nella Figura~\ref{fig:confronto-NPDA-NTM} è possibile osservare varie possibilità di relazione confronto tra \NPDA e \NTM, specialmente nei casi:

\begin{enumerate}[label=\alph*), ref=(\alph*)]
  \item \label{enum:NTM-or-NPDA} \(\NTM \cup\NPDA \neq \emptyset\), quindi \NTM e \NPDA possono riconoscere dei linguaggi in comune
  \item \label{enum:NPDA-in-NTM} \(\NPDA \subseteq \NTM\), quindi le \NTM rappresentano una sotto categoria delle \NPDA
  \item \label{enum:NTM-in-NPDA} \(\NTM \subseteq \NPDA\), quindi le \NPDA rappresentano una sotto categoria delle \NTM
  \item \label{enum:NTM-equiv-NPDA} \(\NTM \equiv \NPDA\), quindi le due categorie coincidono
\end{enumerate}

Tuttavia, si dimostra che i casi:

\begin{itemize}
  \item \ref{enum:NTM-or-NPDA}, \ref{enum:NTM-in-NPDA} sono \textbf{falsi} perché una \NTM può simulare un \NPDA usando il nastro come pila
  \item \ref{enum:NTM-equiv-NPDA} è \textbf{falso} perché la pila \'e una memoria distruttiva, al contrario del nastro
\end{itemize}

Quindi rimane vero il caso \ref{enum:NTM-in-NPDA} e le \NTM hanno potere espressivo superiore.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-13.tikz}
  \caption{Confronto tra \NPDA e \NTM}
  \label{fig:confronto-NPDA-NTM}
  \bigskip
\end{figure}

\clearpage

\section{Grammatiche}
\label{sec:grammatiche}

Fino ad ora, gli automi sono stati usati come modelli astratti nei problemi di riconoscimento dei linguaggi.
Dato un \textit{riconoscitore} \(A\) e il suo linguaggio di ingresso \(I\), il linguaggio da esso definito è:
\[ L(A) = \left\{x \, | \, x \in I^\ast, \, x \text{ è accettata da } A \right\} \]

Tuttavia, esistono altri tipi di formalismi per descrivere un linguaggio: le \textbf{grammatiche formali}.
Essi sono costituiti da un \textbf{insieme di regole} (sono quindi di tipo \textit{generativo}) che costruiscono le frasi di un linguaggio.
Una grammatica formale genera stringhe di un linguaggio attraverso un processo detto di \textit{riscrittura}.
Esso è costituito da un insieme di tecniche per sostituire sottotermini di una formula con altri termini.
Non si applica solo ai linguaggi in senso naturale ma a un'ampia gamma di contesti.

\bigskip
\textit{Esempi} di riscrittura:

\begin{itemize}
  \item \(A \land B\) viene riscritto come \(\lnot (\lnot A \lor \lnot B)\)
  \item \(\lnot A \lor B\) viene riscritto come \(A \rightarrow B\)
\end{itemize}

\bigskip
In generale, un meccanismo di riscrittura è un insieme di \textit{regole linguistiche} che descrivono l'\textit{oggetto principale} (come la frase) come sequenza di \textit{componenti}.
Ogni componente può essere poi \textit{"raffinato"} da oggetti più dettagliati e così via, fino a ottenere una sequenza di \textit{oggetti elementari}.

Una grammatica è quindi composta da:

\begin{itemize}
  \item Oggetto principale, detto \textbf{simbolo iniziale}
  \item Inseme di componenti da sostituire durante il processo di derivazione, detti \textbf{simboli non terminali}
  \item Insieme di elementi di base, detti \textbf{simboli terminali}
  \item Regole di sostituzione, dette \textbf{produzioni}
\end{itemize}

\bigskip

\textit{Noam Chomsky}, uno degli studiosi più importanti delle grammatiche formali, all'interno del libro \inlinequote{On Certain Formal Properties of Grammars, Information and Control} afferma che:

\indentquote{A grammar can be regarded as a device that enumerates the sentences of a language}

\indentquote{A grammar of L can be regarded as a function whose range is exactly L}

\subsection{Definizione formale di grammatica}

Una \textbf{grammatica} (detta anche \textit{grammatica non ristretta}) \(G\) è una tupla di \(4\) elementi \(\langle V_T, V_N, P, S \rangle\):

\begin{itemize}
  \item \(V_T\) è un insieme finito di \textit{simboli terminali}, detto \textbf{alfabeto terminale}
        \begin{itemize}[label=\(\rightarrow\)]
          \item gli elementi di \(V_T\) sono normalmente scritti in \textbf{minuscolo}
        \end{itemize}
  \item \(V_N\) è un insieme finito di \textit{simboli non terminali}, detto \textbf{alfabeto non terminale}
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(V_N\) è costruito in modo che \(V_T \cap V_N = \emptyset\)
          \item l'alfabeto \(V\) è quindi dato da \(V_T \cup V_N = V_T \oplus V_N\)
          \item gli elementi di \(V_N\) sono normalmente scritti in \textbf{maiuscolo}
        \end{itemize}
  \item \(P\) è un insieme finito, detto \textbf{insieme delle produzioni} di \(G\)
        \begin{itemize}
          \item \(P \subseteq V^\ast \cdot V_N^+ \cdot V^\ast \times V^\ast\)
          \item un elemento \(p = \langle \alpha, \beta \rangle\) di \(P\) verrà indicato con \(\alpha \rightarrow \beta\)
          \item la stringa \(\alpha\) è detta \textbf{parte sinistra} di \(p\)
          \item la stringa \(\beta\) è detta \textbf{parte destra} di \(p\)
          \item due regole \(s \rightarrow d_1, s \rightarrow d_2\) con la stessa parte sinistra si possono essere accorpate come \(s \rightarrow d_1 \, | \, d_2\)
        \end{itemize}
  \item \(S\) è un elemento \textit{"particolare"} di \(V_N\), detto \textbf{assioma} o \textbf{simbolo iniziale}
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(S\) non può essere mai parte destra di una derivazione
        \end{itemize}
\end{itemize}

\subsubsection{Relazione di derivazione immediata}

Data una grammatica \(G\), si definisce su \(V^\ast\) la relazione binaria di \textbf{derivazione immediata}, indicata dalla notazione
\[ \alpha \xRightarrow[G]{} \beta \]
è definita se e solo se
\begin{gather*}
  \alpha = \alpha_1 \gamma \alpha_2, \quad \beta = \alpha_1 \delta \alpha_2 \\
  \alpha_1, \alpha_2, \delta \in V^\ast,\quad \gamma \in V_N^+, \quad \gamma \rightarrow \delta \in P
\end{gather*}
Il simbolo \(G\) in \(\xRightarrow[G]{}\) viene normalmente omesso qualora il contesto sia univocamente definito, indicando quindi le derivazioni come \(\xRightarrow{}\).

Come già visto con le operazioni tra linguaggi, (Sezione~\ref{sec:operazioni-linguaggi}), \(\xRightarrow[G]{\ast}, \xRightarrow[G]{+}, \xRightarrow[G]{n}\) indicano rispettivamente la chiusura riflessiva e transitiva, la chiusura transitiva e la potenza \(n\) di \(\xRightarrow[G]{}\).

\subsubsection{Linguaggio di accettazione di una grammatica}

Una grammatica \(G = \langle V_N, V_T, P, S \rangle\) genera un linguaggio \(L(G)\) sull'alfabeto \(V_T\).
Esso è definito come:
\[ L(G) = \left\{x \, | \, S \xRightarrow[G]{\ast} X, x \in V_T^\ast\right\} \]
\textit{Informalmente}, il linguaggio generato da una grammatica è quindi costruito da tutte le \textit{e sole} stringhe di \textbf{soli simboli terminali} che possono essere generate a partire dall'\textbf{assioma} \(S\) applicando un numero qualsiasi di sostituzioni (\textit{passi}).
Notare che gli elementi non terminali di \(G\) non fanno parte della stringa da essa generata.

Le regole non sono univoche: più sostituzioni diverse possono essere applicate alla stessa stringa di partenza.
Il processo di derivazione è quindi intrinsecamente non deterministico.
Inoltre, alcuni cammini di derivazione possono portare a sequenze di terminali e non terminali, generando quindi stringhe non valide.

\subsection{Gerarchia di Chomsky}
\label{sec:gerarchia-di-Chomsky}

La definizione delle grammatiche (come vista in questi appunti) è opera principalmente di \textit{Noam Chomsky}, linguista e filosofo statunitense.
Egli ha infatti introdotto una loro classificazione, nota come \textbf{gerarchia di Chomsky} (il cui diagramma è osservabile in Figura~\ref{fig:gerarchia-Chomsky}).
In Tabella~\ref{tab:gerarchia-Chomsky} sono mostrate alcune caratteristiche di ciascuna grammatica.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.8]{image-11.tikz}
  \caption{Gerarchia di Chomsky}
  \label{fig:gerarchia-Chomsky}
  \bigskip
\end{figure}

\begin{table}[htbp]
  \bigskip
  \centering
  \begin{tabular}{c|c|c|c}
    \textit{tipo} & \textit{grammatica e linguaggio accettato} & \textit{automatismo}              & \textit{forma regole}                            \\ \hline
    \(0\)         & non ristretta - \GG                        & \TM                               & \textit{tutte}                                   \\
    \(1\)         & dipendente dal contesto                    & \textit{linear bounded automaton} & \(\alpha A \beta \rightarrow \alpha\gamma\beta\) \\
    \(2\)         & non contestuale - \CFG                     & \NPDA                             & \(A \rightarrow \gamma\)                         \\
    \(3\)         & regolare - \RG                             & \FSA                              & \(X \rightarrow a\) o \(X \rightarrow aY\)       \\
  \end{tabular}
  \bigskip
  \caption{Gerarchia di Chomsky}
  \label{tab:gerarchia-Chomsky}
\end{table}

% un breakable list
\begin{minipage}{0.95\textwidth}
  La gerarchia è composta dalle grammatiche così come segue:

  \bigskip
  \begin{enumerate}[start=0, label=Tipo \arabic*:]
    \item Include tutte le grammatiche formali
    \item Hanno regole in forma \(\alpha A \beta \rightarrow \alpha\gamma\beta\)
          \begin{itemize}
            \item \(A\) è un non terminale
            \item \(\alpha, \beta, \gamma\) sono stringhe di terminali e non terminali
            \item \(\gamma\) è non vuota
            \item il contesto viene preservato \textit{(solo il non terminale viene sostituito)}
            \item la regola \(S \rightarrow \epsilon\) è consentita se \(S\) non appare a destra in alcuna regola
          \end{itemize}
    \item Hanno regole in forma \(A \rightarrow \gamma\)
          \begin{itemize}
            \item \(A\) è un non terminale
            \item \(\gamma\) è una stringa di terminali e non terminali
          \end{itemize}
    \item Hanno regole in forma \(X \rightarrow a\) e (\(X \rightarrow aY\) o \(X \rightarrow Ya\))
          \begin{itemize}
            \item \(X, Y\) sono non terminali
            \item \(a\) è un terminale
            \item \(a\) può essere seguito (o preceduto) da un terminale \textit{(le due possibilità sono mutualmente esclusive)}
            \item La regola \(S \rightarrow \epsilon\) è consentita se \(S\) non appare a destra in alcuna regola
          \end{itemize}
  \end{enumerate}
  \bigskip
\end{minipage}

Le grammatiche di tipo \(3\) sono le \textbf{meno potenti}, mentre quelle di tipo \(0\) sono le \textbf{più potenti}.

\subsubsection{Grammatiche lineari}

Sia \(G = \langle V_T, V_N, P, S \rangle\) una grammatica.
Si supponga che, per ogni produzione \(\alpha \rightarrow \beta \in P\):

\begin{itemize}
  \item \(|\alpha| = 1\), ossia \(\alpha \in V_N\)
  \item \(\beta\) sia nella forma \(aB\), \(a\) può essere \(\epsilon\)
        \begin{itemize}
          \item \(B \in V_N\), alfabeto dei non terminali
          \item \(a \in V_T\), alfabeto dei terminali
        \end{itemize}
\end{itemize}

Allora \(G\) è una \textbf{grammatica lineare}, oppure \textit{regolare} o di \textit{tipo \(3\)} (si veda la Sezione~\ref{sec:gerarchia-di-Chomsky}).
Le grammatiche di questa categoria hanno al massimo un non terminale nella parte destra di ognuna delle sue derivazioni.

\bigskip
Si riconoscono due tipi particolari di grammatiche lineari:

\begin{itemize}
  \item[\(\leftarrow\)] Lineare \textbf{sinistra} (\(L\)-grammatica)
    \begin{itemize}
      \item Tutte le derivazioni sono nella forma \(\alpha \rightarrow \alpha w\)
      \item \(|\alpha| = 1\) è vuoto o singolo non terminale
      \item \(w\) è una stringa di terminali
    \end{itemize}
  \item[\(\rightarrow\)] Lineare \textbf{destra} (\(R\)-grammatica)
    \begin{itemize}
      \item Tutte le derivazioni sono nella forma \(\alpha \rightarrow w \alpha \)
      \item \(w\) è una stringa di terminali
      \item \(|\alpha| = 1\) è vuoto o singolo non terminale
    \end{itemize}
\end{itemize}

Una \textbf{grammatica} è \textbf{regolare} (in breve \RG) se e solo se è regolare sinistra o regolare destra.
Inoltre, un \textbf{linguaggio} è \textbf{regolare} se e solo se è generato da una grammatica regolare (e quindi esiste almeno una grammatica che lo genera).

\subsubsection{Grammatiche non contestuali}

Sia \(G = \langle V_T, V_N, P, S \rangle\) una grammatica.
Se, per ogni produzione \(\alpha \rightarrow \beta\) si verifica:

\begin{itemize}
  \item \(\alpha \rightarrow \beta \in P\), la produzione è contenuta in \(P\)
  \item \(|\alpha| = 1\),  \(\alpha\) è un non terminale
\end{itemize}

allora \(G\) è una \textbf{grammatica non contestuale} o \CFG (dall'Inglese \textit{context-free grammar}).
Questa denominazione è dovuta al fatto che la riscrittura di \(\alpha\) non dipende dal suo contesto \textit{(la parte della stringa che la circonda)}.

Le \CFG sono anche dette \textit{BNF} (da \textit{Backus-Naur Form}) e vengono impiegate per definire la sintassi di linguaggi di programmazione.
Le \RG sono anche \CFG ma non è vero il contrario:
\[ RG \subseteq CFG \]

\subsubsection{Grammatiche generali}

Le \textbf{grammatiche generali} (per brevità \GG o \textit{non ristrette}) sono tutte le grammatiche che non presentano limitazioni sulle produzioni.
Corrispondono al tipo \(0\) della gerarchia di Chomsky (Sezione~\ref{sec:gerarchia-di-Chomsky}).

Sia le \RG che le \CFG sono \textbf{non ristrette}:
\[ \RG \subseteq \CFG \subseteq \GG \]

\bigskip
In questa categoria cadono le grammatiche \textbf{ricorsivamente enumerabili} e le grammatiche \textbf{ricorsive}.
Per più dettagli sul significato di queste locuzioni, si consultino le Sezioni \ref{sec:insieme-ricorsivamente-enumerabile} e \ref{sec:insiemi-ricorsivi}.

\subsection{\RG e \FSA}

Dato un \FSA \(A\), è possibile costruire una \(R\)-grammatica a esso equivalente, ossia in grado di generare lo stesso linguaggio riconosciuto da \(A\) e viceversa (Sezioni \ref{sec:RG-da-FSA} e \ref{sec:FSA-da-RG}).

La conseguenza diretta è che \RG, \FSA ed espressioni regolari (Sezione~\ref{sec:espressioni-regolari}) sono modelli diversi per descrivere la stessa classe di linguaggi.

\subsubsection{Costruzione di \RG partendo da \FSA}
\label{sec:RG-da-FSA}

Sia \(A = \langle Q, I, \delta, q_0, F \rangle\) un \FSA.
È possibile costruire una \RG \(G = \langle V_N, V_T,  S, P \rangle\) tale che:

\begin{itemize}
  \item \(V_T = I\), l'insieme dei \textbf{terminali} di \(G\) corrisponde all'\textbf{alfabeto di ingresso} di \(A\)
  \item \(V_N = Q\), l'insieme dei \textbf{non terminali} di \(G\) corrisponde agli stati di \(A\)
  \item \(S = q_0\), l'\textbf{assioma} di \(G\) corrisponde allo \textbf{stato iniziale} di \(A\)
  \item Gli elementi di \(P\) \textbf{(l'insieme delle produzioni di \(G\))} hanno la seguente forma:
        \begin{itemize}
          \item \(B \rightarrow bC\) se e solo se \(C \in \delta(B, b)\) - \textit{transizione tra stati}
          \item \(B \rightarrow \epsilon\) se \(B \in F\) - \textit{transizione verso lo stato finale}
          \item \(\delta^\ast(q, x) = q^\prime\) se e solo se \(q \xRightarrow[]{\ast} x q^\prime\)
        \end{itemize}
\end{itemize}

\subsubsection{Costruzione di \FSA partendo da \RG}
\label{sec:FSA-da-RG}

Sia \(G = \langle V_N, V_T, S, P \rangle\) una \RG.
È possibile costruire un \FSA \(A = \langle Q, I, \delta, q_0, F \rangle\) tale che:

\begin{itemize}
  \item \(Q = V_N \cup \{q_F\}\), gli \textbf{stati} di \(A\) sono i \textbf{non terminali} di \(G\)
  \item \(I = V_T\), l'\textbf{alfabeto di ingresso} di \(A\) corrisponde all'\textbf{insieme di terminali} di \(G\)
  \item \(q_0 = S\), lo \textbf{stato iniziale} di \(A\) corrisponde all'\textbf{assioma} di \(G\)
  \item \(F = \left\{q_F\right\}\)
  \item La \textbf{funzione di transizione} \(\delta\) ha la seguente forma:
        \begin{itemize}
          \item \(\delta(A, b) = C\) se e solo se \(A \rightarrow bC\) - derivazione di terminali e non terminali
          \item \(\delta(A, b) = q_F\) se e solo se \(A \rightarrow b\) - derivazioni di terminali
        \end{itemize}
\end{itemize}

\subsection{\CFG e \NPDA}

Analogamente a quanto trattato nella Sezione~\ref{sec:RG-da-FSA}, è possibile costruire un \NPDA partendo da una \CFG \textit{(context-free grammar, grammatica non contestuale)} e viceversa.

\textit{Intuitivamente}, la pila contiene la parte intermedia delle produzioni, fatta di terminale e non terminali.
Rispettando le regole definite della \CFG, è possibile completare le produzioni fino ad ottenere una stringa di non terminali tramite mosse \textbf{non deterministiche} del \NPDA.

Un esempio del funzionamento del \NPDA che emula una \CFG con la regola \(S \Rightarrow aSb \Rightarrow aabb\) è mostrato in Figura~\ref{fig:analogia-cfg-npda}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-17.tikz}
  \caption{Analogia \CFG e \NPDA}
  \label{fig:analogia-cfg-npda}
\end{figure}

\subsection{\GG e \TM}

Analogamente a quanto trattato nella Sezione~\ref{sec:RG-da-FSA}, è possibile costruire una \TM partendo da una \GG \textit{(grammatica generale)} e viceversa.

\subsubsection{Costruzione di \GG partendo da \TM}

Sia \(M\) una \TM a nastro singolo.
È possibile costruire una \GG detta \(G\) tale che \(L(G) = L(M)\) \textit{(il linguaggio generato dalla \GG e accettato dalla \NTM sono equivalenti)}:

\begin{itemize}
  \item Inizialmente, \(G\) genera tutte le stringhe di tipo \(x \$ X\) con
        \begin{itemize}
          \item  \(x \in V_T^\ast\) e \(X\)
          \item \(X\) è la copia di \(x\) composta solo dai simboli \textbf{non terminali}
        \end{itemize}
  \item Successivamente, \(G\) simula le diverse configurazioni di \(M\) sfruttando la stringa a destra del simbolo \(\$\)
        \begin{itemize}
          \item la derivazione \(x \$ X \xRightarrow{\ast} x\) se e solo se \(x\) è accettata da \(M\)
          \item ogni mossa di \(M\) viene \textbf{emulata} con una derivazione immediata di \(G\)
          \item \(G\) ha quindi delle derivazioni della forma \(x \$ X \Rightarrow x \$ q_0 X\) \textit{(configurazione iniziale di \(M\))}
        \end{itemize}
  \item Infine, per simulare le mosse di \(M\) vengono introdotte le seguenti produzioni:
        \begin{enumerate}
          \item Se è definita \(\delta(q, A) = \langle q^\prime, A^\prime, R \rangle\) si definisce in \(G\) la produzione \(qA \rightarrow A^\prime q^\prime\)
          \item Se è definita \(\delta(q, A) = \langle q^\prime, A^\prime, S \rangle\) si definisce in \(G\) la produzione \(qA \rightarrow q^\prime A^\prime\)
          \item Se è definita \(\delta(q, A) = \langle q^\prime, A^\prime, L \rangle\) si definisce in \(G\) la produzione \(BqA \rightarrow q^\prime B A^\prime, \ \forall B\) dell'alfabeto di \(M\)
                \begin{itemize}[label=\(\rightarrow\)]
                  \item gli alfabeti di ingresso, uscita e memoria coincidono
                \end{itemize}
        \end{enumerate}
  \item Per completare la costruzione è infine necessario aggiungere a \(M\) le produzioni che permettono a \(G\) di derivare da \(x \$ \alpha B q A C \beta\) la sola \(x\) solo nei cai in cui \(M\) giunge a una configurazione di accettazione (\(x \$ \alpha B q_F A C \beta\)) cancellando tutto ciò che si trova a destra di \(\$\), esso compreso
\end{itemize}

\subsubsection{Costruzione di \TM partendo da \GG}

Sia \(G = \langle V_N, V_T, S, P \rangle\) una \GG.
È possibile costruire una \NTM \(M\) tale che \(L(M) = L(G)\) \textit{(il linguaggio accettato dalla \NTM e generato dalla \GG sono equivalenti)}:

\begin{itemize}
  \item \(M\) ha un nastro di \textbf{memoria}
  \item La stringa di ingresso \(x\) è sul nastro di \textbf{ingresso}
  \item Il nastro di memoria è inizializzato con l'\textbf{assioma} \(Z_0 S\)
  \item Il nastro di memoria in generale conterrà una \textbf{stringa} \(\alpha \in V^\ast\):
        \begin{itemize}
          \item Viene scandito per cercare la parte \textbf{sinistra} di una produzione \(P\)
          \item Una volta trovata, \(M\) compie una scelta \textbf{non deterministica} e la parte scelta è sostituita dalla parte destra corrispondente
          \item Se ad essa corrispondono \textbf{più parti destre}, ne viene scelta una in modo \textbf{non deterministico}
        \end{itemize}
\end{itemize}

In questo modo vi è in \(G\) una derivazione che porta dalla stringa \(\alpha\) alla stringa \(\beta\) (quindi una derivazione \(\alpha \xRightarrow{\ast} \beta\)) se e solo se esiste una sequenza di mosse tale per cui
\[ c_s = \langle q_s, Z_0 \alpha \rangle \vdash^\ast \langle q_s, Z_0 \beta \rangle \]
per qualche \(q_s\) (come illustrato in Figura~\ref{fig:derivazione-TM-da-GG}).

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.8]{image-12.tikz}
  \caption{Derivazione \(\alpha \xRightarrow{\ast} \beta\) in una \TM costruita da \GG}
  \label{fig:derivazione-TM-da-GG}
  \bigskip
\end{figure}

Se il nastro contiene una stringa \(y \in V_T^\ast\) (composta da soli simboli terminali), essa è confrontata con \(x\):

\begin{itemize}
  \item[\cmark] Se coincidono, allora \(x\) è \textbf{accettata}
  \item[\xmark] Altrimenti, questa particolare sequenza di mosse \textbf{non porta all'accettazione}
\end{itemize}

\paragraph{Note sulla costruzione di \NTM}

\begin{itemize}
  \item  Usare una \NTM facilita la costruzione di una \GG ma non è l'unico metodo ammesso per farlo.
        \begin{itemize}
          \item Poiché il non determinismo può essere emulato da macchine deterministiche, una \TM è sufficiente a tale scopo
        \end{itemize}
  \item Se \(x \notin L(G)\), allora \(M\) può tentare infinite computazioni, nessuna delle quali porta ad accettazione
        \begin{itemize}
          \item alcune di queste potrebbero non terminare mai, quindi senza concludere che \(x \in L(G)\)
          \item analogamente, non si concluderebbe se \(x \notin L(G)\)
        \end{itemize}
  \item La definizione di accettazione richiede che:
        \begin{itemize}
          \item \(M\) raggiunga una configurazione di accettazione se e solo se \(x \in L\)
          \item essa non richiede che \(M\) termini la computazione in uno stato non finale se \(x \notin L\)
          \item il problema del complemento è risolto e si ripresenta l'asimmetria tra risoluzione di un problema in senso positivo o negativo
        \end{itemize}
\end{itemize}

\clearpage

\section{Espressioni regolari - \RE}
\label{sec:espressioni-regolari}

Come visto fino ad ora, i linguaggi possono essere rappresentati tramite diverse \textbf{classi di modelli}, tra cui si riconoscono:

\begin{itemize}
  \item \textit{Insiemi}
  \item \textit{Pattern}
  \item \textit{Espressioni regolari}
  \item \textit{Modelli operazionali}
        \begin{itemize}
          \item Automi \textit{(Sezione~\ref{sec:automi-a-stati-finiti})}
          \item Trasduttori \textit{(Sezione~\ref{sec:trasduttori-a-stat-finiti})}
          \item Reti di Petri
          \item Diagrammi di stato
        \end{itemize}
  \item \textit{Modelli generativi}
        \begin{itemize}
          \item Grammatiche \textit{(Sezione~\ref{sec:grammatiche})}
        \end{itemize}
  \item \textit{Modelli dichiarativi}
        \begin{itemize}
          \item Logica \textit{(Sezione~\ref{sec:logica})}
        \end{itemize}
\end{itemize}

In questo capitolo si tratterà delle \textbf{espressioni regolari} (scritto come \RE o \textit{Regex}).
Esse sono delle espressioni utilizzabili per denotare un linguaggio attraverso la scrittura delle stringhe che lo compongono.

\subsection{Pattern}

Prima di dare una definizione formale di \RE, è necessario introdurre il concetto di \textbf{pattern}.

\bigskip
Un sistema di pattern è una tripla \(\langle A, V, p \rangle\) dove:

\begin{itemize}
  \item \(A\) è un \textbf{alfabeto}
  \item \(V\) è un \textbf{insieme di di variabili}
        \begin{itemize}[label=\(\rightarrow\)]
          \item è definito in modo che \(A \cup V = \emptyset\)
        \end{itemize}
  \item \(p\) è una stringa su \(A \cup V\) detta \textbf{pattern}
\end{itemize}

Il linguaggio generato dal sistema di pattern consiste di tutte le stringhe su \(A\) ottenute da \(p\) sostituendo ogni variabile in \(p\) con una stringa su \(A\).

\bigskip
\textit{Esempio:} \(\langle \underbrace{\left\{0, 1\right\}}_{alfabeto}, \underbrace{\left\{v_1, v_2\right\}}_{variabili}, \underbrace{v_1 v_1 0 v_2}_{pattern} \ \rangle\)
\begin{itemize}
  \item Stringhe che iniziano con \(0 \ (v_1 = \epsilon)\)
  \item Stringhe che iniziano con una stringa su \(A\) ripetuta due volte, seguita da uno \(0\) e da qualunque stringa (inclusa \(\epsilon\))
\end{itemize}

\subsection{Sintassi e semantica delle \RE}

Dato un alfabeto di simboli terminali, mediante le seguenti regole si definiscono le \textbf{espressioni regolari} e i corrispondenti linguaggi denotati.
Sono \RE su un alfabeto \(\Sigma\):

\begin{enumerate}
  \item \(\emptyset\) è una \RE che denota il linguaggio vuoto (\(\emptyset\))
  \item \(\epsilon\) è una \RE che nota il linguaggio \(\{\epsilon\}\)
  \item Ogni simbolo di \(\sigma\) è una \RE che denota il linguaggio \(\{\sigma\}, \ \sigma \in \Sigma\)
  \item Se \(R_1\) e \(R_2\) sono \RE, anche \(R_1 \cup R_2\), \textit{(scritto anche come \(R_1 + R_2\) o \(R_1 \, | \, R_2\))} è una \RE
  \item Se \(R_1\) e \(R_2\) sono \RE, anche \(R_1 \cdot R_2\), \textit{(scritto anche come \(R_1 R_2\))}, è una \RE
  \item Se \(R\) è una \RE, lo è anche \(R^\ast\)
  \item Nient'altro è una \RE
\end{enumerate}

Le \RE seguono la stessa idea dei sistemi di pattern, ma con diverso potere espressivo.
Le espressioni regolari sono diverse dai sistemi di pattern e hanno diverso potere espressivo.

Le \RE \textbf{corrispondono esattamente} ai linguaggi regolari e hanno lo stesso potere espressivo di \RG e \FSA: per ogni \FSA è possibile costruire la \RE equivalente.

\bigskip
\textit{Per dimostrare l'enunciato} è sufficiente osservare che:

\begin{itemize}
  \item Ogni linguaggio denotato da una \RE è regolare. Infatti:
        \begin{enumerate}
          \item i casi base sono linguaggi regolari
          \item i linguaggi regolari sono chiusi rispetto a agli operatori di \textbf{concatenazione}, \textbf{unione} e \textbf{stella di Kleene}
        \end{enumerate}
  \item Data una \RG \(G\), è sempre possibile trovare una \RE \(r\) tale che \(L(G) = L(r)\)
\end{itemize}

\subsubsection{Operatori delle \RE}

Sono definiti i seguenti \textbf{operatori} delle \RE:

\begin{itemize}
  \item La \textbf{concatenazione}, \(\cdot\)
  \item L'\textbf{alternativa} (detta anche \textit{pipe}), \(|\)
  \item La \textbf{stella di Kleene} (già definita nella Sezione~\ref{sec:stella-di-Kleene}), \(\ast\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item di conseguenza, anche il \textbf{più di Kleene}, \(+\)
        \end{itemize}
  \item L'\textbf{opzionalità}, \(?\)
\end{itemize}

\subsubsection{\RE e pattern}

Le espressioni regolari seguono la stessa idea dei sistemi di pattern, ma con diverso potere espressivo.
Infatti, per riconoscere i linguaggi generati dai pattern servirà una \TM, mentre per le \RE sono sufficienti le \FSA.

Di conseguenza, le \RE non definiscono la stessa classe di linguaggi definita dai pattern.
Inoltre le due classi non sono confrontabili, e non sono una sottoclasse dell'altra.

Per la prima volta dall'inizio del corso si osserva un caso di non confrontabilità:

\[ \RE \ \neq \text{ pattern} \]

\subsubsection{\RE \texttt{POSIX}}

Lo standard \texttt{POSIX} è una API standard per i sistemi operativi UNIX/LINUX e definisce anche le \RE.
Esso include:

\begin{itemize}
  \item I \textbf{metacaratteri} \(( \ ) \ [ \ ] \ \textsuperscript{$\wedge$} \ \backslash \ \$ \ \ast \ + \ ? \ \{ \ \}\)
  \item La notazione \([\alpha]\) indica \textbf{un singolo carattere} \(\in \alpha\)
  \item La notazione \([\textsuperscript{$\wedge$} \alpha]\) indica \textbf{un qualunque simbolo} \(\notin \alpha\)
  \item Il simbolo \(\textsuperscript{$\wedge$}\) indica \textbf{\(\epsilon\) all'inizio di una riga} di testo
  \item Il simbolo \(\$\) indica \textbf{\(\epsilon\) alla fine di una riga} di testo
  \item I simboli \(\ast, +, \, | \,, (, )\) rappresentano gli \textbf{operatori} come già definiti \textit{(a esempio in Sezione~\ref{sec:operazioni-linguaggi})}
  \item Il simbolo \(\backslash\) funge da \textbf{escape}
\end{itemize}

In aggiunta agli operatori delle \RE, sono definiti:
\begin{itemize}
  \item \(\alpha ?\) indica che \(\alpha\) è \textbf{opzionale}
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(\alpha\) appare \(0\) o \(1\) volte
        \end{itemize}
  \item \(\alpha \{n\}\) indica la \textbf{potenza} \(\alpha^n\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(\alpha\) appare \(n\) volte
        \end{itemize}
  \item \(\alpha\{n, m\}\) indica \(\alpha^n \cup \alpha^{n+1} \cup \ldots \cup a^{m-1} \cup a^m\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(\alpha\) appare tra le \(m\) e le \(n\) volte
        \end{itemize}
\end{itemize}

\clearpage

\section{Logica nell'informatica}
\label{sec:logica}

In questa sezione verrà analizzata la logica dal punto di vista del suo uso nell'ingegneria informatica.
La logica è un \textit{formalismo descrittivo e universale}, cioè permette di descrivere le proprietà che si vogliono ottenere (o evitare) da un sistema, senza dover per forza formalizzare anche quest'ultimo.

Grazie a questa sua proprietà, la logica può essere applicata in numerosi contesti, come:

\begin{itemize}
  \item Le porte logiche nell'architettura dei calcolatori
  \item La specifica e la verifica di sistemi nell'ingegneria del software
  \item La definizione della semantica dei linguaggi di programmazione
  \item La programmazione logica
  \item I database
\end{itemize}

e molti altri.

All'interno del corso verranno usati la \PL e la \FOL \textit{(introdotte nelle Sezioni \ref{sec:logica-proposizionale} e \ref{sec:logica-primo-ordine})} per:

\begin{itemize}
  \item Definire i linguaggi \textit{(Sezione~\ref{sec:logica-linguaggi})}
  \item Specificare le proprietà di programmi \textit{(Sezione~\ref{sec:precondizioni-postcondizioni})}
  \item Specificare le proprietà dei sistemi \textit{(Sezione~\ref{sec:specifiche})}
\end{itemize}

\subsection{Logica proposizionale - \PL}
\label{sec:logica-proposizionale}

La logica proposizionale è un linguaggio formale dalla sintassi semplice, basata su proposizioni elementari e su connettivi logici di tipo funzionale.
Opera tra proposizioni \textit{(che possono assumere il valore \vero o \falso)} e relazioni tra proposizioni.
Le proposizioni composte sono formate concatenando proposizioni semplici tramite connettivi logici.

Contrariamente a logiche più complicate \textit{(come la \nameref{sec:logica-primo-ordine}, analizzata nella Sezione~\ref{sec:logica-primo-ordine})}, non permette di operare e predicare tra oggetti non logici e quantificatori.

\subsubsection{Sintassi}

Sia \(\mathcal{L}\) un linguaggio della \textbf{logica proposizionale}.

\begin{itemize}
  \item L'\textbf{alfabeto}, di \(\mathcal{L}\) è composto da:
        \begin{itemize}
          \item Un insieme \textbf{numerabile} \textit{(finito o infinito)} di \textbf{proposizioni}: \(A, B, C, \ldots\)
                \begin{itemize}
                  \item le proposizioni sono simboli di relazione \textbf{nullaria}
                  \item I simboli dell'alfabeto sono privi di significato
                \end{itemize}
          \item Un insieme di \textbf{simboli} \textbf{} \(\lnot, \land, \lor, \rightarrow, \leftrightarrow\)
          \item I \textbf{simboli di punteggiatura} \(  (, ) \) - \textit{parentesi tonde}
        \end{itemize}
  \item L'\textbf{insieme di formule} di \(\mathcal{L}\) è il più piccolo insieme tale che:
        \begin{itemize}
          \item Ogni \textit{proposizione} è una \textbf{formula}
          \item Se \(F\) e \(G\) sono formule, allora \(\lnot F\), \(F \land G\), \(F \lor G\), \(F \rightarrow G\),  \(F \leftrightarrow G\) sono \textbf{formule}
        \end{itemize}
  \item Le \textbf{parentesi sono omesse} ovunque possibile usando l'ordine di precedenza:
        \[ \lnot \quad \land \quad \lor \quad \rightarrow \quad \leftrightarrow \]
  \item Se \(A\) è una proposizione, allora \(A\) e \(\lnot A\) sono detti \textbf{letterali}
        \begin{itemize}
          \item \(A\) è detto letterale \textbf{positivo}
          \item \(\lnot A\) è detto letterale \textbf{negativo}
        \end{itemize}
  \item se \(L\) è un letterale, allora \(\overline{L}\) è il \textbf{letterale complementare} definito come \(\lnot A\) \textit{se} \(L = A\) o \(A\) \textit{se} \(L = \lnot A\)
  \item \textit{Informalmente}, una \textbf{sotto formula} è una formula inclusa in un'altra formula
  \item L'insieme \(\tau(F)\) delle sotto formule di \(\mathcal{F}\) è definito come il più piccolo insieme di formule tale che:
        \begin{itemize}
          \item \(F \in \tau(F)\)
          \item se \(\lnot G \in \tau(F)\), allora \(G \in \tau(f)\)
          \item se \(G \land H, G \lor H, G \rightarrow H, G \leftrightarrow H\) appartengono a \(\tau(F)\), allora \(H, G \in \tau(F)\)
        \end{itemize}
\end{itemize}

\subsubsection{Semantica}
\label{sec:semantica-PL}

La \textbf{semantica} è introdotta per assegnare un significato alle formule.

Nella logica proposizionale, ogni formula può corrispondere a un solo valore di verità (\vero o \falso): è quindi una logica \textbf{a due valori}.

Un'\textbf{interpretazione} \(I\) è una funzione totale dell'insieme di proposizioni ai valori di verità.
Ogni interpretazione può essere convenientemente rappresentata come l'insieme delle proposizioni vere.

Con la notazione \(I \vDash F\) si indica che \(I\) \textbf{rende vera} \(F\).

\bigskip

\textit{Definizioni:}

\begin{itemize}
  \item Se \(I \vDash F\), allora \(I\) è un \textbf{modello} di \(F\). Questa nozione può essere estesa agli insiemi di formule
  \item \(F\) è \textbf{valida} (detta anche \textbf{tautologia}) se e solo se per ogni interpretazione \(I\) vale che \(I \vDash F\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item in questo caso si può anche scrivere \(\vDash F\)
        \end{itemize}
  \item \(F\) è \textbf{soddisfacibile} se e solo se esiste un'interpretazione \(I\) tale per cui \(I \vDash F\)
  \item \(F\) è \textbf{falsificabile} se e solo se esiste un'interpretazione \(I\) tale per cui \(I \nvDash F\)
  \item \(F\) è \textbf{insoddisfacibile} se e solo se per ogni interpretazione \(I\) vale  \(I \nvDash F\)
  \item \(F\) è \textbf{contingente} se e solo se è sia \textit{soddisfacibile} sia \textit{falsificabile}
  \item Ogni formula del tipo \(F \land \lnot F\) è detta una \textbf{contraddizione}, indicata con \(\perp\)
  \item La formula \(F \lor \lnot F\) è detta \textbf{principio del terzo escluso}, indicata con \(\top\)
  \item Un insieme di formule \(\mathcal{F}\) \textbf{comporta logicamente} una formula \(G\) (o \(G\) è una \textit{conseguenza logica} di \(\mathcal{F}\)) se ogni modello di \(\mathcal{F}\) è anche un modello di \(G\) e si scrive con \(\mathcal{F} \vDash G\). \textit{Esempio:}
        \begin{itemize}
          \item \(\{A, A \rightarrow B\} \vDash B, \{A, A\rightarrow B\} \vDash B \land C, \{A, A \rightarrow B\} \nvDash C \)
        \end{itemize}
  \item Per determinare sistematicamente se una formula segua da un insieme di formule si possono usare le \textbf{tabelle di verità}
\end{itemize}

\bigskip

Si considerino:

\begin{itemize}
  \item La \textit{proposizione} \(A\)
  \item La \textit{formula} \(F\)
  \item L'\textit{interpretazione} \(I\)
\end{itemize}

Allora:
\[ I \models A  \quad \textit{sse} \quad I(A) = \vero \]
Di conseguenza:
\begin{align*}
  I \vDash \lnot F \quad             & \textbf{sse} \quad I \nvDash F                                                    \\
  I \vDash F \land G \quad           & \textbf{sse} \quad I \vDash F \textbf{ e } I \vDash G                             \\
  I \vDash F \lor G \quad            & \textbf{sse} \quad I \vDash F \textbf{ o } I \vDash G                             \\
  I \vDash F \rightarrow G \quad     & \textbf{sse} \quad I \nvDash F \textbf{ o } I \vDash G                            \\
  I \vDash F \leftrightarrow G \quad & \textbf{sse} \quad I \vDash F \rightarrow G \textbf{ e } I \vDash G \rightarrow F
\end{align*}

Alternativamente, i connettivi possono anche essere esplicitati sotto forma di tabella (Tabella~\ref{tab:tabella-connettivi}).

\begin{table}[htbp]
  \bigskip
  \centering
  \begin{tabular}{c|c|c|c|c|c|c|c}
    \(F\)  & \(G\)  &  & \(\lnot F\) & \(F \land G\) & \(F \lor G\) & \(F \rightarrow G\) & \(F \leftrightarrow G\) \\ \hline
    \vero  & \vero  &  & \falso      & \vero         & \vero        & \vero               & \vero                   \\
    \vero  & \falso &  & \falso      & \falso        & \vero        & \falso              & \falso                  \\
    \falso & \vero  &  & \vero       & \falso        & \vero        & \vero               & \falso                  \\
    \falso & \falso &  & \vero       & \falso        & \falso       & \vero               & \vero                   \\
  \end{tabular}
  \bigskip
  \caption{Tabella di verità}
  \label{tab:tabella-connettivi}
\end{table}

\paragraph{Esempio di interpretazione}

Se:

\begin{itemize}
  \item \(I_1 = \left\{ A, C\right\}\)
  \item \(I_2 = \left\{ C, D\right\}\)
  \item \(F = (A \lor B) \land (C \lor D)\)
\end{itemize}

Allora:

\begin{itemize}
  \item \(I_1 \vDash F\)
  \item \(I_2 \nvDash F\)
\end{itemize}

\subsubsection{Forme normali}
\label{sec:forme-normali-PL}

\begin{itemize}
  \item Due formule \(F, G\) sono \textbf{semanticamente equivalenti} se e solo se vale sia \(F \vDash G\) che \(G \vDash F\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item in questo caso si usa la notazione \(F \equiv G\)
        \end{itemize}
  \item La formula \(G\), sotto formula di \(F\) può essere sostituita da una formula \(H\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item per indicare la formula risultante si usa la notazione \(F [G \, \backslash H]\)
        \end{itemize}
  \item Un insieme di connettivi è detto \textbf{funzionalmente completo} se e solo se qualunque formula proposizionale può essere trasformata in una formula semanticamente equivalente che contiene solo connettivi dell'insieme
        \begin{itemize}[label=\(\rightarrow\)]
          \item L'insieme \(\{\lnot, \land\}\) è funzionalmente complesso
          \item Esistono altri due connettivi booleani che singolarmente sono funzionalmente completi: \texttt{nand} e \texttt{nor}
        \end{itemize}
\end{itemize}

\bigskip

\textit{Equivalenze notevoli:}

\begin{align*}
  (F \land F)                      & \equiv F                                         & \text{idempotenza di } \land       \\
  (F \lor F)                       & \equiv F                                         & \text{idempotenza di } \lor        \\
  (F \land G)                      & \equiv (G\land F)                                & \text{commutatività di } \land     \\
  (F \lor G)                       & \equiv (G \lor F)                                & \text{commutatività di } \lor      \\
  \left(F \land (G \land H)\right) & \equiv \left((F \land (G \land H\right)          & \text{associatività di } \land     \\
  \left(F \lor (G \lor H)\right)   & \equiv \left((F \lor (G \lor H\right)            & \text{associatività di } \lor      \\
  \left((F \land G) \lor F\right)  & \equiv F                                         & \text{assorbimento}                \\
  \left((F \lor G) \land F\right)  & \equiv F                                         & \text{assorbimento}                \\
  (F \land \left(G \lor H\right))  & \equiv \left((F \land G) \lor (F \land H)\right) & \text{distributività}              \\
  (F \lor \left(G \land H\right))  & \equiv \left((F \lor G) \land (F \lor H)\right)  & \text{distributività}              \\
  \left(\lnot (\lnot F)\right)     & \equiv F                                         & \text{doppia negazione}            \\
  \left(\lnot (F \land G)\right)   & \equiv (\lnot F \lor \lnot G)                    & \text{legge di De Morgan}          \\
  \left(\lnot (F \lor G)\right)    & \equiv (\lnot F \land \lnot G)                   & \text{legge di De Morgan}          \\
  (F \leftrightarrow G)            & \equiv (F \rightarrow G) \land (G \rightarrow F) & \text{equivalenza}                 \\
  (F \rightarrow G)                & \equiv (\lnot F \lor G)                          & \text{implicazione materiale}      \\
  (F \rightarrow G)                & \equiv (\lnot G \rightarrow \lnot F)             & \text{implicazione contronominale} \\
\end{align*}

Il \textit{teorema di sostituzione} e le \textit{equivalenze notevoli} possono essere utilizzate per introdurre le cosiddette \textbf{forme normali}:

\begin{itemize}
  \item Una formula è in \textbf{forma normale negativa} se e solo se è composta solo da letterali, congiunzioni (\(\land\)) e disgiunzioni (\(\lor\))
  \item Una formula è in \textbf{forma normale congiuntiva} (\textit{CNF}) se e solo se ha la forma \(C_1 \land C_2 \land \ldots \land C_n\) dove ogni \(C_i\) è la disgiunzione di letterali
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(C_1 \land C_2 \land \ldots \land C_n \equiv C_1 \land C_2 \land \ldots \land C_n \land \top\), quindi si può affermare che \(\top\) è in \textit{CNF} con \(n = 0\)
        \end{itemize}
  \item Una formula è in \textbf{forma normale disgiuntiva} (\textit{DNF}) se e solo se ha la forma \(C_1 \lor C_2 \lor \ldots \lor C_n\) dove ogni \(C_i\) è la congiunzione di letterali
        \begin{itemize}[label=\(\rightarrow\)]
          \item \(D_1 \lor D_2 \lor \land \lor D_n \equiv D_1 \lor D_2 \lor \ldots \lor D_n \lor \perp\), quindi si può affermare che \(\perp\) è in \textit{DNF} con \(n = 0\)
        \end{itemize}
  \item I \(C_i\) sono detti \textbf{clausole}, mentre i \(D_i\) sono detti \textbf{clausole duali}. Normalmente si usa una notazione insiemistica
\end{itemize}

\subsubsection{Sistemi formali - \textit{calculi}}

Un \textit{sistema formale (assiomatico deduttivo)}, oppure \textbf{calculus} in Inglese, consiste in un insieme di assiomi e un insieme di regole di inferenza che producono conseguenze logiche all'interno di una logica.
Questi elementi definiscono una \textit{relazione di derivabilità} (detta anche \textbf{dimostrabilità}) tra un insieme di formule \(\mathcal{F}\) e una formula \(G\).

Se una formula \(G\) può essere ottenuto da \(\mathcal{F}\) applicando solo regole di inferenza e assiomi, si scrive che \(\mathcal{F} \vdash G\).
Idealmente, la relazione di derivabilità dovrebbe essere \textbf{corretta} (cioè se \(\mathcal{F} \vdash G\) allora \(\mathcal{F} \vDash G\)) e \textbf{completa} (cioè se \(\mathcal{F} \vDash G\) allora \(\mathcal{F} \vdash G\)).

Se una formula \(F\) piò essere derivata in una teoria \(\mathcal{F}\) usando gli assiomi e le regole d'inferenza di un sistema, allora diciamo che \(F\) è un \textbf{teorema}.

\subsection{Logica del primo ordine - \FOL}
\label{sec:logica-primo-ordine}

La logica proposizionale (vista in Sezione~\ref{sec:logica-proposizionale}) ha molte applicazioni, ma il suo potere espressivo è ristretto.
Infatti, frasi come \inlinequote{tutti gli esseri umani sono mortali} e \inlinequote{ogni bambino ha dei genitori} possono essere espresse come proposizioni in un modo che non cattura le relazioni sottintese tra esseri umani, mortali, bambini e genitori.

\textit{Gottlob Frege}, logico tedesco, ha sviluppato la \FOL (\textbf{logica del prim'ordine} o \textbf{logica dei predicati}) nel \(1879\), estendendo la logica proposizionale con \textit{funzioni}, \textit{variabili} e \textit{quantificatori}.

\begin{itemize}
  \item Dal punto di vista \textit{epistemologico} (stati della conoscenza) sia \PL che \FOL considerano \textbf{verità} e \textbf{falsità}
        \begin{itemize}
          \item quindi vengono considerati i valori di \vero e \falso assunti dalla logica
        \end{itemize}
  \item Dal punto di vista \textit{ontologico} (ciò che esiste), \PL considera i \textit{fatti}, mentre \FOL considera anche:
        \begin{itemize}
          \item \textbf{oggetti}, quali: \textit{persone, case, numeri, corsi, \ldots}
          \item \textbf{proprietà} e \textbf{relazioni}, quali: \textit{essere rosso, essere felice, essere più grande di, essere parte di, \ldots}
          \item \textbf{funzioni}, quali: \textit{padre di, età di, successore di, \ldots}
        \end{itemize}
\end{itemize}

\subsubsection{Sintassi}

\begin{itemize}
  \item L'\textbf{alfabeto} di un linguaggio della \FOL \(\mathcal{L}\) è composto da:
        \begin{itemize}
          \item Un insieme infinito numerabile di \textbf{variabili}: \(X, Y, Z, \ldots\)
          \item Un insieme di simboli di \textbf{funzione}: \(f, g, \ldots\)
          \item Un insieme di simboli di \textbf{predicati}: \(p, q, r, \ldots\)
          \item I seguenti \textbf{connettivi}: \(\lnot, \land, \lor, \rightarrow, \leftrightarrow\)
          \item I seguenti \textbf{quantificatori}: \(\exists, \forall\)
          \item I \textbf{simboli di punteggiatura}: \((, ), ","\) - \textit{parentesi tonde e virgola}
        \end{itemize}
  \item Ogni simbolo di funzione e relazione ha una \textbf{arietà} fissata che indica il numero di argomenti a esso associati:
        \begin{itemize}
          \item la notazione \(p/n\) indica che il \textit{predicato} (o \textit{funzione}) \(p\) ha \textbf{arietà} \(n\)
          \item un \textit{predicato} (o \textit{funzione}) con arietà zero è detto \textbf{nullario}
          \item un \textit{predicato nullario} \(p()\) è scritto semplicemente come \(p\)
          \item una \textit{funzione nullaria} \(c()\) è semplicemente scritta come \(c\)
        \end{itemize}
  \item Le funzioni nullarie sono dette \textbf{costanti}, i predicati nullari sono detti \textbf{proposizioni}
  \item I \textbf{termini} denotano tutti gli oggetti che \(\mathcal{L}\) può trattare
        \begin{itemize}
          \item sono definiti induttivamente come segue:
                \begin{enumerate}
                  \item Ogni \textit{variabile} è un \textit{termine}
                  \item Se \(f/n\) è un simbolo di funzione e \(t_1, \ldots t_n\) sono termini, allora \(f (t_1, \ldots t_n)\) è un \textit{termine}
                \end{enumerate}
          \item i \textbf{termini generici} sono tipicamente indicati con \(s, t, \ldots\)
        \end{itemize}
  \item L'insieme di \textbf{formule della \FOL} è definito induttivamente come il più piccolo insieme tale che:
        \begin{itemize}
          \item Se \(p/n\) è un simbolo di relazione e \(t_1, \ldots t_n\) sono termini, allora \(p(t_1, \ldots, t_n)\) è una formula detta \textbf{formula atomica} o \textbf{atomo}
          \item se \(F, G\) sono formule e \(X\) è una variabile, allora sono formule anche:
                \[\lnot F \quad F \land G \quad F \lor G \quad F \rightarrow G \quad F \leftrightarrow G \quad \exists \, X F \quad \forall \, X F \]
        \end{itemize}
  \item I \textbf{letterali} sono atomi \textit{(letterali positivi)} o atomi negati \textit{(letterali negativi)}
  \item Le \textbf{parentesi sono omesse ovunque possibile} usando l'ordine di precedenza tra gli operatori:
        \[ \exists \quad \forall \quad \lnot \quad \land \quad \lor \quad \rightarrow \quad \leftrightarrow\]
  \item Per convenienza:
        \begin{align*}
          \exists \, X_1 \left(\ldots \left(\exists \, X_n (F) \right) \ldots \right) \quad & \text{è abbreviato come} \quad \exists \, X_1, \ldots, X_n(F) \\
          \forall \, X_1 \left(\ldots \left(\forall \, X_n (F) \right) \ldots \right) \quad & \text{è abbreviato come} \quad \forall \, X_1, \ldots, X_n(F) \\
        \end{align*}
  \item Se \(QX(F)\) è una formula e \(Q\) è un quantificatore, allora \(F\) si dice \textbf{ambito} di \(Q\) e \(Q\) è \textbf{applicato} a \(F\)
  \item Un'occorrenza di una variabile in una formula è \textbf{legata} se e solo se la sua occorrenza è entro l'ambito di un quantificatore che impiega quella variabile
        \begin{itemize}[label=\(\rightarrow\)]
          \item è legata al quantificatore di ambito più piccolo che la rende legata
          \item in caso contrario, è \textbf{libera}
        \end{itemize}
  \item Una formula è \textbf{chiusa} se e solo se non contiene occorrenze libere di variabili
        \begin{itemize}
          \item la valutazione è omessa quando si considerano formule chiuse
        \end{itemize}
  \item Un'interpretazione \(\mathcal{I}\) è detta \textbf{modello} per \(F\) sse per ogni valutazione \(\Phi\) si ha \(\mathcal{I} \vDash_\Phi F\)
        \begin{itemize}
          \item se \(\mathcal{F}\) è un insieme di formule, un'interpretazione è un \textbf{modello} di \(\mathcal{F}\) sse è un modello \(\forall \, F \in \mathcal{F}\)
        \end{itemize}
\end{itemize}

\paragraph{Osservazioni sulle traduzioni in \FOL}

\begin{itemize}
  \item Il connettivo principale usato con \(\forall\) è \(\rightarrow\)
        \begin{itemize}
          \item \inlinequote{al Polimi sono tutti brillanti} \texttt{(bleah)} si può tradurre con \[ \forall \, X \, (at(X, polimi) \rightarrow smart(X)) \]
          \item notare che la formula \[ \forall \, X \, (at(X, polimi) \land smart(X)) \] significa che \textbf{tutti} sono al Polimi e \textbf{tutti} sono brillanti
        \end{itemize}
  \item Analogamente, il connettivo principale da usare con \(\exists\) è \(\land\)
        \begin{itemize}
          \item \inlinequote{al Polimi qualcuno è brillante} si pu\`o tradurre con \[ \exists \, X \, (at(X, polimi) \land smart(X)) \] % a volte la o accentata mi rompe il documento intero e non va più niente
          \item notare che la formula \[ \forall \, X \, (at(X, polimi) \rightarrow smart(X)) \] significa che c'è qualcuno che o non è al Polimi o è brillante \textit{(o entrambe)}
        \end{itemize}
\end{itemize}

\subsubsection{Semantica}

Come per \PL, anche \FOL ha una semantica a due valori di verità basata sulla nozione di interpretazione.

Una interpretazione \(\mathcal{I}\) di un alfabeto \(\mathcal{A}\) è un \textbf{dominio} non vuoto \(D\) (indicato anche come \(|\mathcal{I}|\)) a una funzione che associa:

\begin{itemize}
  \item Ogni costante \(c \in \mathcal{A}\) a un elemento \(c_{\mathcal{I}} \in D\)
  \item Ogni simbolo di funzione \(f/n \in \mathcal{A}\) a una funzione \(f_{\mathcal{I}}: D^n \rightarrow D\)
  \item Ogni simbolo di predicato \(p/n \in \mathcal{A}\) a una relazione \(p_{\mathcal{I}} \subseteq \underbrace{D \times \ldots \times D}_{n \text{ volte}}\)
\end{itemize}

Tuttavia, prima di assegnare un significato alle formule, va definito il significato di ciascun termine.
Poiché essi possono contenere variabili, occorre una \textbf{valutazione} (o \textbf{stato}) ossia una funzione dalle variabili di \(\mathcal{A}\) a \(|\mathcal{I}|\).

Il \textbf{significato} \(\Phi_{\mathcal{I}}(t)\) di un termine \(t\) nell'interpretazione \(\mathcal{I}\) e valutazione \(\Phi\) è quindi definito induttivamente come:

\begin{enumerate}
  \item \(c_{\mathcal{I}}\) se \(t\) è una costante \(c\)
  \item \(\Phi(X)\) se \(t\) è una variabile \(X\)
  \item \(f_{\mathcal{I}} \left( \Phi_{\mathcal{I}}(t_1), \ldots, \Phi_{\mathcal{I}}(T_n) \right)\) se \(t\) è della forma \(f(t_1, \ldots, t_n)\)
\end{enumerate}

\paragraph{Proprietà delle valutazioni}

Si considerino:

\begin{itemize}
  \item Una \textit{valutazione} \(\Phi\)
  \item Una \textit{variabile} \(X\)
  \item Un'\textit{interpretazione} \(\mathcal{I}\), \(c_{\mathcal{I}} \in |\mathcal{I}|\)
\end{itemize}

Allora \(\Phi[X \mapsto c_{\mathcal{I}}]\) è una valutazione analoga a \(\Phi\) che \textit{mappa} \(X\) in \(c_{\mathcal{I}}\).

Il significato di una formula è un valore di verità che è definito induttivamente come segue:

\begin{enumerate}
  \item Si indica \(I \vDash_\Phi F\) una formula \(F\) vera rispetto a \(\mathcal{I}\) e \(\Phi\)
  \item Si applicano le seguenti uguaglianze:
        \begin{align*}
          \mathcal{I} & \vDash_\Phi p(t_1, \ldots, t_n)   & \textit{sse} \quad & \langle \Phi_{\mathcal{I}}(t_1), \ldots,  \Phi_{\mathcal{I}}(t_n) \rangle \in P_{\mathcal{I}}          \\
          \mathcal{I} & \vDash_\Phi (\lnot F)             & \textit{sse} \quad & \mathcal{I} \nvDash_\Phi F                                                                             \\
          \mathcal{I} & \vDash_\Phi (F \land G)           & \textit{sse} \quad & \mathcal{I} \vDash_\Phi F \textit{ e } \mathcal{I} \vDash_\Phi G                                       \\
          \mathcal{I} & \vDash_\Phi (F \lor G)            & \textit{sse} \quad & \mathcal{I} \vDash_\Phi F \textit{ o } \mathcal{I} \vDash_\Phi G                                       \\
          \mathcal{I} & \vDash_\Phi (F \rightarrow G)     & \textit{sse} \quad & \mathcal{I} \nvDash_\Phi F \textit{ o } \mathcal{I} \vDash_\Phi G                                      \\
          \mathcal{I} & \vDash_\Phi (F \leftrightarrow G) & \textit{sse} \quad & \mathcal{I} \vDash_\Phi (F \rightarrow G)  \textit{ e } \mathcal{I} \vDash_\Phi (G \rightarrow F)    G \\
          \mathcal{I} & \vDash_\Phi (\forall \, X(F))     & \textit{sse} \quad & \mathcal{I} \vDash_{\Phi[X \mapsto c_{\mathcal{I}}]} F \  \forall \, c_{\mathcal{I}} \in |\mathcal{I}| \\
          \mathcal{I} & \vDash_\Phi (\exists \, X(F))     & \textit{sse} \quad & \mathcal{I} \vDash_{\Phi[X \mapsto c_{\mathcal{I}}]} F \  \exists \, c_{\mathcal{I}} \in |\mathcal{I}| \\
        \end{align*}
\end{enumerate}

\paragraph{Da \PL a \FOL}

La relazione di conseguenza logica \(\vDash\) tra insiemi di formule e formule può essere esteso anche a \FOL, così come i concetti di \textbf{validità, soddisfacibilità, falsificabilità, contingenza} e \textbf{insoddisfacibilità} (visti nella Sezione~\ref{sec:semantica-PL}).

Analogamente, anche le equivalenze mostrate per \PL (Sezione~\ref{sec:forme-normali-PL}) possono essere estese a \FOL, con l'aggiunta di:

\begin{align*}
  \forall \, X (F)                       & \equiv \lnot \left(\exists \, (\lnot F)\right) & \textit{ dualità dei quantificatori } 1           \\
  \exists \, X (F)                       & \equiv \lnot \left(\forall \, (\lnot F)\right) & \textit{ dualità dei quantificatori } 2           \\
  \forall \, X(F) \land (\forall \, X) G & \equiv \forall \, X (F \land G)                                                                    \\
  \exists \, X(F) \land (\exists \, X) G & \equiv \exists \, X (F \lor G)                                                                     \\
  (\forall \, X) (\forall \, Y) F        & \equiv (\forall \, Y) (\forall \, X) F                                                             \\
  (\exists \, X) (\exists \, Y) F        & \equiv (\exists \, Y) (\exists \, X) F                                                             \\
  (\forall \, X(F)) \land G              & \equiv \forall \, X (F \land G)                & \textit{ solo se } X \textit{ non è libera in } G \\
  (\forall \, X(F)) \lor G               & \equiv \forall \, X (F \lor G)                 & \textit{ solo se } X \textit{ non è libera in } G \\
  (\exists \, X(F)) \land G              & \equiv \exists \, X (F \land G)                & \textit{ solo se } X \textit{ non è libera in } G \\
  (\exists \, X(F)) \lor G               & \equiv \exists \, X (F \lor G)                 & \textit{ solo se } X \textit{ non è libera in } G \\
\end{align*}

\subsection{Logica e linguaggi}
\label{sec:logica-linguaggi}

La logica può aiutare a descrivere i linguaggi: gli insiemi di stringhe possono essere visti come abbreviazioni di formule \FOL.
Tramite formalismi matematici, infatti, è possibile mostrare i linguaggi in modo descrittivo focalizzandosi sulle proprietà rilevanti delle stringhe anziché sul modo in cui esse sono generate o riconosciute.
Per raggiungere tale obiettivo, bisogna analizzare:

\begin{itemize}
  \item \textbf{Cosa} va descritto
  \item \textbf{Come} le varie parti vanno definite
  \item \textbf{Quali primitive} possono essere assunte
\end{itemize}

\bigskip
Una volta che la lingua viene definita in modo logico, occorrerebbe definire tutti i predicati e le funzioni non elementari \textit{(come \(=, >, +, -, \times, \ast, \cdot, \ldots\))}.
Per evitare di doversi imbarcare in questa operazione, si fa normalmente riferimento a logiche più ristrette dalla sintassi specifica già predisposta.

\subsubsection{Esempi di descrizione della lingua tramite \FOL}

\paragraph{Esempio 1}
\label{par:esempio-fol-1}

L'insieme:
\[ \left\{ a^n b^n \, | \, n \geq 0 \right\} \]
può essere vista come una abbreviazione di:
\[ x \in L \Leftrightarrow \exists \, n \, | \, n \geq 1 \land x = a^n b^n \]

All'interno di questa formula si possono riconoscere:

\begin{itemize}
  \item I \textbf{predicati} \(\in L, \geq, =\)
  \item Le \textbf{funzioni} di \textit{concatenazione, elevamento a potenza}
\end{itemize}

Come già enunciato, sarebbe necessario definire ciascuno di questi elementi sopra elencati.
Per esempio, \(x^n\) può essere descritto \textit{ricorsivamente} come:
\[ \forall \, n \, \forall \, x\,  ((n = 0 \Rightarrow x^n = \epsilon) \land (n > 0 \Rightarrow x^n = x^{n-1} \cdot x)) \]

\paragraph{Esempio 2}
\label{par:esempio-fol-2}

Sia \(L\) il linguaggio definito come:
\[ L = a^\ast b^\ast \]

Quindi \(L\) è il linguaggio delle stringhe su \(\left\{ a, b \right\}\) che iniziano con \(a\).
Più precisamente, fanno parte di \(L\) le stringhe:

\begin{itemize}
  \item vuota (\(\epsilon\))
  \item composta da un carattere \(a\) e un suffisso appartenente a \(L\)
  \item composta da un prefisso appartenente a \(L\) e da un carattere \(b\)
\end{itemize}

Questo linguaggio può quindi essere espresso in \FOL come:
\[ x \in L \Leftrightarrow (x = \epsilon) \lor \left( \, \exists \, y \, | \, x = at \land y \in L \right) \lor \left( \, \exists \, y \, | \, x = yb \land y \in L \right) \]

\paragraph{Esempio 3}
\label{par:esempio-fol-3}

Sia \(L\) il linguaggio delle stringhe su \(\{a, b, c\}\) definito come:
\[ L = a^\ast b^\ast c^\ast \]

Quindi \(L\) è il linguaggio delle stringhe su \(\left\{ a, b, c \right\}\) con tutte le \(a\) all'inizio, poi tutte le \(b\) e poi tutte le \(c\).

Tra i molti modi possibili di rappresentare formalmente \(L\), si consideri il seguente: è possibile definire due linguaggi \textit{"ausiliari"} più semplici \(L_1 = a^\ast b^\ast, L_2 = b^\ast c^\ast\) e affermare che una stringa appartiene a \(L\) se:

\begin{itemize}
  \item è in \(L_1\)
  \item è in \(L_2\)
  \item composta da un carattere \(a\) e un suffisso appartenente a \(L\)
  \item composta da un prefisso appartenente a \(L\) seguito da un carattere \(c\)
\end{itemize}

Questo linguaggio può quindi essere espresso in \FOL come:
\[ x \in L \Leftrightarrow (x \in L_1) \lor (x \in L_2) \lor \, \exists \, y \left((x = ay \land y \in L) \lor (x = yc \land y \in L)\right) \]
in cui \(L_1, L_2\) sono definite come nell'esempio precedente (Sezione~\ref{par:esempio-fol-2}).

\paragraph{Esempio 4}
\label{par:esempio-fol-4}

Sia \(L\) il linguaggio delle stringhe su \(\left\{ a, b \right\}\) in cui il numero di \(a\) sia uguale al numero di \(b\).

Per definire questo linguaggio in \FOL si può introdurre la funzione di \textit{arietà} \(2\) \(\#(a, x)\), che conta il numero di apparizioni del simbolo \(a\) nella stringa \(x\).
Questa funzione è definita formalmente dalla formula:
\[ \left(x = \epsilon \Rightarrow \#(a, x) = 0 \right) \land \, \forall \, y \left((x = ay \Rightarrow \#(a, x) = \#(a, y) + 1\right) \land \left(x =  b y \Rightarrow \#(a, x) = \#(a, y) \right) \]
la cui definizione dipende dall'alfabeto.

\bigskip
Questo linguaggio può quindi essere espresso in \FOL come:
\[ x \in L \Leftrightarrow (x \in \{a, b\}^\ast) \land (\#(a, x) = \#(b, x)) \]

\subsubsection{Osservazioni sulla formulazione logica}

Non esiste una \inlinequote{formula magica} o un sistema di risoluzione unico per ottenere la descrizione in \FOL di un linguaggio, ma può essere utile evidenziare delle linee guida:

\begin{itemize}
  \item Dall'esempio nel Paragrafo~\ref{par:esempio-fol-3} si ricava che, quando si rivolge l'attenzione all'ordine con cui si susseguono le lettere in un linguaggio, si può far ricorso a formule in cui le stringhe siano decomposte nella concatenazione di sottostringhe, appartenenti ad altri linguaggi, eventualmente definiti \textit{ricorsivamente}
  \item Quando è necessario contare le occorrenze di alcune lettere, risulta utile definire una funzione in grado di contare i simboli a cui si è interessati, come nell'esempio nel Paragrafo~\ref{par:esempio-fol-4}
\end{itemize}

\subsection{Logica monadica del primo ordine - \MFO}
\label{sec:logica-monadica-primo-ordine}

Dato un alfabeto di ingresso \(\Sigma\), le formule sulla logica monadica del primo ordine (o \MFO per brevità) sono costruite dai seguenti elementi:

\begin{itemize}
  \item \textbf{Variabili del primo ordine}
        \begin{itemize}
          \item rappresentate da lettere minuscole (\(x, y, \ldots\))
          \item interpretate sull'insieme di numeri positivi \(\mathbb{N}\)
        \end{itemize}
  \item \textbf{Predicati monadici} (unari), uno per ogni simbolo di \(\Sigma\)
        \begin{itemize}
          \item rappresentate come (\(a(\cdot), b(\cdot), \ldots\))
          \item \(a(x)\) è valutata come \vero in una stringa \(w\) se e solo se il carattere di \(w\) in posizione \(x\) è \(a\)
        \end{itemize}
  \item La \textbf{relazione di minore} \(<\) tra numeri naturali
  \item Le normali proposizioni connettive e i quantificatori di primo ordine
\end{itemize}

Più precisamente, sia \(\mathcal{V}\) un insieme finito di variabili del primo ordine, e sia \(\Sigma\) un alfabeto.
Le formule ben formate (\textit{WFF}, dall'Inglese \textit{well formed formula}) della logica \MFO sono definite secondo la seguente sintassi:
\[ \phi \coloneqq a(x) \, | \, x < y \, | \, \lnot \phi \, | \, \phi \lor \phi \, | \, \exists \, x(\phi) \]
con \(a \in \Sigma, \ x, y \in \mathcal{V}\).

\bigskip
Sono verificate le seguenti definizioni di \textbf{connettivi proposizionali}:

\begin{align*}
  \phi_1 \land \phi_2           & \triangleq \lnot (\lnot \phi_1 \lor \lnot \phi_2)                            \\
  \phi_1 \lor \phi_2            & \triangleq \lnot (\lnot \phi_1 \land \lnot \phi_2)                           \\
  \phi_1 \Rightarrow \phi_2     & \triangleq \lnot \phi_1 \lor \phi_2                                          \\
  \phi_1 \Leftrightarrow \phi_2 & \triangleq ( \phi_1 \Rightarrow \phi_2 ) \land ( \phi_2 \Rightarrow \phi_1 ) \\
  \forall \, x (\phi)           & \triangleq \nexists \, x (\lnot \phi)
\end{align*}

I seguenti \textbf{quantificatori del primo ordine}:

\begin{align*}
  x \geq y & \triangleq \lnot (x < y)           \\
  x \leq y & \triangleq \lnot (x > y)           \\
  x = y    & \triangleq x \leq y \land y \leq x \\
  x \neq y & \triangleq \lnot (x = y)           \\
  x > y    & \triangleq y < x
\end{align*}

E le seguenti definizioni di:

\begin{itemize}
  \item \textbf{Costante}: \(x = 0 \triangleq \forall \, y \lnot (y < x)\)
  \item \textbf{Successore} di un numero naturale: \(\succop(x, y) \triangleq x < y \lnot \, \exists \, (x < z \lor z < y)\)
  \item \textbf{Somma} di valori costanti: \(y = x + k \triangleq \exists \, z_0, \ldots, z_k (z_0 = x \land \succop (z_0, z_1) \land \ldots \succop (z_{k-1}, z_k) \land y = z_k) \)
  \item \textbf{Primo}: \(\firstop(x) \triangleq \lnot \exists \, y \, (y < x)\)
        \begin{itemize}
          \item equivalente a \(x = 0\)
        \end{itemize}
  \item \textbf{Ultimo}: \(\firstop(x) \triangleq \lnot \exists \, y \, (y > x)\)
  \item \textbf{Sottrazione} di valori costanti: \(y = x - k \triangleq x = y + k\)
        \begin{itemize}
          \item[\(\rightarrow\)] \(k\) è una costante in \(\mathbb{N}\)
        \end{itemize}
\end{itemize}

\subsubsection{Esempi di \MFO}

\begin{itemize}
  \item Formula che è vera se tute e sole le parole il cui primo simbolo esiste ed è \(a\): \[ \exists \, x \, (x = 0 \land a(x)) \]
  \item Formula che è vera su tutte le parole in cui ogni \(a\) è seguita da una \(b\): \[ \forall \, x \, \left(a(x) \Rightarrow \exists \, y \, (y = \succop(x) \land b(y) ) \right)\]
  \item Parole non vuote il cui ultimo simbolo è \(a\): \[ \exists \, x \, ( \lastop(x) \land a(x) )\]
  \item Parole di almeno \(3\) simboli in cui il terzultimo simbolo è \(a\): \[ \exists \, x \left( a(x) \land \exists \, y \, (y =  x + 2 \land \lastop(y) ) \right) \]
\end{itemize}

\subsubsection{Semantica}

Una formula \MFO è interpretata su una stringa \(w \in \Sigma^+\) rispetto all'assegnazione \(v : \mathcal{V} \rightarrow U\) con \(U = \{0, \ldots, |w| - 1 \} \), che mappa \(\mathcal{V}\) a una posizione nella stringa \(w\).

La relazione di assegnamento (indicata con \(\vDash\)) è definita nel modo seguente:

\begin{align*}
  w, v & \vDash a(x)                & \textit{ sse } w = uav, |u| = v(x)                                                          \\
  w, v & \vDash x < y               & \textit{ sse } v(x)  < v(y)                                                                 \\
  w, v & \vDash \lnot \Phi          & \textit{ sse } w, v \nvDash \Phi                                                            \\
  w, v & \vDash \Phi_1 \land \Phi_2 & \textit{ sse } w, v \vDash \Phi_1 \land w, v \vDash \Phi_2                                  \\
  w, v & \vDash \forall \, x(\Phi)  & \textit{ sse } w, v^\prime \vDash \Phi \, \forall \, v^\prime, v^\prime(y) = v(y), y \neq x
\end{align*}

Data una stringa \(\phi\), il linguaggio \(L(\phi)\) è definito come:
\[ L(\phi) = \left\{ w \in \Sigma^+ \, | \, \exists \, v :  w, v \vDash \phi \right\} \]

\subsubsection{Proprietà di \MFO}
\label{sec:proprieta-mfo}

I linguaggi esprimibili mediante \MFO sono \textbf{chiusi} rispetto a:

\begin{itemize}
  \item \textbf{Unione}
  \item \textbf{Intersezione}
  \item \textbf{Complemento}
\end{itemize}

Mentre non \textbf{non sono chiusi} rispetto alla stella di Kleene.
Infatti, i linguaggi definiti tramite questa logica prendono il nome di \textit{star-free}, cioè definibili unicamente tramite unione, intersezione, complemento e concatenazione di linguaggi finiti.

\bigskip
Ne consegue che:

\begin{itemize}
  \item \MFO è strettamente meno potente degli \FSA
        \begin{itemize}
          \item data una formula \MFO si può sempre costruire un \FSA equivalente
          \item \(L_P\) può invece essere riconosciuto facilmente mediante \FSA
        \end{itemize}
  \item In \MFO non è possibile esprimere il linguaggio \(L_P\) rappresentante tutte e sole le parole di lunghezza pari con \(l = \{ a \}\)
        \begin{itemize}
          \item La formula \MFO \(a(0) \land a(1) \land \lastop(1)\) definisce il linguaggio \(L_{P2}\) fatto dalla sola parola \(\{aa\}\) di lunghezza \(2\)
          \item Poiché \(L_P = L_{P2}^\ast\), a causa della mancata chiusura rispetto alla stella di Kleene, si spiega perché
        \end{itemize}
\end{itemize}

\subsection{Logica monadica del secondo ordine - \MSO}

Per ottenere lo stesso potere espressivo degli \FSA è necessario poter quantificare sui predicati monadici.

Quindi la \textbf{logica monadica del secondo ordine} è costruita sugli stessi elementi della \MFO (visti nella Sezione~\ref{sec:logica-monadica-primo-ordine}) con in aggiunta:

\begin{itemize}
  \item \textbf{Variabili del secondo ordine}
        \begin{itemize}
          \item rappresentate da lettere maiuscole \(X, Y, \ldots\)
          \item interpretate su \textit{insiemi} di numeri naturali
        \end{itemize}
\end{itemize}

Le formule ben formate \textit{(WFF)} della logica \MSO sono definite secondo la seguente sintassi:
\[ \phi \coloneqq a(X) \, | \, X(x) \, | \, x < y \, | \, \lnot \phi \, | \, \phi \lor \phi \, | \, \exists \, x (\phi) \, | \, \exists \, X (\phi) \]
con \(a \in \Sigma, \ x, y \in \mathcal{V}_1, X \in \mathcal{V}_2\).

\bigskip
Le definizioni della logica \MFO sono sempre valide, con l'aggiunta di:

\begin{align*}
  x \in X       & \triangleq X(x)                                          \\
  X \subseteq Y & \triangleq \forall \, x \, (x \in X \Rightarrow x \in Y) \\
  X = Y         & \triangleq (X \subseteq Y) \land (Y \subseteq X)         \\
  X \neq Y      & \triangleq \lnot (X = Y)
\end{align*}
con \(a \in \Sigma, \ x, y \in \mathcal{V}_1, \ X, Y \in \mathcal{V}_2\).

\subsubsection{Semantica}

Una formula \MSO è interpretata su una stringa \(w \in \Sigma^+\) rispetto alle assegnazioni:

\begin{align*}
  v_1 : \mathcal{V}_1 & \rightarrow \{0, \ldots, |w| - 1 \}      \\
  v_2 : \mathcal{V}_2 & \rightarrow \wp(\{0, \ldots, |w| - 1 \})
\end{align*}

dove:
\begin{itemize}
  \item \(v_1\) mappa ogni variabile di prim'ordine di \(\mathcal{V}_1\) a una posizione nella stringa \(w\)
  \item \(v_2\) mappa ogni variabile di second'ordine di \(\mathcal{V}_2\) a un insieme di posizioni della stringa \(w\)
\end{itemize}

Quindi, la relazione di assegnamento per le formule della logica \MSO è definita nel modo seguente:

\begin{align*}
  w, v_1, v_2 & \vDash  a(x)                 & \text{ sse } w = w_1 a w_2 \land |w_1| = v_1(x)                                                                                                     \\
  w, v_1, v_2 & \vDash X(x)                  & \text{ sse } v_1(x) \in v_2(X)                                                                                                                      \\
  w, v_1, v_2 & \vDash x < y                 & \text{ sse } v_1(x) < v_1(y)                                                                                                                        \\
  w, v_1, v_2 & \vDash \lnot \phi            & \text{ sse } w, v_1, v_2 \nvDash \phi                                                                                                               \\
  w, v_1, v_2 & \vDash \phi_1 \lor \phi_2    & \text{ sse } w, v_1, v_2 \vDash \phi_1 \lor     w, v_1, v_2 \vDash \phi_2                                                                           \\
  w, v_1, v_2 & \vDash \, \exists \, x(\phi) & \text{ sse } w, v^\prime_1, v_2 \vDash \phi \text{ per un } v^\prime_1, \ v^\prime_1(y) = v_1(y) \, \forall \, y \in \mathcal{V}_1 \backslash \{x\} \\
  w, v_1, v_2 & \vDash \, \exists \, X(\phi) & \text{ sse } w, v_1, v^\prime_2 \vDash \phi \text{ per un } v^\prime_2, \ v^\prime_2(Y) = v_2(Y) \, \forall \, Y \in \mathcal{V}_2 \backslash \{X\}
\end{align*}

\subsubsection{Espressività della logica \MSO}

Poiché ogni formula \MFO è anche formula di \MSO, e poiché è risultato impossibile descrivere il linguaggio \(L_P\) (come nella Sezione~\ref{sec:proprieta-mfo}), si deduce immediatamente che la logica \MSO è più espressiva della logica \MFO.

Per lo stesso motivo, si può dimostrare che la \MSO ha la stessa espressività degli \FSA, e quindi per ogni \FSA sarà possibile costruire una formula \MSO e viceversa.

\paragraph{Da \FSA a \MSO}

In generale, grazie alle quantificazioni del second'ordine è possibile trovare, per ogni \FSA, una formula \MSO equivalente.
L'idea generale della costruzione consiste nell'usare una variabile di secondo ordine \(X_q\) per ogni stato \(q\) dell'\FSA \(M\).
Il valore di ciascun \(X_q\) corrisponde all'insieme delle posizioni di tutti i caratteri che \(M\) può leggere in una transizione partendo dallo stato \(q\).

Assumendo che l'insieme di stati di \(M\) sia \(Q = \{0, 1, \ldots, k\}\) per un qualsiasi \(k\), con \(0\) che indica lo stato iniziale, la definizione di \(M\) che riconosce il linguaggio \(L\) è data dalla congiunzione di molteplici condizioni, ognuna traducente una parte della definizione di \(M\).

\paragraph{Da \MSO a \FSA}

Data una formula \MSO \(\Phi\), è possibile costruire un \FSA che accetta un linguaggio \(L\) definito da \(\Phi\) tramite il teorema di \textit{Büchi-Elgot-Trakhtenbrot}.

\bigskip
I passaggi che portano alla costruzione non sono oggetto di questo corso e non saranno mostrati.

\subsection{Precondizioni e postcondizioni}
\label{sec:precondizioni-postcondizioni}

Quando si programma una funzione è importante definire precisamente cosa fa, senza necessariamente specificare come lo fa.
Questo è lo scopo di \textbf{precondizioni} e \textbf{postcondizioni}.

\bigskip
A questo scopo viene introdotta la \textbf{notazione di Hoare}:

\begin{center}
  \{Precondizione: \(Pre\)\} \\
  Funzione \(F\) \\
  \{Postcondizione: \(Post\)\}
\end{center}

dove:
\begin{itemize}
  \item La \textbf{precondizione} indica cosa deve valere \textbf{prima} che la funzione \(F\) sia invocata
  \item La \textbf{postcondizione} indica cosa deve valere \textbf{dopo} che la funzione \(F\) ha finito terminato la propria esecuzione
\end{itemize}

Le precondizioni e postcondizioni possono essere definite in modi diversi, come:
\begin{itemize}
  \item Linguaggi naturali
  \item Linguaggi per asserzioni
  \item Linguaggi ad hoc
\end{itemize}

Tra di questi si riconosce su tutte la \FOL, che può essere usata a questo scopo.

\subsubsection{Specifiche}
\label{sec:specifiche}

Una specifica deve essere usata come un \inlinequote{contratto}, che contiene tutte le informazioni necessarie senza assunzioni a priori.
Quando una qualche condizione viene eliminata dalla precondizione, la specifica diventa insoddisfacente.

\bigskip
Una \textbf{specifica formale} è una descrizione matematica di un sistema.
Come per le specifiche, esistono più linguaggi di specifica diversi.
Dopo aver specificato i requisiti di un algoritmo (o di un sistema), è necessario verificare la correttezza del medesimo.
Utilizzando un modello matematico dell'implementazione costruita, è possibile ottenere la prova di correttezza come una dimostrazione di teorema.

\bigskip
Il problema nella specifica dei sistemi è detto \textbf{frame problem}.
Esso rappresenta il problema di esprimere un dominio in logica senza dover descrivere esplicitamente le condizioni che non sono modificate da un'azione.
Per questo motivo situazioni estremamente semplici possono richiedere formalizzazioni complesse.

\subsubsection{Esempi di specifiche}

\paragraph{Algoritmo di ricerca}

Sia \(F\) una funzione che implementa la ricerca di un elemento \(x\) in un array ordinato \(a\) di \(n\) elementi.

Allora, \textit{informalmente}:

\begin{itemize}
  \item \textit{Precondizione}: l'array \(a\) è ordinato
  \item \textit{Postcondizione}: la variabile logica \texttt{found} deve essere vera se e solo se l'elemento \(x\) esiste nell'array \(a\)
\end{itemize}

Formalizzando le precedenti definizioni in \FOL si ottiene:

\begin{itemize}
  \item \textit{Precondizione:} \(\forall \, i \, (1 \leq n \leq n-1 \rightarrow a[1] \leq a[i+1])\)
  \item \textit{Postcondizione:} \(\texttt{found} \leftrightarrow \exists \, i \, (1 \leq i \leq n \land a[i] = x)\)
\end{itemize}

\paragraph{Agoritmo di ordinamento}

Sia \(ORD\) un programma che ordina un'array \(a\) di \(n\) elementi senza ripetizioni.

Allora, \textit{informalmente}:

\begin{itemize}
  \item \textit{Precondizione}: l'array \(a\) non contiene ripetizioni
  \item \textit{Postcondizione}: l'array ottenuto \(o\) è ordinato
\end{itemize}

Tuttavia questa definizione \textbf{non è sufficiente}: bisogna anche indicare che ogni elemento di \(a\) deve essere presente in \(o\).
Si aggiunge un array \(b\) (non utilizzato da \(ORD\)) con gli stessi elementi di \(a\) per poter fare rifermento a quest'ultimo prima che venga modificato.

Formalizzando le precedenti definizioni in \FOL si ottiene:

\begin{itemize}
  \item \textit{Precondizione:} \[\lnot \exists \, i, j \, (1 \leq i \leq n \land 1 \leq j \leq n \land i \neq j \land a[i] = a[j]) \land \forall \, i \, (1 \leq i \leq n \rightarrow a[i] = b[i])\]
  \item \textit{Postcondizione:}
        \begin{align*}
           & \forall \, i \, (i \leq i \leq n \rightarrow a[i] \leq a[i+1]) \ \land                                    \\
           & \forall \, i \, (1 \leq j \leq n \rightarrow \exists \, j \, (1 \leq j \leq n) \land a[i] = b[j]) \ \land \\
           & \forall \, j \, (1 \leq i \leq n \rightarrow \exists \, i \, (1 \leq i \leq n) \land a[i] = b[j])
        \end{align*}
\end{itemize}

\subsubsection{Esempi di specifica formale}

\paragraph{Comportamento di una lampada}

\textit{Se premo il tasto, la luce si accende entro \(\Delta\) unità di tempo.}

È necessario introdurre dei predicati opportuni:

\begin{itemize}
  \item \(P_B(t)\): istante di pressione del tasto
  \item \(L_{ON}(T)\): istante di accensione della luce
\end{itemize}

A questo punto si potrebbe \textbf{erroneamente} definire la equivalente formula \FOL della specifica come:
\[\forall \, t \, (P_B(t) \rightarrow \exists \, t_1 \, ((t \leq t_1 \leq t + \Delta) \land L_{ON}(t_1)))\]

Tuttavia:
\begin{itemize}
  \item \textbf{Non è specificato} che qualcuno debba premere il pulsante affinché la luce si possa accendere.
  \item \textbf{Non è specificato} cosa succede dopo che la luce si è accesa
  \item \textbf{Non è specificato} cosa succede se si preme il pulsante quando la luce è accesa
  \item \textbf{Non è specificato} cosa succede se si preme il pulsante due volte
  \item \textbf{Non è specificato} se la luce può essere accesa senza premere il tasto
\end{itemize}

\bigskip
Per poter dare una specifica corretta, si altera leggermente la definizione di funzionamento, supponendo che dopo essersi accesa la lampada rimanga nel suo stato per \(k\) unità di tempo per poi spegnersi.
Lo schema di funzionamento è riportato nella Figura~\ref{fig:schema-funzionamento-lampada}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-14.tikz}
  \caption{Schema di funzionamento della lampada}
  \label{fig:schema-funzionamento-lampada}
  \bigskip
\end{figure}

La specifica corretta sarà:

\begin{gather*}
  \forall \, t \, ((P_B(t) \land L_{OFF}(t)) \rightarrow \forall \, t_1 \, (t \leq t_1 \leq t + k) \rightarrow L_{ON}(t_1) \land L_{OFF}(t+k)) \\ \land \\
  \forall \, t_3, t_4 \, ((L_{OFF}(t_3) \land \, \forall \, t_5 \, (t_3 \leq t_5 \leq t_4) \rightarrow \lnot P_B(t_5)) \rightarrow L_{OFF}(t_4))
\end{gather*}

\clearpage

\section{Teoria della computabilità}

Dopo aver illustrato un numero significativo di modelli atti a descrivere problemi di elaborazione delle informazioni e la loro soluzione e dopo averne studiato le capacità e i limiti, ci dedicheremo ora a studiare quali problemi possano essere affrontati e risolti mediante macchine da calcolo.
Prima di poter continuare, è importante riassumere alcuni concetti appresi fino ad ora:

\begin{enumerate}
  \item Automi, grammatiche e altri formalismi possono essere considerati dispositivi meccanici per risolvere problemi matematici (e quindi i problemi pratici che essi modellano)
  \item Alcuni formalismi sono più potenti di altri, ossia sono in grado di riconoscere alcuni linguaggi che altri sono incapaci di riconoscere
  \item Nessun formalismo, tra quelli visti fino ad ora, è più potente di una \TM
        \begin{itemize}[label=\(\rightarrow\)]
          \item alcuni hanno tuttavia la stessa medesima potenza delle \TM
          \item questi modelli verranno d'ora in poi chiamati \textbf{formalismi massimi}
        \end{itemize}
\end{enumerate}

Una volta costruita una visione di insieme più completa, può venire naturale porsi delle domande del tipo:

\begin{itemize}
  \item \inlinequote{I formalismi introdotti sono adeguati per catturare l'essenza di un solutore meccanico?}
  \item \inlinequote{La capacità di un meccanismo di risolvere un problema dipende da come è formalizzato?}
  \item \inlinequote{Esistono formalismi e modelli più potenti delle \TM?}
  \item \inlinequote{Una volta che un problema è stato formalizzato adeguatamente, è sempre possibile risolverlo tramite dispositivi meccanici?}
\end{itemize}

Per rispondere a queste \textit{(e altre)} domande, è nata una branca dell'informatica detta \textbf{teoria della computabilità}.

\subsection{Formalizzazione di un problema matematico}

I formalismi fino ad ora studiati sono adeguati per tutti i problemi con domini numerabili, ovverosia gli insiemi i cui elementi possono essere messi in corrispondenza biunivoca con gli elementi di \(\mathbb{N}\).
Il problema originale, quindi, si riduce a quello del calcolo di una funzione \(f: \mathbb{N} \rightarrow \mathbb{N}\), esattamente come una traduzione o come un riconoscimento di linguaggio in un determinato alfabeto.

\bigskip
Si consideri, come \textit{esempio}, il problema relativo alla ricerca delle soluzioni di un sistema di equazioni del tipo:
\[\begin{cases}
    a_1 x_1 + a_2 x_2 = c_1 \\
    b_1 x_1 + b_2 x_2 = c_2
  \end{cases}\]
Esso può essere descritto come il calcolo della funzione \(f\) da \(\mathbb{Z}^6\) a \(\mathbb{Q}^2\) definita come:
\[ f(a_1, a_2, b_1, b_2, c_1, c_2) = \langle x_1, x_2 \rangle \]
dove \(x_1, x_2\), se esistono, sono i numeri razionali che soddisfano il sistema.

Si nota poi che è possibile far corrispondere biiettivamente \(\mathbb{Z}\) \textit{(numeri interi)} e \(\mathbb{Q}\) \textit{(numeri razionali)}  con \(\mathbb{N}\)  \textit{(numeri interi positivi)}, riducendo il problema al calcolo dell'opportuna funzione \(f: \mathbb{N} \rightarrow \mathbb{N}\) come proposto prima.

Poiché tutti e \(3\) gli insiemi sono \textbf{enumerabili}, la correlazione esiste e può essere costruita.
Un esempio di tale correlazione è mostrata nella Tabella~\ref{tab:correlazione-z-n}.

\begin{table}[htbp]
  \bigskip
  \centering
  \begin{tabular}{c|cccccccc}
    \(\mathbb{Z}\) & \(0\) & \(1\) & \(-1\) & \(2\) & \(-2\) & \(3\) & \(-3\) & \dots \\ \hline
    \(\mathbb{N}\) & \(0\) & \(1\) & \(2\)  & \(3\) & \(4\)  & \(5\) & \(6\)  & \dots \\
  \end{tabular}
  \bigskip
  \caption{Correlazione tra \(\mathbb{Z}\) e \(\mathbb{N}\)}
  \label{tab:correlazione-z-n}
\end{table}

\subsection{Riconoscimento e traduzione}

Il \textbf{riconoscimento} e la \textbf{traduzione} sono due formulazioni di un problema che possono essere ridotte l'una nell'altra.

Infatti, il problema di stabilire se, per una data stringa \(x\) e un dato linguaggio \(L\), \(x \in L\) si può anche impostare come la traduzione \(\tau_L\), dove

\[ \tau_L(x) =
  \begin{cases}
    1 & \text{se } x \in L \\
    0 & \text{altrimenti }
  \end{cases} \]

\bigskip
Viceversa, data una traduzione \(\tau : V^\ast_1 \rightarrow V^\ast_2\) si può definire il linguaggio:
\[ L_\tau = \left\{ z \, | \, z = x \$ y, \ x \in V^\ast_1, \ y \in V^\ast_2, \ \$ \notin (V_1 \cup V_2), \ y = \tau(x) \right\} \]
cioè il linguaggio delle stringhe formate da una stringa su \(V^\ast_1\), seguita dalla sua traduzione separata dal carattere speciale \(\$\).

Un dispositivo che riconosce \(L_\tau\) può essere usato come trasduttore che calcola \(\tau\): per ogni \(x\), è possibile enumerare \textbf{tutte le infinite} \(y \in V^\ast_2\) e verificare se \(x \$ y \in L_\tau\).
Se ciò avviene, si può concludere che la stringa \(y\) è la traduzione rispetto a \(\tau\) della stringa \(x\), cioè \(\tau(x) = y\).
Tuttavia questo processo termina se e solo se \(\tau(x)\) è definita, perché se così non fosse si procederebbe a enumerare le \(y \in V^\ast_2\) senza mai trovare una stringa che appartenga a \(L_\tau\).

\bigskip
Infine, è importante ricordare che:

\begin{itemize}
  \item Tutti i formalismi esaminati sono discreti, perché domini matematici numerabili definiti in modo finito
  \item La classe dei problemi che possono essere risolti da una \TM è indipendente dall'alfabeto scelto \textit{(a patto sia composto da almeno due simboli)}
\end{itemize}

\subsection{\TM e linguaggi di programmazione}

Data una \TM \(M\), è possibile scrivere un programma che la simuli in un qualsiasi linguaggio.
Analogamente, dato un qualunque programma, in un qualsiasi linguaggio di programmazione, è possibile costruire una \TM che calcola la stessa funzione eseguita dal primo.

Quindi, è immediato dedurre che \textbf{le \TM hanno lo stesso potere espressivo dei linguaggi di programmazione di alto livello} e di conseguenza sarà possibile sfruttare una \TM per eseguire un algoritmo.

\subsubsection{Algoritmi}
\label{sec:algoritmi}

Il concetto di \textbf{algoritmo} è uno dei concetti centrali dell'informatica.
\textit{Intuitivamente}, esso indica la procedura di risoluzione di un problema mediante un dispositivo automatico di calcolo, come un calcolatore elettronico.
Alternativamente può indicare un metodo per descrivere una serie astratta di comandi elementari, indipendenti dal linguaggio comprensibile a un calcolatore, per risolvere un dato problema.

Una definizione formale di algoritmo sarebbe di difficile formulazione, quindi ci si limita a descriverne delle proprietà in modo informale:

\begin{enumerate}[label=\Alph*), ref=(\Alph*)]
  \item \label{enum:proprieta-1-algoritmi} Un algoritmo contiene una sequenza \textbf{finita} di istruzioni
  \item Ogni istruzione deve essere \textbf{immediatamente eseguibile} tramite qualche procedimento meccanico
  \item  \label{enum:proprieta-3-algoritmi} Il processore è dotato di \textbf{dispositivi di memoria} in cui possono essere immagazzinati i risultati intermedi
  \item \label{enum:proprieta-4-algoritmi} La computazione è un processo \textbf{discreto}
        \begin{itemize}[label=\(\rightarrow\)]
          \item l'informazione è codificata in forma digitale
          \item la computazione procede attraverso passi discreti
        \end{itemize}
  \item \label{enum:proprieta-5-algoritmi} Gli algoritmi sono eseguiti in modo \textbf{deterministico}
  \item \label{enum:proprieta-6-algoritmi} \textbf{Non esiste un limite} alla quantità di \textbf{dati} di ingresso e uscita
  \item \label{enum:proprieta-7-algoritmi} \textbf{Non esiste un limite} alla quantità di \textbf{memoria} richiesta per effettuare i calcoli
  \item \label{enum:proprieta-8-algoritmi} \textbf{Non esiste limite} al numero di passi discreti richiesti per effettuare un calcolo
        \begin{itemize}[label=\(\rightarrow\)]
          \item si possono avere computazioni infinite
        \end{itemize}
\end{enumerate}

Le ipotesi dalla \ref{enum:proprieta-1-algoritmi}~alla~\ref{enum:proprieta-4-algoritmi} si applicano agli algoritmi nei calcolatori digitali, mentre quelle dalla \ref{enum:proprieta-5-algoritmi}~alla~\ref{enum:proprieta-8-algoritmi} si applicano in senso generale.
Il punto~\ref{enum:proprieta-4-algoritmi} relativo al non determinismo è una semplificazione \textit{(analogamente al punto~\ref{enum:proprieta-3-algoritmi} riguardante la memoria)} rispetto al formalismo delle \TM.

\subsection{Tesi di Church}
\label{sec:tesi-di-church}

La tesi di Church è intrinsecamente non dimostrabile perché è basata solo sull'esperienza precedente e sull'evidenza intuitiva.
Per questo motivo non è considerato un teorema.

È divisa in due parti.

\subsubsection{Prima parte}
La prima parte della \textbf{tesi di Church} afferma che:

\indentquote{Non esiste alcun formalismo, per modellare una determinata computazione meccanica, che sia più potente delle \TM e dei formalismi a esse equivalenti.}

Con la locuzione \textit{potenza di calcolo} si intende la classe di problemi risolvibili tramite un determinato formalismo e non lo \inlinequote{sforzo} che la macchina impiega nel calcolo della soluzione.

Questa tesi è da verificare ogni qualvolta viene inventato un modello più potente di calcolo \textit{(come i computer quantistici)} ma tutt'oggi non è ancora stata provata la sua falsità.

\subsubsection{Seconda parte}

La seconda parte della \textbf{tesi di Church} afferma che:

\indentquote{Ogni algoritmo per la soluzione automatica di un problema può essere codificato in termini di una \TM (o di un formalismo equivalente).}

Ciò implica che nessun algoritmo può risolvere problemi che non possano già essere risolti da una \TM: essa infatti rappresenta il più potente calcolatore che sia possibile costruire.

\bigskip
Per la definizione di algoritmo, si faccia riferimento alla Sezione~\ref{sec:algoritmi}.

\subsection{Enumerazione delle \TM e \TM universali}
\label{sec:enumerazione-TM-UTM}

Fino ad ora le \TM sono state analizzate in quanto dispositivi in grado di risolvere \textbf{un particolare} problema.
In particolare, è stato definito il problema definito dalla funzione di transizione \(\delta\), facente parte della struttura della macchina stessa.

Per questo motivo, le \TM possono essere considerate come macchine calcolatori astratti, specializzati e non programmabili.
Una volta caricato il programma nella memoria \textit{(l'automa di controllo)}, la macchina potrà eseguire solo quella particolare sequenza di istruzioni.

A questo punto può venire naturale chiedersi:

\indentquote{È possibile modellare un calcolatore programmabile con una \TM? Le \TM sono in grado di calcolare tutte le funzioni da \(\mathbb{N}\) a \(\mathbb{N}\)?}

Le prossime Sezioni si dedicheranno alla ricerca di una soluzione a entrambe le domande.

\subsubsection{Enumerazione algoritmica}
\label{sec:enumerazione-algoritmica}

Dato un qualsiasi insieme \(S\), esso può essere \textbf{numerato algoritmicamente} se è possibile stabilire una relazione biettiva fra lo stesso \(S\) e l'insieme dei numeri naturali \(\mathbb{N}\), tramite un algoritmo o una \TM \textit{(grazie alla tesi di Church)}.
È possibile dimostrare formalmente che le \TM possono essere enumerate \textbf{algoritmicamente}.

\bigskip
Per chiarire le idee sulla enumerazione, si consideri il seguente \textit{esempio}:

Sia \(L\) l'insieme \(\{a, b\}^\ast\) delle stringhe sull'alfabeto \(\{a, b\}\).
Tale insieme è algoritmicamente enumerabile: per esempio, si possono ordinare le stringhe per lunghezza crescente e, a parità di lunghezza, assegnare un ordine tra gli elementi dell'alfabeto su cui sono definite.
Il risultato è mostrato nella Tabella~\ref{tab:enumerazione-algoritmica}.

\begin{table}[htbp]
  \bigskip
  \centering
  \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
    \(\epsilon\) & \(a\) & \(b\) & \(aa\) & \(ab\) & \(ba\) & \(bb\) & \(aaa\) & \(aab\) & \dots \\ \hline
    \(0\)        & \(1\) & \(2\) & \(3\)  & \(4\)  & \(5\)  & \(6\)  & \(7\)   & \(8\)   & \dots
  \end{tabular}
  \bigskip
  \caption{Enumerazione algoritmica su \(\{a, b\}^\ast\)}
  \label{tab:enumerazione-algoritmica}
\end{table}

\subsubsection{Algoritmo di enumerazione}

Per semplicità, si fissi un alfabeto \(A\) e si consideri l'insieme \(\{\TM_A\}\) delle \TM a nastro singolo e senza stati finali, aventi \(A\) come insieme di simboli.
Le \TM in \(\{\TM_A\}\) accettano una stringa se e solo se arrivano in uno stato di \texttt{halt}. In caso contrario, compierebbero infinite mosse, entrando in un loop da cui non possono più uscire.
Queste semplificazioni non comportano perdita di generalità perché queste operazioni sono possibili su ciascuna \TM vista fino ad ora.

L'obiettivo di questa dimostrazione sarà spiegare come \(\{\TM_A\}\) possa essere numerato, ossia come sia possibile stabilire una biiezione \(\mathscr{E}: \mathbb{N} \leftrightarrow \{\TM_A\}\).

Si osservi in primo luogo che \(\forall \, k \in \mathbb{N}\) esiste un numero finito di \TM, con \(k\) stati e con alfabeto \(A\).
Infatti, la funzione di transizione \(\delta\) sarà definita su dominio e codominio finiti.
In senso più generale, data una funzione \(f: D \rightarrow R\), con \(D, R\) finiti, vi sono esattamente \(|R|^{|D|}\) funzioni totali \(f\) diverse.

Seguendo lo stesso ragionamento per le \TM, poiché la loro funzione di transizione di transizione parziale \(\delta\) è definita come
\[ \delta : Q \times A \rightarrow Q \times A \times \{R, L, S\}^{k+1} \cup \{\bot\}\]
esisteranno esattamente:
\[ (1 + 3 \times |Q| \cdot |A|)^{|Q| \cdot |A|} = (1 + 3 \times h \cdot k)^{h \cdot k} \]
\TM, aventi \(|A| = h, |Q| = k\) \textit{(cardinalità dell'alfabeto e degli stati)}.
Il termine unitario è dovuto alla presenza di \(\bot\) nel codominio.

Per poter enumerare le \TM dell'insieme sarà però prima necessario ordinarle in base all'\textit{ordine lessicografico}.
A tal fine è sufficiente imporre un ordinamento all'interno degli insiemi \(Q\), \(A\) ed \(\{R, L, S\}\), ottenendo un insieme ordinato:
\[ \mathscr{E}: \{TM_A\} \mapsto \mathbb{N} \]

L'enumerazione \(\mathscr{E}\) è algoritmica: è possibile implementare la costruzione con un algoritmo \textit{(sia esso implementato in un linguaggio di programmazione o a sua volta una \TM)} che emetta in uscita tutti gli elementi di \(\{\TM_A\}\) nell'ordine lessicografico appena imposto.
Analogamente, sarà possibile costruire un algoritmo che faccia l'operazione inversa, cioè che partendo da una \TM ne estragga il corrispondente \(k \in \mathbb{N}\).

\bigskip
Infine l'enumerazione calcolabile da una \TM prende il nome di \textbf{Gödelizzazione} e il numero naturale associato verrà chiamato \textbf{numero di Gödel} dell'oggetto.
Nella precedente enumerazione, ogni \TM è biiettivamente identificata dal suo numero di Gödel.

\subsubsection{Riassunto dell'enumerazione}
\label{sec:riassunto-enumerazione}

Per riassumere i concetti visti nell Sezione precedente, si ottiene un enunciato fondamentale:

\begin{enumerate}
  \item Per ogni alfabeto \(A\), il corrispondente l'insieme di \TM \(\{\TM_A\}\) può essere numerato algoritmicamente, perché può essere sempre stabilita una biiezione \(\mathscr{E}: \{TM_A\} \leftrightarrow \mathbb{N}\)
  \item Tutte le funzioni \textbf{algoritmicamente computabili} si possono enumerare algoritmicamente
  \item Tutti i linguaggi \textbf{algoritmicamente riconoscibili} si possono enumerare algoritmicamente
\end{enumerate}

Si noti che, sebbene \(\mathscr{E}\) sia una funzione biettiva, le enumerazioni delle funzioni computabili e dei linguaggi riconoscibili non sono sempre tali, perché si verifica che molte \TM sono in grado di calcolare la medesima funzione.

\bigskip
Per comodità, nel resto del corso si farà riferimento a \TM che si comportano come dispositivi che calcolano funzioni da \(\mathbb{N}\) a \(\mathbb{N}\).
Per ogni funzione \(f: D \rightarrow R\), con \(D\), \(N\) diversi da \(\mathbb{N}\), si presupporrà implicitamente la codifica algoritmica di \(D\) e \(R\) in funzione di \(\mathbb{N}\).
Analogamente si farà riferimento alla \(y\)-esima \TM generata dalla enumerazione \(\mathscr{E}\) con \(M_y\) (quindi \(M_y = \mathscr{E}(y)\)).

\subsubsection{Macchina di Turing universale - \UTM}
\label{sec:macchina-turing-universale}

Fino a ora le \TM sono state considerate come calcolatori astratti, specializzati e non programmabili.
In questa sezione verrà introdotta la \textbf{macchina di Turing universale} (o \UTM, dall'Inglese \textit{Universal Turing Machine}), ossia una \TM in grado di modellare dispositivi di risoluzione dei problemi, in cui il problema da risolvere non viene codificato nella struttura del dispositivo ma viene fornito in ingresso insieme ai dati su cui operare.

\bigskip
La \UTM può essere definita come una \TM che calcola la funzione \(g(y, x) = f_y(x)\), dove:

\begin{itemize}
  \item \(y \in \mathbb{N}\) indica l'indice della \textbf{funzione} \(f_y\) calcolata dalla \TM \(M_y\)
  \item \(x \in \mathbb{N}\) rappresenta l'\textbf{ingresso} su cui opera \(M_y\)
\end{itemize}

Apparentemente la \UTM così definita non sembrerebbe appartenere all'insieme \(\{\TM_A\}\) in quanto le funzioni a esso associate sono in una variabile, mentre \(g\) è definita a due variabili.
Questo problema si dimostra facilmente risolvibile in quanto esiste \textbf{sempre} una biezione \(\mathbb{N} \times \mathbb{N} \mapsto \mathbb{N}\).
Un esempio di tale biiezione è dato dalla funzione:
\[ d(x, y) = \dfrac{(x+y)(x+y+1)}{2} + x \]
che permette di associate una coppia \(x, y \in \mathbb{N}\) a un numero  \(k \in \mathbb{N}\) in modo biunivoco.
Allo stesso modo sarà possibile costruire la funzione inversa \(d^{-1}\).

La Figura~\ref{fig:associazione-biunivoca} illustra il funzionamento della biezione.

\bigskip
Poiché si dimostra che \(d\) e \(d^{-1}\) sono computabili, la \TM sarà \textbf{sempre} un grado di calcolarla.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-15.tikz}
  \caption{Illustrazione dell'associazione biunivoca}
  \label{fig:associazione-biunivoca}
  \bigskip
\end{figure}

Si osservi che \(g\) è una funzione \TM-computabile, ossia esiste un \(i \in \mathbb{N}\) tale che \(f_i = g\) ed è possibile calcolarla mediante i passi seguenti:

\begin{enumerate}
  \item Si sceglie un alfabeto finito \(A\) per codificare ogni informazione richiesta per la computazione.
        \begin{itemize}[label=\(\rightarrow\)]
          \item qualunque \(A\) con \(|A| \geq 2\) è adatto allo scopo
        \end{itemize}
  \item Si traduce la rappresentazione di \(n\) nella opportuna coppia \(\langle x, y \rangle\) tramite la funzione biettiva scelta
  \item Si traduce il numero \(y\) in un'opportuna codifica della \TM \(y\)-esima nella enumerazione \(\mathscr{E}\), \(M_y\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item La traduzione viene memorizzata sul nastro della \UTM
        \end{itemize}
  \item Si simula la computazione di \(M_y\) su \(x\)
        \begin{itemize}[label=\(\rightarrow\)]
          \item la \UTM lascia sul nastro \(f_y(x)\) se e solo se \(M_y\) termina la sua computazione su \(x\)
        \end{itemize}
\end{enumerate}

Quindi viene dimostrato che la \UTM è in grado di calcolare \(g(x, y) = f_y(x) \ \forall \, x, y\), cioè il comportamento di ogni altra \TM.
La \UTM è quindi in grado di calcolare anche il comportamento di \textbf{sé stessa}.

\bigskip
Con questo enunciato viene risposta la prima domanda posta nella Sezione~\ref{sec:enumerazione-algoritmica}.

\subsection{Problemi algoritmicamente irrisolvibili}

Prima di potersi occupare della risolvibilità dei problemi \textit{(e della loro relativa irrisolvibilità)}, è necessario studiare il numero delle stesse funzioni computabili.
Come concluso nella Sezione~\ref{sec:riassunto-enumerazione}, infatti, tutte le funzioni computabili \(f_y : \mathbb{N} \rightarrow \mathbb{N}\) sono enumerabili.
Questo significa che la cardinalità del loro insieme è \(\aleph_0\).

Analogamente, la cardinalità della classe \(\mathscr{F}\) delle funzioni su \(\mathbb{N}\), cioè \(\mathscr{F} = \left\{ f \, | \, f: \mathbb{N} \rightarrow \mathbb{N} \right\}\) è:
\[ |\mathscr{F}| = |\mathbb{N}| \times |\mathbb{N}| = \aleph_0 \times \aleph_0 = 2^{\aleph_0} \]
Questo valore è noto come \textbf{cardinalità del continuo} perché pari alla cardinalità dell'insieme \(\mathbb{R}\) dei numeri reali.

Quindi rapportando le due cardinalità \textit{(rispettivamente \(\aleph_0\) e \(2^{\aleph_0}\) per le funzioni risolvibili e per tutte le funzioni)} si conclude che la prima è minore della seconda (\(\aleph_0 < 2^{\aleph_0}\)), e quindi \textbf{non tutte le tutte le funzioni sono risolvibili}.

\bigskip
Così facendo è stata data risposta anche alla seconda domanda posta nella Sezione~\ref{sec:enumerazione-algoritmica}.

\subsubsection{Problemi definibili}

\textit{Intuitivamente}, le conclusioni raggiunte nella sezione precedente non forniscono grandi limitazioni.
Infatti, nonostante la classe di tutte le funzioni \(f: \mathbb{N} \rightarrow \mathbb{N}\) sia di cardinalità superiore alla classe delle funzioni computabili, le sole funzioni \textit{interessanti} da calcolare sono quelle possono essere \textit{definite}.

Per poter quindi definire un problema, è necessario usare una stringa di un qualche linguaggio, sia esso di tipo naturale o formale.
Per esempio, alcune descrizioni di problemi possono essere:

\begin{itemize}
  \item \textit{il numero che moltiplicato per se stesso è uguale a y}
  \item \(f(x) = x^4- \pi \cdot x^2\)
  \item \(\displaystyle f(x) = \int_0^x z \cdot sin(z) dz\)
  \item \(f(x) = \displaystyle \lim_{x \rightarrow \infty} \dfrac{e^x}{x^2+x}\)
\end{itemize}

Ogni linguaggio è un sottoinsieme di \(A^\ast\) \textit{(il linguaggio delle \UTM prese in esame)} e quindi è numerabile.
Così si prova che l'insieme dei problemi che si possono definire è esso stesso numerabile e:
\[ \{\text{problemi risolvibili}\} \subseteq \{\text{problemi definibili}\}\]
ma, \textit{purtroppo}, i due insiemi non coincidono \textbf{e parte dei problemi definibili non sarà risolvibile}.
La relazione tra i due insiemi sarà quindi:
\[ \{\text{problemi risolvibili}\} \subset \{\text{problemi definibili}\}\]

\textit{In pratica}, la classe dei problemi non definibili è rappresentata da tutti quei problemi che, per carenze di tipo linguistico \textit{(sia esso naturale, matematico o altro)} non è possibile esprimere.
L'assenza di una soluzione per essi non è rilevante in quanto non è neanche possibile porsi il problema di cercarla.

\subsection{Problema della terminazione - \textit{halting problem}}
\label{sec:problema-della-terminazione}

Una delle proprietà che si vuole garantire durante la stesura di un programma, qualunque sia il linguaggio, è la capacità del programma stesso di \textit{terminare} correttamente, ossia che \inlinequote{non vada in loop}.

Sarebbe estremamente utile poter determinare a priori, prima di eseguire una funzione con un determinato input, se essa porti a risultati in tempo finito o cada in loop infiniti che la portino a non terminare mai.

Questo problema, detto \textbf{problema della terminazione} (o in Inglese \textit{halting problem}), è \textit{descrivibile}, ma purtroppo \textit{non decidibile}.

\bigskip
Analizzando il problema in modo \textit{formale}, si indicherà con \(M_y\) la \TM \(y\)-esima secondo la numerazione \(\mathscr{E}\) e con \(f_y\) la funzione calcolata da \(M_y\).
Allora, è verificato che nessuna \TM può calcolare la funzione totale \(g: \mathbb{N} \times \mathbb{N} \rightarrow \{0, 1\}\) definita nel seguente modo:
\[ g(x, y) =
  \begin{cases}
    1 \text{ se } f_y(x) \neq \bot \\
    0 \text{ altrimenti}
  \end{cases}
\]
e che quindi nessuna \TM può decidere se, per una generica \TM \(M\) e un generico ingresso \(x\), \(M\) si arresta in uno stato finale \textit{(denotato come \(f_y(x) = \bot\))} con l'ingresso \(x\).

La dimostrazione avverrà tramite diagonalizzazione, dopo l'introduzione della tecnica stessa nella prossima Sezione.

\subsubsection{Dimostrazioni per diagonalizzazione}

Il ragionamento originale per diagonalizzazione fu usato da \textit{Georg Cantor} nel 1891 per dimostrare che \(\mathbb{R}\) ha una cardinalità maggiore di \(\mathbb{N}\).
È un metodo spesso usato per mostrare l'indecidibilità di alcuni problemi famosi e si riferisce a uno schema comune: si cerca una contraddizione quando esiste un insieme di funzioni \(A \rightarrow B\) indicizzato su \(A\).
Se \(f_k\) è una funzione indicizzata da \(k \in A\), un ragionamento per diagonalizzazione è dato dall'analisi degli elementi sulla diagonale della tabella ottenuta enumerando le \(f_k\), come nella Tabella~\ref{tab:dimostrazione-per-diagonalizzazione} (in cui la diagonale è evidenziata in \textbf{bianco su grigio}).

\begin{table}[htbp]
  \bigskip
  \centering
  \begin{tabular}{c | c c c c c}
    n          & 1                        & 2                        & 3                        & 4                        & \(\ldots\) \\ \hline
    \(f_1\)    & \whiteongray{\(f_1(1)\)} & \(f_1(2)\)               & \(f_1(3)\)               & \(f_1(4)\)               & \(\ldots\) \\
    \(f_2\)    & \(f_2(1)\)               & \whiteongray{\(f_2(2)\)} & \(f_2(3)\)               & \(f_2(4)\)               & \(\ldots\) \\
    \(f_3\)    & \(f_3(1)\)               & \(f_3(2)\)               & \whiteongray{\(f_3(3)\)} & \(f_3(4)\)               & \(\ldots\) \\
    \(f_4\)    & \(f_4(1)\)               & \(f_4(2)\)               & \(f_4(3)\)               & \whiteongray{\(f_4(4)\)} & \(\ldots\) \\
    \(\vdots\) & \(\vdots\)               & \(\vdots\)               & \(\vdots\)               & \(\vdots\)               & \(\ddots\) \\
  \end{tabular}
  \bigskip
  \caption{Dimostrazione per diagonalizzazione}
  \label{tab:dimostrazione-per-diagonalizzazione}
\end{table}

Allora si definisce l'elemento diagonale come una funzione \(d: A \rightarrow B\) che differisce dalla diagonale in ogni valore.
Poiché \(d\) è una funzione \(A \rightarrow B\), essa appare nella lista.

Visto che essa differisce anche da ogni elemento della lista sulla diagonale, la dimostrazione è avvenuta \textbf{per assurdo}.

\subsubsection{Dimostrazione della cardinalità del continuo}

La dimostrazione avviene per assurdo ed è strutturata in passi:

\begin{enumerate}
  \item Si supponga, per assurdo, che l'intervallo \([0, 1]\) sia numerabile
  \item Ciò implica che gli elementi di \([0, 1]\) possano essere messi in corrispondenza biunivoca con i numeri naturali, tramite successione di numeri reali \(\{n_1, n_2, \ldots\}\) che esaurisce tutti i numeri reali compresi tra \(0\) e \(1\)
  \item È possibile rappresentare ciascun numero della successione in forma decimale e visualizzare la successione di numeri reali come una matrice infinita che avrà circa questo aspetto:
        \[\begin{matrix}
            n_1 = 0, & 3 & 1 & 3 & 5 & 6 & 3 & 1 & \ldots \\
            n_2 = 0, & 6 & 8 & 1 & 3 & 9 & 8 & 7 & \ldots \\
            n_3 = 0, & 1 & 6 & 6 & 5 & 7 & 1 & 1 & \ldots \\
            n_4 = 0, & 1 & 3 & 6 & 6 & 5 & 5 & 8 & \ldots \\
            n_5 = 0, & 8 & 5 & 2 & 4 & 1 & 8 & 4 & \ldots \\
            n_6 = 0, & 4 & 1 & 2 & 4 & 2 & 5 & 7 & \ldots \\
            n_7 = 0, & 7 & 8 & 8 & 4 & 6 & 6 & 3 & \ldots \\
          \end{matrix}\]
  \item Si osservino ora le cifre lungo la diagonale della matrice, cioè sulla successione il cui \(k\)-esimo elemento è la \(k\)-esima cifra decimale di \(r_k\) come mostra la figura:
        \[\begin{matrix}
            n_1 = 0, & \whiteongray{3} & 1               & 3               & 5               & 6               & 3               & 1               & \ldots \\
            n_2 = 0, & 6               & \whiteongray{8} & 1               & 3               & 9               & 8               & 7               & \ldots \\
            n_3 = 0, & 1               & 6               & \whiteongray{6} & 5               & 7               & 1               & 1               & \ldots \\
            n_4 = 0, & 1               & 3               & 6               & \whiteongray{6} & 5               & 5               & 8               & \ldots \\
            n_5 = 0, & 8               & 5               & 2               & 4               & \whiteongray{1} & 8               & 4               & \ldots \\
            n_6 = 0, & 4               & 1               & 2               & 4               & 2               & \whiteongray{5} & 7               & \ldots \\
            n_7 = 0, & 7               & 8               & 8               & 4               & 6               & 6               & \whiteongray{3} & \ldots \\
          \end{matrix}\]
        Questa successione di cifre sulla diagonale, vista come un'espansione decimale, definisce un numero reale (che in questo caso può corrispondere a \(0.3866153 \ldots\)).
  \item Si consideri ora un nuovo numero reale \(x\) che abbia tutte le cifre differenti dalla sequenza sulla diagonale, definendolo come:
        \begin{itemize}
          \item se la \(k\)-esima cifra decimale di \(n_k\) è \(5\), allora la \(k\)-esima cifra di \(x\) è \(4\)
          \item se la \(k\)-esima cifra decimale di \(n_k\) non è \(5\), allora la \(k\)-esima cifra di \(x\) è \(5\)
        \end{itemize}
  \item All inizio della dimostrazione si era supposto che la lista \(\{n_1, n_2, \ldots\}\) enumerasse tutti i numeri reali compresi tra \(0\) e \(1\), quindi dovrebbe essere verificato che \(n_k = x_k\) per un qualche \(k\) e poiché \(x\) non ha dei \(9\) tra le cifre decimali, la sua rappresentazione è unica e sarà presente nella riga \(k\)
  \item A questo punto emerge la contraddizione: sia \(a\) la \(n\)-esima cifra decimale di \(n_k = x\). Essa può essere \(4\) o \(5\): per la definizione, \(a\) potrà essere \(4\) se e solo se è uguale a \(5\), e \(5\) negli altri casi. Questo è impossibile e ne segue che l'ipotesi di partenza è falsa.
\end{enumerate}

\subsubsection{Dimostrazione del Problema di Terminazione}
\label{sec:dimostrazione-problema-terminazione}

Come già annunciato, la dimostrazione avviene tramite tecnica diagonale.
Si assuma, per assurdo, che

\begin{minipage}{0.95\textwidth}
  \bigskip
  \begin{minipage}[]{0.55\textwidth}
    \[ g(y, x) =
      \begin{cases}
        1 \text{ se } f_y(x) \neq \bot & \quad \textit{ termina}    \\
        0 \text{ altrimenti}           & \quad \textit{non termina}
      \end{cases}
    \]
  \end{minipage}
  \begin{minipage}[]{0.35\textwidth}
    \begin{verbatim}
      int g(y, x) {
        if (halts(f_y(x)))
          return 1;

        return 0;
      }
    \end{verbatim}
  \end{minipage}
  \bigskip
\end{minipage}

\textit{(descritta sia matematicamente che in pseudocodice)} sia computabile.

Partendo da \(g\) si definisce ora una funzione \(h\) tale che:

\begin{minipage}{0.95\textwidth}
  \bigskip
  \begin{minipage}[]{0.55\textwidth}
    \[ h(x) =
      \begin{cases}
        1 \text{ se } g(x, x) = 0 & \quad f_x(x) \textit{ termina}      \\
        \bot \text{ altrimenti}   & \quad  f_x(x) \textit{ non termina}
      \end{cases}\]
  \end{minipage}
  \begin{minipage}[]{0.35\textwidth}
    \begin{verbatim}
      int h(x) {
        if (g(x, x) == 0)
          return 1;

        while (1) {};
      }
    \end{verbatim}
  \end{minipage}
  \bigskip
\end{minipage}

dove \(h(x) = 1 \Rightarrow g(x, x) = 0,\ f_x(x) = \bot\).
L'indefinizione non implica che la funzione \(h\) non sia calcolabile, ma significa che la funzione calcolata dalla corrispondente \TM non termina.

Usando \(h\), ci si trova quindi nella \textit{diagonale} della tabella delle \(f_y(x)\).

\bigskip
Se \(g\) fosse computabile, allora anche \(h\) lo sarebbe.
Analogamente, se \(h\) fosse computabile, allora si verificherebbe che \(h(i) = f_i(i)\) per una qualche \(i\).

Si calcoli ora \(h(i)\) ricordando che \(f_x(i) = h(i)\) e applicando la definizione appena data:
\[ h(i) =
  \begin{cases}
    1 \Rightarrow f_i(i) \neq \bot \Rightarrow g(i, i) = 0 \Rightarrow f_i(i) = \bot       & \textbf{\color{BrickRed}{assurdo}} \\
    \bot \Rightarrow f_i(i) = \bot \Rightarrow g(i, i) = \bot \Rightarrow f_i(i) \neq \bot & \textbf{\color{BrickRed}{assurdo}}
  \end{cases}
\]
Quindi entrambi i casi sono inammissibili e la dimostrazione è terminata.

\subsubsection{Lemma 1}

Sia \(h^\prime\) la funzione definita come:
\[ h^\prime(x) =
  \begin{cases}
    1 \text{ se } f_x(x) \neq \bot \\
    0 \text{ altrimenti }
  \end{cases}
\]
Allora \(h^\prime\) \textbf{non è computabile.}

Si noti che \(h^\prime(x)\) \inlinequote{si muove} ancora sulla diagonale (coincide con \(g(x, x)\)) ed è quindi una semplificazione di \(f(x)\).
È un problema analogo al problema di terminazione, di cui è appunto un lemma, ma non dipende da esso.

\paragraph[Dimostazione del Lemma 1]{Dimostrazione del Lemma \(1\)}

La dimostrazione segue la traccia della dimostrazione del Teorema di Terminazione, nel avvenuta nella Sezione~\ref{sec:dimostrazione-problema-terminazione}.

\bigskip
Si definisca inizialmente la funzione

\[ h(x) =
  \begin{cases}
    1 \text{ se } k(x) = 0 \\
    \bot \text{ altrimenti }
  \end{cases}
\]
Si osservi ora che \(h\) è computabile se \(k\) è computabile.
Una volta ottenuta una \TM \(M\) in grado di calcolare \(k\), sono sufficienti solo semplici modifiche per adattarla al calcolo di \(h\).

Considerando ora \(x_0\) il numero di Gödel di una terminata \TM \(M_{x_0}\) che calcola \(h\), ossia \(h = f_{x_0}\) e quindi \(h\) è la funzione calcolata dalla \(x_0\)-esima \TM.

Si verifica facilmente, tuttavia, che \(h(x_0)= 1\) implica \(f_{x_0}(x_0) = 1\), mentre \(h(x_0) = \bot\) implica \(h(x_0) = 1\).

Quindi \(f_{x_0}(x_0) = h(x_0) = 1\), che è una contraddizione.

\subsubsection{Lemma 2}

La funzione \(h^\prime(x)\) è un caso particolare della funzione \(g(y, x)\) perché:
\begin{itemize}
  \item \(h^\prime(x) = g(x, y)\)
  \item \(g\) non è computabile, perché \(g(y, x)\) è essa stessa non computabile
\end{itemize}

Se un problema non è risolvibile, allora un suo caso speciale può essere risolvibile.
Contrariamente, la generalizzazione di un problema non risolvibile è necessariamente non risolvibile.

Infine, se un problema è risolvibile, allora una sua generalizzazione potrebbe non essere risolvibile, mentre qualunque sua specializzazione è certamente risolvibile.

\subsubsection{Lemma 3}

Sia \(k(y)\) la funzione definita come:
\[ k(y) =
  \begin{cases}
    1 \text{ se } \forall \, x \in \mathbb{N}, \ f_y(x) \neq \bot \\
    0 \text{ altrimenti }
  \end{cases}
\]
Allora \(k(y)\) \textbf{non è computabile.}

Questa è una quantificazione su tutti i possibili dati in ingresso.
In qualche caso potrebbe essere possibile stabilire se \(f_y(x) \neq \bot\) per un qualche \(x\) specifico, ma non sarà mai possibile per ogni \(x \in \mathbb{N}\) \textit{(perché è un insieme infinito)}.

Se ciò, \textit{per assurdo}, fosse possibile, determinare se \(f_y\) è una funzione totale sarebbe impossibile.

\paragraph[Dimostrazione del Lemma 3]{Dimostrazione del Lemma \(3\)}

La dimostrazione segue la traccia della dimostrazione del Teorema di Terminazione, nel avvenuta nella Sezione~\ref{sec:dimostrazione-problema-terminazione}.

Per ipotesi, sia \(k(y)\) la funzione definita come:
\[ k(y) =
  \begin{cases}
    1 \text{ se } \forall \, x \in \mathbb{N}, \ f_y(x) \neq \bot \\
    0 \text{ altrimenti }
  \end{cases}
\]
Sempre per ipotesi, \(k(y)\) è \textbf{totale} e \textbf{computabile}.

Allora sarà possibile definire \(g(x) = w\), con \(w\) pari al numero di Gödel della \(x\)-esima \TM (definita nell'insieme \(\mathscr{E}\)) che calcola una funzione totale.

Se \(k\) fosse veramente computabile e totale, allora lo sarebbe anche \(g\).
Infatti:

\begin{enumerate}
  \item Sarebbe possibile calcolare \(k(0),\ k(1),\ \ldots\)
  \item Sia \(w_0\) il primo valore tale per cui \(k(w_0) = 1\)
  \item Sia \(g(0) = w_0\)
\end{enumerate}

La procedura è algoritmica, e siccome esistono infinite funzioni totali, \(g(x)\) è sicuramente definita per ogni \(x \in \mathbb{N}\), quindi è \textbf{totale} e \textbf{strettamente monotona}, perché \(w_{x+1} > w_x\).

Di conseguenza, anche \(g^{-1}\) è strettamente monotona ma non totale, perché \(g^{-1}\) è definita solo se \(w\) è il numero di Gödel di una funzione totale.

Allora si definisce la funzione \(h(x)\) come
\[ h(x) = f_{g(x)} + 1 = f_w(x) + 1 \]
e poiché \(f_w(x)\) è computabile e totale, anche \(h(x)\) lo sarà.

Per qualche valore di \(w_i\), infine, si verifica \(h(x) = f_{w_i}(x)\) e poiché \(h\) è totale, \(g^{-1}(w_i) = x_i \Rightarrow g^{-1}(w_i) \neq \bot\).

Confrontando gli ultimi due valori ottenuti, si giunge alla conclusione che:

\begin{align*}
  h(x_i) = f_{g(x_i)}(x_i) + 1 = f_{w_i}(x_1) + 1 \\
  h(x) = f_{w_i}(x) \Rightarrow h(x_i) = f_{w_i}(x_1)
\end{align*}

Cadendo quindi in una evidente contraddizione.

\subsection{Osservazioni sulla risolvibilità}

È importante notare che sapere che un problema è risolvibile non implica che il metodo di risoluzione sia noto.
In matematica è spesso necessario dare dimostrazioni non costruttive: si mostra che un oggetto matematico esiste, ma non lo si calcola.

In questo caso:
\begin{itemize}
  \item Un problema è \textbf{risolvibile} se esiste una \TM che lo risolve
  \item Per alcuni problemi si può concludere che esiste una \TM che li risolve, \textbf{senza tuttavia poterla costruire}
\end{itemize}

\subsection{Problema di decisione}

Un \textbf{problema di decisione} è una domanda che ha due possibili risposte: \textcolor{ForestGreen}{\texttt{SI}} o \textcolor{BrickRed}{\texttt{NO}}.
La domanda \textit{(e la consecutiva risposta)} sono relative a un qualche particolare ingresso.

\bigskip
\textit{Esempi} di problemi di decisione:
\begin{itemize}
  \item Dato un grafo \(G\) e un insieme di vertici \(K\), quest'ultima è una cricca \textit{(clique)}?
  \item Dato un grafo \(G\) e un insieme di lati \(M\), quest'ultimo è un albero ricoprente \textit{(spanning tree)}?
  \item Dato un insieme di assiomi, un insieme di regole e una formula, è possibile dimostrare quest'ultima a partire dai primi due? \textit{problema di incompletezza}
\end{itemize}

\subsubsection{Decidibilità}

Un problema si dice \textbf{decidibile} se esiste un algoritmo \textit{(procedura di decisione)} tale che:

\begin{itemize}
  \item È un procedimento \textbf{automatico} \textit{(da definizione di algoritmo)}
  \item Se il problema è \textbf{vero} per un determinato ingresso, allora ritorna \textcolor{ForestGreen}{\texttt{SI}}
  \item Se il problema è \textbf{falso} per un determinato ingresso, allora ritorna \textcolor{BrickRed}{\texttt{NO}}
\end{itemize}

\subsubsection{Semidecidibilità}

Un problema si dice \textbf{semidecidibile} se esiste un algoritmo \textit{(procedura di semidecisione)} tale che:

\begin{itemize}
  \item È un procedimento \textbf{automatico} \textit{(da definizione di algoritmo)}
  \item Se il problema è \textbf{vero} per un determinato ingresso, allora ritorna \textcolor{ForestGreen}{\texttt{SI}}
  \item Negli altri casi, \textbf{potrebbe non terminare mai}
\end{itemize}

Il problema della terminazione (Sezione~\ref{sec:problema-della-terminazione}) è \textbf{indecidibile} e \textbf{semidecidibile} allo stesso tempo.

\paragraph{Problema della terminazione e semidecidibilità}

È dimostrato che il problema della terminazione (Sezione~\ref{sec:problema-della-terminazione}) è \textbf{semidecidibile}.
Si consideri infatti l'analoga formulazione \(\exists \, z \, | \, f_x(z) \neq \bot\).

\textit{Schema di dimostrazione}:
\begin{itemize}
  \item Se si calcola \(f_x(0)\) e ne risulta che \(f_x(0) \neq \bot\) allora la risposta è \texttt{SI}
  \item Se la computazione non termina, come è possibile accorgersene?
  \item Dimostrazione con metodo diagonale:
        \begin{enumerate}
          \item Si simuli \(1\) passo di esecuzione di \(f_x(0)\). Se termina, allora la risposta è \texttt{SI}
          \item Altrimenti, si simuli un passo di computazione di \(f_x(1)\). Se termina, allora la risposta è \texttt{SI}
          \item Ancora una volta si simuli \(2\) passi di \(f_x(0)\), \(1\) passo di \(f_x(2)\), \(2\) passi di \(f_x(1)\), \(3\) di \(f_x(0)\) e così via, secondo lo schema in Figura~\ref{fig:associazione-biunivoca-2} \textit{(già vista nella Sezione relativa alla \nameref{sec:macchina-turing-universale})}
        \end{enumerate}
\end{itemize}

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.75]{image-15.tikz}
  \caption{Illustrazione dell'associazione biunivoca}
  \label{fig:associazione-biunivoca-2}
  \bigskip
\end{figure}

\paragraph{Osservazioni}

Grande parte dei problemi indecidibili è \textbf{anche} semidecidibile.
Un tipico esempio di questa affermazione è data dagli errori a \textit{runtime} nei programmi.

Si nota che in questi casi il problema semidecidibile è dato dalla ricerca della presenza dell'errore, non nella dimostrazione della sua assenza.
Ciò porta a una importante implicazione sulla verifica basata sul testing.
Secondo Dijkstra:

\indentquote{Il testing può dimostrare la presenza di errori e non la loro assenza.}

Molti problemi, seppur ben definiti, non possono essere risolti mediante procedimenti algoritmici.
Il risultato sarà trovato tramite tecniche differenti.

Allo stesso modo esistono problemi la cui soluzione algoritmica è nota, senza che il procedimento stesso sia noto.

\subsection{Insiemi ricorsivi}
\label{sec:insiemi-ricorsivi}

In questa sezione ci si dedicherà a studiare i sottoinsiemi di \(\mathbb{N}\), che verranno indicati d'ora in poi con la lettera \(S\).
Dati \(x \in \mathbb{N}, S \subseteq \mathbb{N}\), si vuole studiare l'appartenenza dell'elemento \(x\) allo stesso \(S\).

Questa formalizzazione del problema è del tutto generale perché applicabile a qualsiasi insieme che si possa mettere in corrispondenza biunivoca ed effettiva con \(\mathbb{N}\), ossia per tutti gli elementi numerabili.

\subsubsection{Funzione caratteristica di un insieme}

Dato un insieme \(S\), la sua \textbf{funzione caratteristica} \(c_s : \mathbb{N} \rightarrow \{0, 1\}\) è definita come:
\[
  c_s = \begin{cases}
    1 \text{ se } x \in S \\
    0 \text{ altrimenti }
  \end{cases}
\]
L'appartenenza di un elemento a un insieme e la ricorsività dello stesso è un problema la cui risolvibilità dipende dalla computabilità della sua funzione caratteristica.

\subsubsection{Definizione di insieme ricorsivo}

Un insieme \(S\) viene detto \textbf{ricorsivo} (oppure \textit{decidibile})  se e solo se la sua funzione caratteristica \(c_s\) è \textbf{computabile}.

Si ricordi che, per definizione, \(c_s\) è \textbf{totale} per ogni insieme \(S\).
Infatti, dato un qualsiasi elemento \(x \in \mathbb{N}\), esso appartiene (se \(c_1=1\)) o non appartiene (se \(c_s = 0\)) a \(S\).

\subsection{Insieme ricorsivamente enumerabile}
\label{sec:insieme-ricorsivamente-enumerabile}

Un insieme viene detto \textbf{ricorsivamente enumerabile} (oppure \textit{semidecidibile}) se e solo se è l'\textbf{insieme vuoto} oppure è l'\textbf{immagine di una funzione} totale e computabile \(g_s\), cioè:
\[ S = I_{g_s} = \left\{ x \, | \, \exists \, y, \ y \in \mathbb{N} \land x = g_s(y)  \right\} \]
Gli insiemi ricorsivamente enumerabili devono il loro nome al fatto che il problema dell'appartenenza può essere risolto \textbf{meccanicamente} \textit{(oppure algoritmicamente)}.
Un calcolatore meccanico in grado di implementare la funzione caratteristica di un insieme fornirà necessariamente una risposta alla domanda \(x \in S \ \forall \, x\).
Gli insiemi ricorsivamente enumerabili sono una definizione \textit{più debole} degli insiemi \textit{ricorsivi}.

\bigskip
Il termine \textit{“semidecidibile”} puo essere spiegato intuitivamente come:

\indentquote{se si suppone che \(x \in S\), allora enumerando gli elementi di \(S\) anche \(x\) verrà trovato e sarà possibile rispondere alla domanda. Ma se \(x \notin S\)?}

\subsection[Teorema 1/2 + 1/2 = 1]{Teorema \(\frac{1}{2} + \frac{1}{2} = 1\)}

Sia \(S\) un insieme.
Allora:

\begin{enumerate}[label=\Alph*), ref=(\Alph*)]
  \item \label{enum:teorema-0.5-0.5-1-a} Se \(S\) è \textbf{ricorsivo}, è anche \textbf{ricorsivamente enumerabile}
  \item \label{enum:teorema-0.5-0.5-1-b} \(S\) è \textbf{ricorsivo} se e solo se sia \(S\) che \(\overline{S} = \mathbb{N} - S\) sono \textbf{ricorsivamente enumerabili}
\end{enumerate}

\textbf{Corollario}: la classe di insiemi decidibili \textit{(linguaggi, problemi, \dots)} è chiusa rispetto al complemento.

\subsubsection{Dimostazione punto \ref{enum:teorema-0.5-0.5-1-a}}

Sia \(c_s\) la funzione caratteristica di \(S\).
Se \(S\) è vuoto, allora è ricorsivamente enumerabile per definizione.
Altrimenti, esiste almeno un elemento \(k \in S\).

Sia \(g_s(x)\) la funzione definita come:
\[ g_s(x) = \begin{cases}
    x \text{ se } c_s(x) = 1 \\
    k \text{ altrimenti }
  \end{cases}
\]

Allora \(g_s(x)\) è totale, computabile e \(I_{g_s} = S\).
Quindi poiché \(S\) \'e l'immagine di una funzione totale e computabile, è ricorsivamente enumerabile per definizione.

\bigskip
Questa è una definizione \textbf{costruttiva}.
Infatti, non è definito né se \(S = \emptyset\) né l'algoritmo per trovare un \(k\) specifico.
Viene definita solo l'esistenza di \(g_s\) per \(S \neq \emptyset\).

\subsubsection{Dimostazione punto \ref{enum:teorema-0.5-0.5-1-b}}

\textbf{Dimostrazione che se \(S\) è ricorsivo, è anche ricorsivamente enumerabile.}

Se \(S\) è vuoto o coincide con \(\mathbb{N}\), allora l'enunciato è ovvio.

Altrimenti, esistono due funzioni totali e computabili \(g_s, g_{\overline{s}}\) tali che \(S = I_{g_s}, \overline{s} = I_{g_{\overline{s}}}\) nella forma:

\begin{align*}
  S            & = \left\{ g_s(0), g_s(1), \ldots \right\}                           \\
  \overline{S} & = \left\{ g_{\overline{s}}(0), g_{\overline{s}}(1), \ldots \right\}
\end{align*}

Per costruzione
\[ S \cup \overline{S} = \mathbb{N} \quad S \cap \overline{S} = \emptyset \]
e quindi:
\[ \forall \, x \, \in \mathbb{N} \, (\exists \, y \, | \, x = g_s(y) \lor x = g_{\overline{s}} \land \lnot (\, \exists \, z, w \, | \, x = g_s(z) \land x = g_{\overline{s}}(w) )) \]
cioè ogni \(x\) appartiene in modo esclusivo a \(S\) o a \(\overline{S}\).

\bigskip
Si consideri ora l'enumerazione:
\[ \mathscr{E}({L}) = \left\{ g_s(0), g_{\overline{s}}(0),  g_s(1), g_{\overline{s}}(1), \ldots \right\} \]
Poiché \(\mathscr{E} \equiv \mathbb{N}\), \(\forall \, x, x \in \mathscr{L}\) e, se \(x\) compare in \(\mathscr{L}\) in posizione parti, allora non compare mai in posizione dispari e viceversa.

Si definisca \(c_s\) come:
\[
  c_{s_i} = \begin{cases}
    1 \text{ se } x = s_i \in \mathscr{L} \text{ e } i = 2\cdot k + 1 \\
    0 \text{ altrimenti }
  \end{cases}
\]
Chiaramente \(c_s\) è ben definito, poiché \(x\) potrà apparire in \(\mathscr{L}\) solo in posizione o pari o dispari (e non entrambe).

Inoltre risulta totale e computabile, in quanto l'enumerazione \(\mathscr{L}\) può essere effettuata da una qualche \TM.
Basta infatti cercare se \(x\) (elemento dato) appare in posizione pari o dispari di \(\mathscr{L}\) per concludere se appartiene o meno a \(S\).

\bigskip
\textbf{Dimostrazione che se \(S\) è ricorsivo, allora anche \(\overline{S}\) lo è.}

Per definizione, la funzione caratteristica di \(\overline{S}\) è definita come:
\[  g_{\overline{s}}(x) = \begin{cases}
    1 \text{ se } c_s(x) = 1 \\
    0 \text{ altrimenti }
  \end{cases}  \]

Quindi in virtù del punto~\ref{enum:teorema-0.5-0.5-1-b} del teorema appena dimostrato, sia \(S\) che \(\overline{S}\) sono ricorsivamente enumerabili.

\subsubsection{Osservazioni di interesse pratico}

Si consideri l'insieme \(S\) con le seguenti caratteristiche:

\begin{itemize}
  \item \(i \in S \rightarrow f_i\) totale (cioè \(S\)) contiene solo indici di funzione \textbf{totali} e \textbf{computabili}
  \item \(f\) è \textbf{totale} e \textbf{computabile}, \(\exists \, i \in S \, | \, f_i = f\) e quindi \(S\) contiene ogni \(i\)
\end{itemize}

\(S\) è quindi l'insieme di indici di funzioni totali e computabili e non è ricorsivamente enumerabile.

Questa affermazione è dimostrabile per diagonalizzazione.
Quindi non esiste un formalismo ricorsivamente enumerabile che possa definire tutte le funzioni computabili e totali e solo quelle.
Gli \FSA possono definire solo alcune \textit{(non tutte)} funzioni totali, mentre le \TM definiscono tutte le funzioni computabili, anche quelle non totali.

Il linguaggio \texttt{C} permette di codificare qualunque algoritmo, ma anche quelli che non terminano.
Non esiste nessun sottoinsieme del linguaggio \texttt{C} che definisca esattamente tutti i programmi che terminano.

\bigskip
La relazione tra insiemi di problemi è illustrata nella Figura~\ref{fig:relazione-tra-insiemi-di-problemi}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-16.tikz}
  \caption{Relazione tra insiemi di problemi}
  \label{fig:relazione-tra-insiemi-di-problemi}
  \bigskip
\end{figure}

\subsection{Teoremi di \textit{Kleene} e \textit{Rice}}

\subsubsection{Teorema del punto fisso di \textit{Kleene}}

Sia \(t\) una funzione \textbf{totale} e \textbf{computabile}.
Allora è sempre possibile trovare un intero \(p\) tale che:
\[ f_p = f_{t(p)} \]

La funzione \(f_p\) è detta \textbf{punto fisso} di \(t\) perché \(t\) trasforma \textit{(una macchina che calcola)} \(f_p\) in \textit{(una macchina che calcola)} \(f_p\) stessa.

\paragraph{Dimostrazione del Teorma di \textit{Kleene}}

Sia \(u\) un qualsiasi numero naturale.
Si definisca un \TM che realizza la seguente procedura applicata al valore in ingresso \(x\):

\begin{enumerate}
  \item Calcola \(z = f_u(u)\)
  \item Quando la computazione di \(f_u(u)\) si ferma, se ciò avviene, calcola \(f_z(x)\) che non è detto che sia definito
\end{enumerate}

Poiché la procedura precedente è effettiva, esiste una \TM in grado di implementarla.
Inoltre è possibile costruire tale macchina e calcolare poi il suo indice \(g(u) \, \forall \, u\), usando la numerazione \(\mathscr{G}\) introdotta nella Sezione~\ref{sec:enumerazione-TM-UTM}.

\subsubsection{Teorema di \textit{Rice}}

Sia \(F\) un insieme qualunque funzioni \textbf{computabili}.

L'insieme \(S = \left\{ x \, | \, f_x \in F \right\}\) degli indici di \TM che calcolano le funzioni di \(F\) è \textbf{decidibile} (e quindi \textbf{ricorsivo}) se e solo se \(F = \emptyset\) oppure \(F\) è l'insieme di \textbf{tutte} le funzioni computabili.

Di conseguenza in tutti i casi non banali \(S\) \textbf{non è decidibile}.

\paragraph{Dimostrazione del Teorema di \textit{Rice}}

Per assurdo, si supponga che \(S\) sia ricorsivo, che \(F \neq \emptyset\) e che \(F\) non coincida con l'insieme di tutte le funzioni computabili.

Si consideri ora la funzione caratteristica \(c_s\) di \(S\) definita come:
\[ c_s(x) = \begin{cases}
    1 \text{ se } f_x \in F \\
    0 \text{ altrimenti }
  \end{cases} \]
Per ipotesi assurda, si supponga che \(c_s\) sia computabile.

Sempre per ipotesi, enumerando effettivamente tutte le \TM \(M_i\) si può trovare:
\begin{enumerate}[label=\alph*), ref=(\Alph*)]
  \item \label{enum:dimostrazione-rice-1} Il primo \(i \in S\) per cui \(f_i \in F\)
  \item \label{enum:dimostrazione-rice-2} Il primo \(j \notin S\) per cui \(f_j \notin S\)
  \item \label{enum:dimostrazione-rice-3} La funzione \(\overline{c}_s(x)\), computabile e totale, definita come
        \[ \overline{c}_s = \begin{cases}
            i \text{ se } f_x \notin F \\
            j \text{ altrimenti }
          \end{cases} \]
  \item \label{enum:dimostrazione-rice-4} Per il teorema di Kleene, un \(\overline{x}\) tale che \(f_{\overline{c}_s(\overline{x})} = f_{\overline{d}}\)
\end{enumerate}

\bigskip
Supponendo che \(\overline{c}_s (\overline{x}) = i\), per \ref{enum:dimostrazione-rice-3}, segue che \(f_{\overline{x}} \notin F\).
Tuttavia, per \ref{enum:dimostrazione-rice-4}, si ha che \(f_{\overline{x}}\) e per \ref{enum:dimostrazione-rice-1} si ha che \(f_i \in F\), una \textbf{contraddizione}.

Supponendo invece che \(\overline{c}_s (\overline{x}) = k\), per \ref{enum:dimostrazione-rice-4} si ha che \(f_{\overline{x}} = f_j\).
Tuttavia, per \ref{enum:dimostrazione-rice-3} si ha che \(f_{\overline{x}} = f_j\) e, per \ref{enum:dimostrazione-rice-2} si ha che \(f_j \notin F\), ottenendo ancora una \textbf{contraddizione}.

\paragraph{Conseguenze}

Il teorema di \textit{Rice} ha un forte impatto pratico negativo.

Ad esempio, ponendo \(f = \{g\}\), quindi un insieme composto dalla sola funzione \(g\), per il teorema \textbf{non è decidibile} se una generica \TM calcoli \(g\) o meno.
Tuttavia, per la tesi di \textit{Church} (Sezione~\ref{sec:tesi-di-church}) il risultato non è ristretto alle \TM e al formalismo delle funzioni.
Non è quindi possibile stabilire algoritmicamente se un dato algoritmo sia in grado di risolvere un determinato problema o se due programmi siano equivalenti \textit{(cioè se calcolano la stessa funzione)}.

Inoltre, non è possibile stabilire se un algoritmo risolve un problema che goda di una qualsiasi proprietà non banale \textit{(e quindi una proprietà che non sia appartenenza ad una categoria non esistente o coincidente con tutti i problemi risolvibili)}.

\subsection{Riduzione di problemi}

Un problema \(P^\prime\) è \textbf{ridotto} ad un problema \(P\) se un algoritmo per risolvere \(P\) viene usato per risolvere \(P^\prime\).

Quindi \(P\) è \textbf{risolvibile} ed esiste un algoritmo che, per ogni data istanza di \(P^\prime\):

\begin{enumerate}
  \item \textbf{Determina} una corrispondente istanza di \(P\)
  \item \textbf{Costruisce} algoritmicamente la soluzione dell'istanza di \(P^\prime\) dalla soluzione dell'istanza di \(P\)
\end{enumerate}

\textit{Formalmente}:

\begin{enumerate}
  \item Si vuole verificare se \(x \in S\)
  \item Si è in grado di verificare se \(y \in S^\prime\)
  \item Se esiste una funzione \(t\), computabile e totale tale che \[ x \in S \Leftrightarrow t(x) \in S^\prime \] allora è possibile determinare algoritmicamente se \(x \in S\)
\end{enumerate}

\bigskip
\textit{Esempio}: si supponga che il problema di cercare un elemento in un insieme sia risolvibile.
Allora si può risolvere anche il problema di calcolare l'intersezione tra due sistemi, attuando una riduzione.

\subsubsection{Implicazione della riduzione}

Il procedimento della riduzione funziona anche al contrario:

\begin{itemize}
  \item Si vuole sapere se è possibile risolvere \(x \in S\)
  \item Si sa che non è possibile risolvere \(y \in S^\prime\) (quindi \(S^\prime\) \textit{non è decidibile})
  \item Se esiste una funzione \(t\), computabile e totale, tale che \[ y \in S^\prime \Leftrightarrow t(y) \in S \] allora è possibile concludere che \(x \in S\) non è decidibile
\end{itemize}

In conclusione, è immediato dedurre che:

\begin{itemize}
  \item Se \(P^\ast\) è \textbf{risolvibile}, lo è anche \(P\)
  \item Se \(P\) è \textbf{irrisolvibile}, lo è anche \(P^\ast\)
\end{itemize}

\clearpage

\section{Complessità del calcolo}

Fin'ora è stato affrontato la determinazione della computazione di un problema, senza che venisse però analizzato il vero \inlinequote{costo} della soluzione.

Si supponga infatti di avere accesso al più potente super calcolatore al mondo.
Anche sfruttando tutta la sua capacità di calcolo, alcuni problemi \textit{(come la determinazione della partita perfetta a scacchi)} può richiedere una quantità di tempo superiore all'intera vita dell'universo \textit{(e si dimostra che questa tesi corrisponde alla realtà)}.

Di conseguenza, alcuni problemi vengono definiti \textbf{intrattabili}, perché pur esistendo un algoritmo per giungere alla sua soluzione, la sua esecuzione richiederebbe un tempo \inlinequote{inaccettabile}.
L'intrattabilità costituisce una grossa limitazione alle applicazioni pratiche, in quanto esistono parecchi problemi \inlinequote{interessanti} che sono anche intrattabili.

Il concetto di \textbf{trattabilità}, è strettamente legato a quello della \textbf{complessità}, che può essere vista come la misura del \inlinequote{costo} della risoluzione: all'aumentare della prima, il secondo lieviterà.
L'obiettivo di questa Sezione sarà definire in modo formale la nozione e lo studio del costo della soluzioni dei problemi, per poi essere in grado di progettare e combinare algoritmi e strutture dati in modo da realizzare soluzioni efficienti e non solo corrette.

\subsection{Analisi della complessità}

La tecnica che prende il nome di \textbf{analisi della complessità} consiste nel costruire strumenti per valutare la complessità di un calcolo, per poter successivamente analizzare algoritmi e strutture di nati notevoli.

Essi verranno sottoposti ad analisi quantitative su:

\begin{itemize}
  \item Tempo di calcolo impiegato - \textit{complessità temporale}
  \item Spazio di memoria occupato - \textit{complessità spaziale}
\end{itemize}

L'analisi si limiterà a criteri di costo \textbf{oggettivi} e formalizzabili, quindi non terrà conto di parametri come i costi di sviluppo e \textit{trade off} tra obiettivi contrastanti.

\bigskip
Per la Tesi di Church \textit{(Sezione~\ref{sec:tesi-di-church})}, un problema è calcolabile indipendentemente dallo strumento usato, purché esso sia Turing completo.

Questa affermazione non è valida per la complessità di calcolo: non è verosimile affermare che modelli di calcolo differenti impieghino lo stesso tempo per eseguire programmi analoghi.
Infatti, poiché non esiste una \inlinequote{Tesi di Church per la complessità}, sarà necessario costruire uno strumento in grado di valutare la complessità temporale e spaziale di un calcolo che tralasci \inlinequote{considerazioni superflue} e che sia utilizzabile per la maggioranza dei modelli di calcolo.

Contrariamente a quanto appena affermato, nonostante l'inesistenza di tale teorema, sarà possibile correlare in modo sistematico soluzioni proposte per modelli diversi.
Poiché non esiste un formalismo di calcolo perfettamente adatto a costruire il modello, l'analisi avverrà a partire dalle \TM deterministiche.

\subsection{Complessità temporale e spaziale}

Sia \(M\) una \TM \textbf{deterministica} con \(k\) nastri e sia \(c_0 \vdash^\ast c_r\) una computazione su \(M\).
Allora è possibile definire:

\begin{itemize}
  \item La \textbf{complessità temporale} come:
        \begin{itemize}
          \item \(T_M(x) = r\) se \(M\) termina in \(c_r\)
          \item \(\infty\) se \(M\) non termina
          \item poiché \(M\) è deterministica, la computazione sarà \textbf{unica} sull'ingresso \(x\)
        \end{itemize}
  \item La \textbf{complessità spaziale} come:
        \begin{itemize}
          \item \(\displaystyle S_M(x) = \sum^k_{j=1} \max_{i \in \{0, \ldots, r\}} \left(|\alpha_{ij}|\right)\)
                \begin{itemize}[label=\(\rightarrow\)]
                  \item \(\left(|\alpha_{ij}|\right)\) indica il contenuto del \(j\)-esimo nastro alla \(i\)-esima mossa
                  \item la complessità è pari alla somma delle quantità massime di nastro occupate per ogni nastro
                \end{itemize}
          \item la complessità \textbf{spaziale} è sempre minore della complessità \textbf{temporale}
                \begin{itemize}[label=\(\rightarrow\)]
                  \item \(\forall \, x \ \dfrac{S_M(x)}{k} \leq T_M(X)\)
                \end{itemize}
        \end{itemize}
\end{itemize}

In queste definizioni tuttavia si tiene conto della natura dell'ingresso \(x\), mentre risulta più conveniente studiare la complessità di un algoritmo in funzione della \textbf{dimensione} di \(x\).
Quindi si semplifica lo studio di \(T_M(x)\) ed \(S_M(x)\) ponendo \(n = |x|\) \textit{(la dimensione di \(x\))} e studiando \(T_M(n)\) e \(S_M(n)\).

Questa semplificazione implica une perdita di generalità perché ingressi diversi, seppur di dimensioni uguali, potrebbero avere complessità diverse.
\textit{Formalmente}:

\begin{align*}
  |x_1| = |x_2| \quad & \nRightarrow \quad \ T_M(x_1) = T_M(x_2) \\
  |x_1| = |x_2| \quad & \nRightarrow \quad \ T_S(x_1) = T_S(x_2)
\end{align*}

\bigskip
Per gestire la variabilità dell'ingresso così introdotta, è necessario distinguere tra:
\begin{itemize}[itemsep=5pt]
  \item \textbf{Caso pessimo} \(T_M(n) = \displaystyle \max_{n = |x|} \left(T_M(x)\right) \)
  \item \textbf{Caso ottimo} \(T_M(n) = \displaystyle \min_{n = |x|} \left(T_M(x)\right) \)
  \item \textbf{Caso medio} \(T_M(n) = \dfrac{\displaystyle \sum_{n = 0}^{|x|} T_M(x)}{|I|^n} \)
        \begin{itemize}[label=\(\rightarrow\)]
          \item somma dei tempi per parole lunghe \(n\) \textit{diviso} il numero di parole lunghe \(n\)
          \item è una media pesata
        \end{itemize}
\end{itemize}

Tipicamente, si considera il caso pessimo, perché è normalmente il più rilevante \textit{(fornisce un limite superiore)} e l'analisi risulta più semplice che nel caso medio \textit{(che dovrebbe tenere conto di ipotesi probabilistiche sulla distribuzione dei dati, complicando ulteriormente l'analisi stessa)}.

\subsubsection[Crescita di T(n) e S(n)]{Crescita di \(T_M(n)\) e \(S_M(n)\)}

Subito ci si può accorgere di come i valori esatti di \(T_M(n)\) e \(S_M(n)\) non siano particolarmente utili ai fini dello studio della complessità di un particolare problema.
Ciò che risulta rilevante, infatti, è il comportamento \textbf{asintotico} delle funzioni di costo, ossia quando \(n \rightarrow \infty\).

Questa è una semplificazione \inlinequote{aggressiva}, perché porta a perdere la distinzione tra casi del tipo

\begin{align*}
  T_M(n) & = n^4 - 5n^2 + 2    \\
  T_M(n) & = 10^{80} \cdot n^4
\end{align*}

entrambi \textit{ridotti} a \(n^4\), seppur con andamenti radicalmente diversi per valori finiti.

\bigskip
Si introducono quindi tre notazioni per indicare il comportamento asintotico di una funzione:
\begin{itemize}
  \item \bigO-grande: limite asintotico \textbf{superiore}
  \item \(\Omega\)-grande: limite asintotico \textbf{inferiore}
  \item \(\Theta\)-grande: limite asintotico sia \textbf{superiore} che \textbf{inferiore}
\end{itemize}

L'uso di questa notazione (tramite gli insiemi \(\bigO,\ \Omega,\ \Theta\)) permette di evidenziare con facilità la parte più importante di una funzione di complessità.

Questo modello tuttavia presenta delle limitazioni:

\begin{itemize}
  \item Potrebbe non essere sufficientemente accurato per valori \textbf{piccoli} di \(n\)
        \begin{itemize}
          \item la notazione \(\Omega\)-grande è più precisa rispetto alla notazione \bigO-grande per i valori piccoli di \(n\)
        \end{itemize}
  \item Algoritmi con complessità asintotica maggiore possono essere più veloce di uno a complessità minore per valori \textbf{piccoli} di \(n\)
\end{itemize}

Per ovviare a questi problemi, si faranno uso di tutte e tre le notazioni in base ai casi presi in analisi.

\subsubsection[Notazione O-grande]{Notazione \bigO-grande}

\textbf{Definizione}: siano \(f(n),\ g(n)\) funzioni \(\mathbb{N} \rightarrow \mathbb{R}^+\).
Allora \(\bigO(g(n))\) è l'insieme di funzioni definito come:

\[ \bigO(g(n)) = \left\{ f(n) \, | \, \exists \, c > 0, n_0 > 0 \text{ tali che } \forall \, n > n_0, \ 0 \leq f(n) \leq c \cdot g(n) \right\} \]

\bigskip
\textit{Informalmente}, le funzioni in \(\bigO(g(n))\) sono \textit{dominate} da \(c \cdot g(n)\) a partire da \(n_0\).
Un'illustrazione di questa relazione è rappresentata in Figura~\ref{fig:notazione-o-grande}.

\bigskip
Per brevità, è comune scrivere \(f(n) = \bigO(g(n))\) al posto della più corretta notazione \(f(n) \in \bigO(g(n))\).

\subsubsection[Notazione Omega-grande]{Notazione \(\Omega\)-grande}

\textbf{Definizione}: siano \(f(n),\ g(n)\) funzioni \(\mathbb{N} \rightarrow \mathbb{R}^+\).
Allora \(\Omega(g(n))\) è l'insieme di funzioni definito come:

\[ \Omega(g(n)) = \left\{ f(n) \, | \, \exists \, c > 0, n_0 > 0 \text{ tali che } \forall \, n > n_0, \ 0 \leq c \cdot g(n) \leq f(n)  \right\} \]

\bigskip
\textit{Informalmente}, le funzioni in \(\Omega(g(n))\) \textit{dominano}  \(c \cdot g(n)\) a partire da \(n_0\).
Un'illustrazione di questa relazione è rappresentata in Figura~\ref{fig:notazione-omega-grande}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{minipage}[b]{0.45\textwidth}
    \centering
    \tikzfig[0.5]{image-20.tikz}
    \caption{Notazione \bigO-grande}
    \label{fig:notazione-o-grande}
  \end{minipage}
  \begin{minipage}[b]{0.45\textwidth}
    \centering
    \tikzfig[0.5]{image-21.tikz}
    \caption{Notazione \(\Omega\)-grande}
    \label{fig:notazione-omega-grande}
  \end{minipage}
  \bigskip
\end{figure}

\subsubsection[Notazione Theta-grande]{Notazione \(\Theta\)-grande}

\textbf{Definizione}: siano \(f(n),\ g(n)\) funzioni \(\mathbb{N} \rightarrow \mathbb{R}^+\).
Allora \(\Theta(g(n))\) è l'insieme di funzioni definito come:

\[ \Theta(g(n)) = \left\{ f(n) \, | \, \exists \, c_1 > 0, c_2 > 0, n_0 > 0 \text{ tali che } \forall \, n > n_0, \ 0 \leq c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n)  \right\} \]

\bigskip
\textit{Informalmente}, le funzioni in \(\Theta(g(n))\) \textit{sono comprese} tra  \(c_1 \cdot g(n)\) e \(c_2 \cdot g(n)\) a partire da \(n_0\).
Un'illustrazione di questa relazione è rappresentata in Figura~\ref{fig:notazione-theta-grande}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.5]{image-22.tikz}
  \caption{Notazione \(\Theta\)-grande}
  \label{fig:notazione-theta-grande}
  \bigskip
\end{figure}

\paragraph{Definzioni come limiti}

Se è vero che:
\[ \displaystyle \lim_{n \rightarrow \infty} \dfrac{f(n)}{g(n)} = c, \quad c \neq 0, \quad c \neq \infty \]
allora \(f(n) \in \Theta(g(n))\) e gli andamenti asintotici di \(f,\ g\) differiscono per una costante moltiplicativa.

\bigskip
Allo stesso modo, se è vero che:
\[ \displaystyle \lim_{n \rightarrow \infty} \dfrac{f(n)}{g(n)} = 0 \]
allora \(f(n) \in \bigO(g(n)), \ f(n) \notin \Theta(g(n))\) e \(f\) cresce più velocemente di \(g\).

Alternativamente si usa la notazione \(\Theta(f(n)) < \Theta(g(n))\).

\subsubsection[Proprietà di O, Omega, Theta]{Proprietà di \(\bigO,\ \Omega,\ \Theta\)}

\begin{itemize}
  \item Per definizione:
        \begin{itemize}
          \item \(f \in \Theta(g) \Leftrightarrow f \in \bigO(g) \lor f \in \Omega(g)\)
        \end{itemize}
  \item Proprietà \textbf{transitiva}
        \begin{itemize}
          \item se \(f(n) = \Theta(g(n)),\ g(n) = \Theta(h(n)) \Rightarrow f(n) = \Theta(h(n))\)
          \item se \(f(n) = \bigO(g(n)),\ g(n) = \bigO(h(n)) \Rightarrow f(n) = \bigO(h(n))\)
          \item se \(f(n) = \Omega(g(n)),\ g(n) = \Omega(h(n)) \Rightarrow f(n) = \Omega(h(n))\)
        \end{itemize}
  \item Proprietà \textbf{riflessiva}
        \begin{itemize}
          \item \(f(n) = \Theta(f(n))\)
          \item \(f(n) = \bigO(f(n))\)
          \item \(f(n) = \Omega(f(n))\)
        \end{itemize}
  \item \textbf{Simmetria}
        \begin{itemize}
          \item \(f(n) = \Theta(g(n)) \Leftrightarrow g(n) = \Theta(f(n))\)
        \end{itemize}
  \item \textbf{Simmetria trasposta}
        \begin{itemize}
          \item \(f(n) = \bigO(g(n)) \Leftrightarrow g(n) = \Omega(f(n))\)
        \end{itemize}
\end{itemize}

\subsubsection{Complessità dei modelli deterministici di calcolo}

Si vuole ora studiare la complessità dei modelli \textbf{deterministici} di calcolo studiati fin'ora.
Il risultato della trattazione è illustrato in Tabella~\ref{tab:complessita-modelli-calcolo}.

\begin{table}[htbp]
  \bigskip
  \centering
  \begin{tabular}{c|c|c|c}
    \textit{modello}              & \textit{complessità spaziale} & \textit{complessità temporale} & \textit{note}                               \\ \hline
    \FSA                          & \(S(n) \in \Theta(1)\)        & \(T(n) \in \Theta(n)\)         & \footnotesize più precisamente \(T(n) = n\) \\
    \PDA                          & \(S(n) \in \bigO(n)\)         & \(T(n) \in \Theta(n)\)                                                       \\
    \TM \textit{a nastro singolo} & \(S(n) \in \Theta(n)\)        & \(T(n) \in \Theta(n^2)\)       & \footnotesize meno efficienti di \PDA       \\
  \end{tabular}
  \bigskip
  \caption{Complessità dei modelli di calcolo}
  \label{tab:complessita-modelli-calcolo}
\end{table}

\subsection{Teorema di accelerazione lineare}

Quattro enunciati analoghi del Teorema di accelerazione lineare sono mostrati nelle seguenti Sezioni, seguiti ai relativi risvolti pratici.

\subsubsection[Enunciato 1]{Enunciato \(1\)}
\label{sec:enunciato-accelerazione-1}

\indentquote{Se \(L\) è accettato da una \TM \(M\) a \(k\) nastri in \(S_M(n)\), per ogni \(c \in \mathbb{R}^+\) è possibile costruire una \TM \(M^\prime\) a \(k\) nastri che accetta \(L\) con \(S_{M^\prime}(n) < c \cdot S_M(n)\)}

\bigskip
Il Enunciato mostra che è possibile avere una accelerazione lineare \textit{(da qua il nome)} per un certo fattore \(c \in \mathbb{R}^+\) se \(0 < c < 1\).

\paragraph[Dimostrazione Enunciato 1]{Dimostrazione Enunciato \(1\)}
\label{par:dimostrazione-enunciato-accelerazione-1}

\textit{Schema della dimostrazione:}
\begin{enumerate}
  \item Si sceglie una fattore di compressione \(r\) tale che \(r \cdot c > 2\)
  \item Per ogni alfabeto \(\Gamma_i\) dell'\(i\)-esimo nastro di \(M\) si costruisce \(\Gamma^\prime_i\) dell'\(i\)-esimo nastro di \(M^\prime\) assegnando un elemento per ogni \(s \in \Gamma^r_i\)
  \item Si costruisce l'organo di controllo di \(M^\prime\) in modo tale per cui:
        \begin{itemize}
          \item calcoli i nuovi simboli sui nastri emulando le mosse di \(M\) spostando le testine sui nastri ogni \(r\) movimenti di \(M\)
          \item memorizzi la posizione della testina arricchendo ulteriormente gli alfabeti di nastro \(\Gamma_i\) \textbf{oppure} arricchendo l'insieme degli stati
        \end{itemize}
\end{enumerate}

\subsubsection[Enunciato 2]{Enunciato \(2\)}
\label{sec:enunciato-accelerazione-2}

\indentquote{Se \(L\) è accettato da una \TM \(M\) a \(k\) nastri in \(S_M(n)\), è possibile costruire una \TM \(M^\prime\) a \(1\) nastro che accetta \(L\) con \(S_{M^\prime}(n) = S_M(n)\), concatenando tutti i nastri su uno solo}

\subsubsection[Enunciato 3]{Enunciato \(3\)}

\indentquote{Se \(L\) è accettato da una \TM \(M\) a \(k\) nastri in \(S_M(n)\), per ogni \(c \in \mathbb{R}^+\) è possibile costruire una \TM \(M^\prime\) a \(1\) nastro che accetta \(L\) con \(S_{M^\prime}(n) < c \cdot S_M(n)\)}

Il risultato è analogo a quello del \nameref{sec:enunciato-accelerazione-2}, con in aggiunta la compressione già vista nel \nameref{sec:enunciato-accelerazione-1}.

\subsubsection[Enunciato 4]{Enunciato \(4\)}

\indentquote{Se \(L\) è accettato a una \TM \(M\) a \(k\) nastri in \(T_M(n)\), per ogni \(c \in \mathbb{R}^+\), è possibile costruire una \TM \(M^\prime\) a \(k+1\) nastri che accetta \(L\) con \(T_{M^\prime}(n) = \max(n + 1,\ c \cdot T_M(n))\)}

Il risultato è analogo a quello del \nameref{sec:enunciato-accelerazione-2}, con in aggiunta la compressione già vista nel \nameref{sec:enunciato-accelerazione-1}.

\paragraph[Dimostrazione Enunciato 4]{Dimostrazione Enunciato \(4\)}

L'approccio della dimostrazione è analogo a quello già visto in \ref{par:dimostrazione-enunciato-accelerazione-1}

\begin{enumerate}
  \item Si codifica in modo compresso i simboli dell'alfabeto di \(M\)
  \item Poiché la compressione avviene a \textit{runtime} il caso ottimale è \(T_{M^\prime}(n) = n\)
  \item Comprimendo \(r\) simboli in \(1\) nel caso pessimo servono \(3\) mosse di \(M^\prime\) per emularne \(r+1\) di \(M\)
\end{enumerate}

\subsubsection{Conseguenze pratiche}

Questi teoremi portano alle seguenti conseguenze pratiche:

\begin{itemize}
  \item Lo schema di dimostrazione usato per le \TM vale anche per il modello di calcolatore di Von Neumann
        \begin{itemize}[label=\(\rightarrow\)]
          \item È possibile avere accelerazioni lineari arbitrariamente grandi \textbf{aumentando il parallelismo fisico}
        \end{itemize}
  \item Miglioramenti più che lineari nel tempo di calcolo possono essere ottenuti solo \textbf{cambiando algoritmo}
        \begin{itemize}[label=\(\rightarrow\)]
          \item concepire ed utilizzare algoritmi \textbf{efficienti} è molto più efficiente di procedere di \textit{forza bruta}
        \end{itemize}
  \item Un calcolatore è in grado di eseguire operazioni aritmetiche su tipi a dimensione finita in tempo costante, mentre la \TM richiede di propagare gli effetti al singolo \texttt{bit}, uno alla volta
        \begin{itemize}[label=\(\rightarrow\)]
          \item il calcolatore opera su un alfabeto più vasto, di dimensione \(|I|=2^w\), con \(w\) dimensione della parola
          \item un calcolatore può accedere direttamente ad una cella di memoria, mentre una \TM impiega \(\Theta(n)\), con \(n\) pari alla distanza di quest'ultima dalla testina
        \end{itemize}
\end{itemize}

\subsection{Macchina \RAM}

L'introduzione della macchina \RAM consente di fare un passo per arrivare ad un livello di astrazione più vicino alla realtà.
È infatti un modello classico, ispirato all'architettura di \textit{Von Neumann}.

La \textbf{macchina \RAM} è dotata di un nastro di \textbf{lettura} (\texttt{IN}) e uno di \textbf{scrittura} (\texttt{OUT}), simili a quelli di cui può essere disposta una \TM.
Il programma è cablato all'interno dell'organo di controllo tramite una serie \textit{(finita)} di istruzioni in un certo linguaggio.
Esse \textbf{non sono alterabili} durante il funzionamento della macchina.
L'indice dell'istruzione da eseguire in ogni dato momento è indicato dal \textit{program counter}.

L'accesso alla memoria avviene tramite indirizzamento diretto, in cui ogni cella (contenente un intero) viene letta o scritta dalla \textit{unità aritmetica}. Ogni cella di memoria è quindi associata ad un intero e definita come \(\texttt{N}[n],\ n \in \mathbb{N}\).
Grazie alla sua struttura, l'accesso ad una cella di memoria non implica uno scorrimento delle stesse.
Le istruzioni del programma caricato nell'organo di controllo usano come primo operando sorgente o destinazione il primo elemento della memoria, \(\texttt{N}[0]\), detto \textbf{accumulatore}.

\bigskip
La struttura di una macchina \RAM è mostrata nella Figura~\ref{fig:struttura-macchina-ram}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-23.tikz}
  \caption{Struttura di una macchina \RAM}
  \label{fig:struttura-macchina-ram}
  \bigskip
\end{figure}

\bigskip

I programmi all'interno dell'unità di controllo sono codificati tramite una semantica simile al codice \texttt{Assembly}, seppur di molto semplificato.
Viene definita una specifica dei comandi (un \textit{Instruction Set}) che permette operazioni matematiche, logiche e di salto.

Un'esempio di tale linguaggio è mostrato nella Tabella~\ref{tab:instruction-set-macchina-RAM}.

\begin{table}[htbp]
  \bigskip
  \centering
  \begin{minipage}[t]{.3\textwidth}
    \scalebox{0.8}{
      \centering
      \begin{tabular}[t]{ll}
        \textit{istruzione} & \textit{semantica}                                                       \\ \hline
        \texttt{LOAD X}     & \(\texttt{N}[0] \leftarrow \texttt{N}[\texttt{X}]\)                      \\
        \texttt{LOAD= X}    & \(\texttt{N}[0] \leftarrow \texttt{X}\)                                  \\
        \texttt{LOAD* X}    & \(\texttt{N}[0] \leftarrow \texttt{N}[\texttt{N}[\texttt{X}]]\)          \\
        \texttt{STORE X}    & \(\texttt{N}[\texttt{X}] \leftarrow \texttt{N}[0]\)                      \\
        \texttt{STORE* X}   & \(\texttt{N}[\texttt{N}[\texttt{X}]] \leftarrow \texttt{N}[0]\)          \\
        \texttt{ADD X}      & \(\texttt{N}[0] \leftarrow \texttt{N}[0] + \texttt{N[\texttt{X}]}\)      \\
        \texttt{SUB X}      & \(\texttt{N}[0] \leftarrow \texttt{N}[0] - \texttt{N[\texttt{X}]}\)      \\
        \texttt{MUL X}      & \(\texttt{N}[0] \leftarrow \texttt{N}[0] \times \texttt{N[\texttt{X}]}\) \\
        \texttt{DIV X}      & \(\texttt{N}[0] \leftarrow \texttt{N}[0] \div \texttt{N[\texttt{X}]}\)   \\
      \end{tabular}
    }
  \end{minipage}
  \begin{minipage}[t]{.3\textwidth}
    \scalebox{0.8}{
      \centering
      \begin{tabular}[t]{ll}
        \textit{istruzione} & \textit{semantica}                                                                   \\ \hline
        \texttt{ADD= X}     & \(\texttt{N}[0] \leftarrow \texttt{N}[0] + \texttt{X}\)                              \\
        \texttt{ADD* X}     & \(\texttt{N}[0] \leftarrow \texttt{N}[0] + \texttt{N}[\texttt{N}[\texttt{X}]]\)      \\
        \texttt{SUB= X}     & \(\texttt{N}[0] \leftarrow \texttt{N}[0] - \texttt{X}\)                              \\
        \texttt{SUB* X}     & \(\texttt{N}[0] \leftarrow \texttt{N}[0] - \texttt{N}[\texttt{N}[\texttt{X}]]\)      \\
        \texttt{MULT= X}    & \(\texttt{N}[0] \leftarrow \texttt{N}[0] \times \texttt{X}\)                         \\
        \texttt{MULT* X}    & \(\texttt{N}[0] \leftarrow \texttt{N}[0] \times \texttt{N}[\texttt{N}[\texttt{X}]]\) \\
        \texttt{DIV= X}     & \(\texttt{N}[0] \leftarrow \texttt{N}[0] \div \texttt{X}\)                           \\
        \texttt{DIV* X}     & \(\texttt{N}[0] \leftarrow \texttt{N}[0] \div \texttt{N}[\texttt{N}[\texttt{X}]]\)   \\
      \end{tabular}
    }
  \end{minipage}
  \begin{minipage}[t]{.3\textwidth}
    \scalebox{0.8}{
      \centering
      \begin{tabular}[t]{ll}
        \textit{istruzione} & \textit{semantica}                                                  \\ \hline
        \texttt{READ X}     & \(\texttt{N}[\texttt{X}] \leftarrow \texttt{IN}\)                   \\
        \texttt{READ* X}    & \(\texttt{N}[\texttt{N}[\texttt{X}]] \leftarrow \texttt{IN}\)       \\
        \texttt{WRITE X}    & \(\texttt{OUT} \leftarrow \texttt{N}[\texttt{X}]\)                  \\
        \texttt{WRITE= X}   & \(\texttt{OUT} \leftarrow \texttt{X}\)                              \\
        \texttt{WRITE* X}   & \(\texttt{OUT} \leftarrow \texttt{N}[\texttt{N}[\texttt{X}]]\)      \\
        \texttt{JUMP L}     & \(\texttt{PC} \leftarrow \texttt{L}\)                               \\
        \texttt{JZ L}       & \(\texttt{PC} \leftarrow \texttt{L} \text{ se } \texttt{N}[0] = 0\) \\
        \texttt{JGZ L}      & \(\texttt{PC} \leftarrow \texttt{L} \text{ se } \texttt{N}[0] > 0\) \\
        \texttt{JLZ L}      & \(\texttt{PC} \leftarrow \texttt{L} \text{ se } \texttt{N}[0] < 0\) \\
        \texttt{HALT}       & \texttt{-}                                                          \\
      \end{tabular}
    }
  \end{minipage}
  \bigskip
  \caption{Instruction set della macchina \RAM}
  \label{tab:instruction-set-macchina-RAM}
\end{table}

\subsubsection{Criterio di costo logaritmico}
\label{sec:criterio-costo-logaritmico}

L'approssimazione di costo tramite \(\Omega\), \(\Theta\) e \(\bigO\) è corretta ma presenta molti \textbf{limiti}.

Infatti non si tiene conto di una limitatezza della memoria: una singola parola di memoria non può sempre contenere in un solo simboli tutti gli interi usati in un determinato algoritmo.
Poiché non si considera il numero di cifre necessarie a rappresentare un numero, il calcolo della complessità può risultare molto errato in certe circostanze.
Questo criterio di costo prende il nome di \textbf{costante} e si contrappone al criterio di costo \textbf{logaritmico}.

\bigskip
Per effettuare una approssimazione più simile alla realtà, è necessario tenere conto del numero di cifre necessarie a rappresentare i numeri su cui si opera.
Di conseguenza, al variare dell'ordine di grandezza degli operandi, potrà essere più o meno complicato eseguire istruzioni matematiche.
Analogamente, le operazioni di accesso alla memoria \textit{(\texttt{LOAD} e \texttt{STORE})} avranno complessità variabile in funzione della lunghezza dei parametri.

Il criterio prende il nome di \textbf{logaritmico} perché per rappresentare un numero decimale \(x\) in base \(b\) serviranno un numero di simboli pari a \(\lceil \log_b(x) \rceil\).

La complessità logaritmica delle operazioni elementari della macchina \RAM è mostrato nell'elenco seguente, diviso per ogni categoria di operazione.

\begin{minipage}[htbp]{.99\textwidth}
  \bigskip
  \begin{itemize}
    \item Il costo delle operazioni \textbf{aritmetiche} e \textbf{logiche} elementari dipende dall'operazione. Ponendo \(d = \log_2(i)\):
          \begin{itemize}[label=\(\rightarrow\)]
            \item \textbf{lettura} e \textbf{scrittura} (\texttt{READ}, \texttt{WRITE}) ha costo \(\Theta(d)\)
            \item \textbf{operazioni di memoria} (\texttt{LOAD}, \texttt{SAVE}) ha costo \(\Theta(d)\)
            \item \textbf{addizioni} e \textbf{sottrazioni} (\texttt{ADD}, \texttt{SUB}) ha costo \(\Theta(d)\)
            \item \textbf{divisioni} e \textbf{moltiplicazioni} (\texttt{MUL}, \texttt{DIV}) con \textbf{metodo scolastico} ha costo \(\Theta(d^2)\)
                  \begin{itemize}[label=\(\rightarrow\)]
                    \item usando \textbf{algoritmi più efficienti} ha costo \(\Theta\left(d \log{(d)}\right)\)
                  \end{itemize}

          \end{itemize}
    \item Il costo delle operazioni di \textbf{salto} (\texttt{JZ}, \texttt{JGZ}, \texttt{JLZ}) sarà costante:
          \begin{itemize}[label=\(\rightarrow\)]
            \item sono tutte \(\Theta(1)\)
            \item viene sommato un \textit{offset} di lunghezza fissa al \textit{program counter} \textit{(anch'esso di lunghezza fissa)}
          \end{itemize}
  \end{itemize}
\end{minipage}

\subsubsection{Scelta del criterio di costo}

La scelta del criterio di costo (tra logaritmico o costante) dipende da una serie di fattori.
Infatti, se:

\begin{itemize}
  \item L'elaborazione \textbf{non altera l'ordine di grandezza} dei dati di ingresso
  \item La memoria allocata inizialmente \textbf{non varia durante l'esecuzione} del programma
        \begin{itemize}
          \item la memoria \textbf{non dipende} dai dati
          \item di conseguenza \textbf{una singola cella è elementare} e con essa le operazioni relative
          \item la dimensione di ogni singolo elemento in ingresso \textbf{non varia significativamente} nel tempo
        \end{itemize}
\end{itemize}

allora il \textbf{criterio di costo costante} è adeguato.
In caso contrario è indispensabile il criterio di costo logaritmico, l'unico in grado di approssimare con correttezza il costo della funzione.

\bigskip
La conseguenza immediata di questa differenza di complessità implica che con macchine diverse lo stesso algoritmo può dare luogo a complessità diverse.
Non è possibile identificare un modello \inlinequote{migliore} tra i vari formalismi studiati e non esiste un'analogo alla Tesi di Church (Sezione~\ref{sec:tesi-di-church}), però è possibile stabilire una relazione di maggiorazione a priori tra le complessità dei diversi modelli di calcolo.
Essa prende il nome di \textit{\nameref{sec:tesi-correlazione-polinomiale}}.

\paragraph{Relazione tra i criteri di costo}

La relazione tra i due criteri di costo \textbf{non è fissa}.
Infatti, non è possibile sempre affermare che sia valida la relazione:
\[ C_{\text{log}} = C_{\text{cost}} \log{(n)} \]
Questa uguaglianza sarà tuttavia valida nei casi in cui \(C_{\text{cost}} = n\) o \(C_{\text{cost}} = 1\).

%TODO pagine 420 libro - esempio 9.15

\subsection{Tesi di correlazione polinomiale}
\label{sec:tesi-correlazione-polinomiale}

La Tesi di correlazione polinomiale, in analogia con la Tesi di Church, prova una relazione tra le complessità di diverse modelli di automi.
Essa afferma che:

\indentquote{Se un problema è risolvibile mediante il modello \(\mathscr{M}_1\) con complessità spaziale o temporale \(C_1(n)\), allora è risolvibile da un qualsiasi altro modello Turing completo \(\mathscr{M}_2\) con complessità \(C_2(n) \leq \pi \left(C_1(n)\right),\text{ con } \pi \text{ polinomio }\)}

\subsubsection{Dimostrazione della Tesi di correlazione polinomiale}

L'obiettivo di questa sezione sarà dimostrare la Tesi di correlazione polinomiale \textit{(che d'ora in poi prenderà quindi la definizione di Teorema)}.

Essa sarà strutturata in due parti, che mostreranno rispettivamente la simulazione di una \TM tramite \RAM e viceversa.

\paragraph[Simulazione di una \TM a k nastri tramite \RAM]{Simulazione di una \TM a \(k\) nastri tramite \RAM}

Verrà ora dimostrato il funzionamento della simulazione delle azioni una \TM a \(k\) nastri tramite macchina \RAM.

Inizialmente, sarà necessario compiere questi passi per inizializzare la memoria della \RAM

\begin{itemize}
  \item La \TM viene \textbf{mappata} sulla \RAM:
        \begin{itemize}
          \item lo stato della \TM verrà posto sulla prima cella di memoria della \RAM
          \item verrà usata una cella di memoria della \RAM per ogni cella del nastro
          \item la memoria rimanente viene divisa in blocchi da \(4\) celle
        \end{itemize}
  \item I blocchi vengono \textbf{riempiti} con la seguente strategia:
        \begin{itemize}
          \item Il blocco \(0\) conterrà le posizioni delle \(k\) testine
          \item I blocchi \(n\)-esimi, \(n > 0\), conterranno l'\(n\)-esimo simbolo di ognuno dei \(k\) nastri
        \end{itemize}
  \item La \RAM emula la lettura di un carattere sotto la testina con un accesso indiretto
        \begin{itemize}
          \item viene usato l'indice contenuto nel blocco \(0\)
        \end{itemize}
\end{itemize}

Una volta inizializzata la memoria, sarà possibile simulare propriamente le azioni della \TM.
In particolare:

\begin{itemize}
  \item \textbf{Lettura} dei nastri
        \begin{itemize}
          \item lettura del blocco \(0\) e dello stato: \(\Theta(k)\) \textit{mosse}
          \item lettura dei valori sui nastri in corrispondenza delle testine: \(\Theta(k)\) \textit{accessi indiretti}
        \end{itemize}
  \item \textbf{Scrittura} dei nastri
        \begin{itemize}
          \item scrittura dello stato: \(\Theta(1)\)
          \item scrittura della celle dei nastri: \(\Theta(k)\) \textit{accessi indiretti}
          \item scrittura nel blocco \(0\) per aggiornare le posizioni delle \(k\) testine: (\(\Theta(k)\) \textit{mosse}
        \end{itemize}
\end{itemize}

Per valutare la complessità, bisogna considerare che la \RAM emula una mossa della \TM con \(\Theta(k)\) mosse:

\begin{itemize}
  \item \(T_{RAM}(n) = \Theta\left(T_{\mathscr{M}}(n)\right)\) per il costo \textbf{costante}
  \item \(T_{RAM}(n) = \Theta\left(T_{\mathscr{M}}(n)\right) \log\left(T_{\mathscr{M}}(n)\right)\) per il costo \textbf{logaritmico}
\end{itemize}

\paragraph[Simulazione di una \RAM nastri tramite \TM a k nastri]{Simulazione di una \RAM tramite \TM a \(k\) nastri}

In questo paragrafo verrà dimostrata la simulazione di una \RAM tramite \TM a \(k\) nastri, omettendo le operazioni \texttt{MUL} e \texttt{DIV} per semplicità.

\begin{itemize}
  \item Un nastro di memoria della \TM viene \textbf{inizializzato} come in Figura~\ref{fig:inizializzazione-memoria-ram}
        \begin{itemize}
          \item le celle di memoria indicizzate da \(i\) sono state coinvolte da almeno una operazione di \texttt{STORE}
          \item il simbolo \(\$\) è usato come delimitatore tra indice della cella e contenuto della cella
          \item il simbolo \(\#\) è usato per separare tra di loro le celle di memoria
        \end{itemize}
  \item Un secondo nastro di memoria ospita il contenuto dell'\textbf{accumulatore} \(\texttt{N}[0]\)
        \begin{itemize}
          \item il valore viene memorizzato con codifica \textbf{binaria}
        \end{itemize}
  \item Un terzo nastro viene utilizzato come memoria \textbf{temporanea}
        \begin{itemize}
          \item viene usato per spostare i dati qualora sia necessario memorizzare \(\texttt{N}[i_j]\)
                \begin{itemize}
                  \item deve valere \(\texttt{N}[i_k],\ \texttt{N}[i_l],\ i_k < i_j < i_l\)
                \end{itemize}
        \end{itemize}
\end{itemize}

\begin{figure}
  \bigskip
  \centering
  \tikzfig{image-24.tikz}
  \caption{Inizializzazione della memoria \RAM}
  \label{fig:inizializzazione-memoria-ram}
  \bigskip
\end{figure}

Una volta inizializzata la memoria, sarà possibile simulare propriamente le azioni della macchina \RAM.

In particolare:

\begin{itemize}
  \item L'operazione \texttt{LOAD x} avverrà come:
        \begin{enumerate}
          \item ricerca di \texttt{x} sul nastro principiale
          \item copia  di dati letti sulla cella corrispondente a \(\texttt{N}[0]\) usando il nastro di supporto
        \end{enumerate}
  \item L'operazione \texttt{STORE x} avverrà come:
        \begin{enumerate}
          \item ricerca di \texttt{x} sul nastro principiale
          \item se esso non viene trovato, è necessario creare dello spazio usando il nastro di servizio \textit{(se necessario) }
          \item valore di \(\texttt{N}[0]\) viene salvato in \(\texttt{N}[\texttt{x}]\)
        \end{enumerate}
  \item L'operazione \texttt{ADD x} avverrà come:
        \begin{itemize}
          \item ricerca di \texttt{x}
          \item copia di \(\texttt{N}[x]\) sul nastro di supporto
          \item calcolo della somma
          \item scrittura del valore risultante in \(\texttt{N}[0]\)
        \end{itemize}
\end{itemize}

In generale, la simulazione di una mossa della \RAM richiede alla \TM un numero di mosse \(n\) inferiore alla lunghezza del nastro principale per una costante opportuna.
In simboli:
\[ n \leq c \cdot \text{ lunghezza nastro principale } \]

Di conseguenza:
\[ T_{\RAM}  = \bigO(T_{\mathscr{M}}) \]

\subsubsection{Lemma della Tesi di correlazione polinomiale}

In seguito all'enunciazione e alla dimostrazione del Teorema di correlazione polinomiale, risulta che:

\indentquote{Lo spazio occupato sul nastro principale è \(\bigO \left(T_\text{RAM}(n)\right)\)}

\paragraph{Dimostrazione del Lemma}

\begin{itemize}
  \item Ogni cella \(i_j\)-esima della \RAM occupa \(\log{(i_j)} + \log{(\texttt{N}[i_j])}\)
  \item Ogni cella della \RAM viene materializzata solo se la \RAM effettua una operazione di \texttt{STORE}
  \item L'operazione \texttt{STORE} ha un costo per la \RAM pari a \(\log{(i_j)} + \log{(\texttt{N}[i_j])}\)
  \item Per riempire \(r\) celle, la \RAM impiega \(\displaystyle \sum_{j=1}^{r} \log{(i_j)} + \log{(\texttt{N}[i_j])}\) unità di tempo
        \begin{itemize}[label=\(\rightarrow\)]
          \item la stessa quantità di spazio occupata sul nastro
        \end{itemize}
\end{itemize}

\subsubsection{Conclusioni sulla Tesi di correlazione}

A valle delle dimostrazioni, si può concludere che:

\begin{itemize}
  \item La \TM impiega \textbf{al più} \(\Theta(T_\text{RAM}(n))\) per simulare \textbf{una mossa} della \RAM
  \item Se la \RAM ha \textbf{complessità} \(T_\text{RAM}(n)\), essa effettua \textbf{al più} \(T_\text{RAM}(n)\) mosse (ogni mossa costa almeno \(1\))
  \item La simulazione completa della \RAM da parte della \TM costa \textbf{al più} \(\Theta\left((T_\text{RAM}(n))^2\right)\)
  \item Il legame tra \(T_\text{RAM}(n)\) e \(T_\text{TM}(n)\) è \textbf{polinomiale}
\end{itemize}

\bigskip
Infine, è possibile fare le seguenti \textit{osservazioni}:

\begin{itemize}
  \item Per quanto grande possa essere il grado di un polinomio, sarà sempre inferiore ad una funzione esponenziale
        \begin{itemize}[label=\(\rightarrow\)]
          \item è sufficiente confrontare l'andamento delle funzioni \(n^k\) e \(2^n\)
        \end{itemize}
  \item Grazie al teorema di correlazione polinomiale è possibile parlare dei problemi risolvibili in tempo e/o spazio polinomiale astraendo dalla classe del calcolatore
  \item \textit{classe dei problemi trattabili in pratica} \(\equiv\) \textit{classe dei problemi risolvibili in tempo polinomiale}
        \begin{itemize}
          \item questa classe prende il nome di \Pset
          \item i problemi di interesse pratico che sono in \Pset hanno anche grado del polinomio \inlinequote{accettabile}
        \end{itemize}
\end{itemize}

\clearpage

\section{Algoritmi}

L'obiettivo di questa sezione sarà introdurre i concetti di base dei grafi e degli algoritmi, per poi iniziare ad analizzare alcuni problemi di ordinamento.
Una analisi più approfondita dei grafi verrà fatta più avanti \textit{(nella Sezione~\ref{sec:grafi})}.

\subsection{Introduzione ai Grafi}

Un grafo è una coppia \(G = \langle V, E \rangle\) in cui:

\begin{itemize}
  \item \(V\) è un insieme finito di \textbf{nodi} (detti anche \textbf{vertici})
  \item \(E \subseteq V \times V\) è una relazione binaria \textit{(costituente un insieme finito)} su \(V\) di coppie di nodi
        \begin{itemize}
          \item gli elementi della relazione sono detti \textbf{archi}
          \item se \(u\) e \(v\) sono nodi del grafo, la coppia \(\langle u, v \rangle\) è un arco.
        \end{itemize}
\end{itemize}

L'arco è detto \textbf{orientato} se \(u\) e \(v\) sono coppie di nodi ordinati, ovvero se esiste una relazione di ordinamento tra i due \textit{(Figura~\ref{fig:grafo-arco-orientato})}.
In caso contrario, è detto \textbf{non orientato} \textit{(Figura~\ref{fig:grafo-arco-non-orientato})}.
In questi ultimi l'ordine dei vertici negli archi non è rilevante.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{subfigure}[t]{0.495\textwidth}
    \centering
    \begin{tikzpicture}[auto, on grid, node distance=3cm, >=Triangle]
      \node [state, minimum size=1cm](u) {\textit{u}};
      \node [state, minimum size=1cm](v) [right=of u] {\textit{v}};

      \path[->, thick]
      (u) edge [] node {} (v);

    \end{tikzpicture}
    \caption{Grafo con arco orientato}
    \label{fig:grafo-arco-orientato}
  \end{subfigure}
  \begin{subfigure}[t]{0.495\textwidth}
    \centering
    \begin{tikzpicture}[auto, on grid, node distance=3cm, >=Triangle]
      \node [state, minimum size=1cm](u) {\textit{u}};
      \node [state, minimum size=1cm](v) [right=of u] {\textit{v}};

      \path[-, thick]
      (u) edge [] node {} (v);

    \end{tikzpicture}
    \caption{Grafo con arco non orientato}
    \label{fig:grafo-arco-non-orientato}
  \end{subfigure}
  \caption{Grafo orientato e grafo non orientato}
  \label{fig:grafo-orientato-non-orientato}
  \bigskip
\end{figure}

\bigskip
\textbf{Proprietà} dei grafi:

\begin{itemize}
  \item Se tutti gli archi di un grafo sono \textbf{orientati}, allora il grafo è detto \textbf{orientato}
        \begin{itemize}
          \item in caso contrario, è detto \textbf{non orientato}
        \end{itemize}
  \item Un \textbf{cammino} è una sequenza di nodi \(v_0,\,c v_1,\,c \ldots,\,c v_n\) tali che tra ogni coppia di nodi della sequenza \(\langle v_i, v_{i+1} \rangle\) esiste un arco
        \begin{itemize}
          \item i nodi \(v_0,\,c v_1,\,c \ldots,\,c v_n\) appartengono tutti al cammino
          \item la lunghezza del cammino è data da \(n\) \textit{(il numero di vertici \(-1\))}
        \end{itemize}
  \item In un grafo \textbf{non orientato}, il cammino forma un \textbf{ciclo} se \(v_0 = v_n\)
        \begin{itemize}
          \item un grafo che non contiene cicli è detto \textbf{aciclico}
        \end{itemize}
  \item Un grafo \textbf{non orientato} è connesso se tra ogni coppia di vertici esiste un cammino
  \item Un grafo \textbf{non orientato} e \textbf{aciclico} prende il nome di \textit{DAG}, o \textit{directed acyclic graph}
  \item \(|V|\) indica il numero di \textbf{vertici}, \(|E|\) denota il numero di \textbf{archi}
        \begin{itemize}
          \item tra i due intercorre la relazione \(0 \leq |E| \leq |V|^2\)
        \end{itemize}
  \item un grafo è detto \textbf{denso} se \(|E| \approx |V|^2\) (il numero di lati è prossimo al numero massimo)
        \begin{itemize}
          \item in caso contrario è detto \textbf{sparso}
        \end{itemize}
\end{itemize}

\subsubsection{Alberi}
\label{sec:alberi}

Un \textbf{albero} è un grafo connesso, aciclico e non orientato.
È detto \textbf{radicato} se un nodo viene indicato come \textbf{radice}.

Ogni nodo del grafico è \textbf{raggiungibile} dalla radice tramite un \textbf{cammino}.
Esso sarà \textbf{unico} in quanto il grafo è aciclico.
Gli ultimi nodi dei cammini dalla radice sono detti \textbf{foglie}.
Viene detta \textbf{altezza} dell'albero la distanza massima tra la radice e una sua foglia.

Un albero è detto \textbf{completo} se sono valide le seguenti proprietà:

\begin{itemize}
  \item Tutte le \textbf{foglie} hanno la stessa \textbf{profondità} \textit{(distanza dalla radice)} \(h\)
  \item Tutti i \textbf{nodi interni} hanno esattamente \(2\) figli.
\end{itemize}

\bigskip
Ogni \textbf{nodo} ha un \textbf{padre} \textit{(tranne la radice)} e uno o più \textbf{figli} \textit{(tranne le foglie)}.
Più precisamente vengono chiamati:

\begin{itemize}
  \item \textbf{Nodi interni}: tutti i nodi dei cammini tra la radice e le foglie
  \item \textbf{Profondità} di un nodo \(N\): la \textbf{distanza} di \(N\) dalla radice
  \item \textbf{Antenato} di un nodo \(N\):  ogni nodo che \textbf{precede} \(N\) sul cammino dalla radice a \(N\)
  \item \textbf{Padre} di un nodo \(N\): il nodo che \textbf{precede} immediatamente \(N\) lungo il cammino dalla radice a \(N\)
  \item \textbf{Figlio} di un nodo \(N\): ogni nodo che \textbf{succede} immediatamente \(N\) lungo il cammino dalla radice a \(N\)
  \item \textbf{Fratelli} di un nodo \(N\): ogni nodo che ha lo stesso padre di \(N\)
\end{itemize}

Infine un albero è detto \textbf{binario} se ogni nodo ha \textbf{al più} \(2\) figli.

\bigskip
Un esempio di albero è mostrato in Figura~\ref{fig:esempio-di-albero}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-26.tikz}
  \caption{Esempio di albero \textit{non binario}}
  \label{fig:esempio-di-albero}
  \bigskip
\end{figure}

\paragraph{Lemma di altezza dell'albero binario}
\label{par:lemma-altezza-albero-binario}

Il Lemma di altezza dell'albero binario recita che:

\indentquote{Ogni albero binario di altezza \(h\) ha un numero di foglie al più pari a \(2^h\).}

La dimostrazione è banale e avviene per induzione.

\subsubsection{Heap}
\label{sec:heap}

Uno \textbf{heap binario} è un albero binario detto \textbf{quasi completo}.
Infatti tutti i suoi livelli sono completi \textit{(su ogni livello ogni nodo ha due figli)}, tranne l'ultimo che potrebbe essere completo solo fino ad un certo punto da sinistra.

Un \textbf{max-heap} è uno heap tale che, per ogni nodo \(x\) dell'albero, il valore contenuto nel padre di \(x\) è \(\geq\) del contenuto di \(x\).
Considerando i singoli nodi, \(A\left(\left\lfloor\sfrac{i}{2}\right\rfloor\right) \geq A[i]\).
Nei \textbf{min-heap}, al contrario, il valore contenuto nel padre di \(x\) è \(\leq\) del contenuto di \(x\).

\bigskip
È sempre possibile costruire e rappresentare un heap tramite un \textbf{array}.
L'albero binario che deriva da questa interpretazione è quasi completo.

\bigskip
Un esempio di \textbf{max-heap} e la sua corrispondenza con un array è mostrata nella Figura~\ref{fig:esempio-di-max-heap}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-27.tikz}
  \caption{Esempio di max heap}
  \label{fig:esempio-di-max-heap}
  \bigskip
\end{figure}

\subsection{Introduzione agli Algoritmi}

\textit{Informalmente}, un \textbf{algoritmo} è una qualsiasi procedura ben definita che prende un valore \textit{(o un insieme di valori)} in \textbf{input} e restituisce un valore \textit{(o un insieme di valori)} in \textbf{output}, calcolato tramite una serie ben definita di operazioni.
L'algoritmo può essere visto anche come uno strumento per risolvere un particolare problema di computazione.

In particolare, si definiscono:

\begin{itemize}
  \item \textbf{Problema}: il compito da svolgere
        \begin{itemize}
          \item quali \textit{output} si vuole ottenere a fronte di certi \textit{input}
          \item quali \textit{operazioni} devono essere effettuate (la funzione che deve svolgere)
        \end{itemize}
  \item \textbf{Algoritmo}: i passi da eseguire per risolvere un problema
        \begin{itemize}
          \item prende gli \textit{input} in ingresso ad un problema e li trasforma in opportuni \textit{output}
          \item è formato da una sequenza di \textit{operazioni concrete}
          \item è eseguibile da una \textbf{macchina}
          \item è sempre \textbf{corretto}
        \end{itemize}
\end{itemize}

Gli algoritmi possono essere descritti in diversi linguaggi \textit{(ad esempio, in \texttt{C}, \texttt{Java}, \texttt{Python}, \ldots)}.
La complessità di un algoritmo non varia significativamente in base al linguaggio in cui è implementato.
Infatti essa potrà differire solo di un fattore moltiplicativo.
All'interno del corso si userà lo \textbf{pseudocodice}, che assomiglia nella sintassi ad un linguaggio vero e proprio senza tuttavia esserlo.

\subsubsection{Pseudocodice}

La sintassi dello \textbf{pseudocodice} è la seguente:

\begin{itemize}
  \item \textbf{Assegnamento}: \texttt{i := j}
        \begin{itemize}
          \item è ammesso l'assegnamento multiplo: \texttt{i := j := e}
          \item le variabili sono locali alla procedura
        \end{itemize}
  \item \textbf{Cicli} e \textbf{salti} condizionali: \texttt{while}, \texttt{for}, \texttt{if-then-else}
        \begin{itemize}
          \item la struttura a blocchi è determinata dall'indentazione
          \item non si usano le parentesi
          \item nei cicli \textbf{\texttt{for}} il valore dell'indice termina al valore massimo \textbf{incluso}
                \begin{itemize}
                  \item il valore indicato da \textbf{\texttt{to}} sarà uguale al valore massimo dell'indice
                \end{itemize}
        \end{itemize}
  \item \textbf{Commenti}:
        \begin{itemize}
          \item si indicano con la notazione \texttt{// commento}
          \item sono sempre a riga singola
        \end{itemize}
  \item \textbf{Array}: la notazione è analoga a quella del \texttt{C}
        \begin{itemize}
          \item il primo elemento può avere indice diverso da \(0\)
                \begin{itemize}
                  \item normalmente il primo elemento dell'array \texttt{A} è \texttt{1}
                \end{itemize}
          \item \texttt{A[j]} è l'elemento \(j\) dell'array \texttt{A}
          \item \texttt{A[i..j]} è il sotto array che inizia dall'elemento \(i\) e termina all'elemento \(j\)
        \end{itemize}
  \item Gli \textbf{oggetti} sono strutture che raggruppano dati composti
        \begin{itemize}
          \item possiedono degli attributi, anche detti campi
          \item per accedere all'attributo \texttt{attr} dell'oggetto \texttt{x}, si usa \texttt{x.attr}
          \item gli \textbf{array} sono dati composti, quindi sono assimilabili ad oggetti
                \begin{itemize}[label=\(\rightarrow\)]
                  \item ogni array ha un attributo \texttt{length} che indica la lunghezza dell'array
                \end{itemize}
        \end{itemize}
  \item Una \textbf{variabile} che corrisponde ad un oggetto è un puntatore all'oggetto
        \begin{itemize}
          \item analogo a quanto avviene in \texttt{Java}
          \item un puntatore che non fa riferimento ad un oggetto ha valore \texttt{NIL}
        \end{itemize}
  \item Il \textbf{passaggio dei parametri} avviene per valore
        \begin{itemize}
          \item la procedura invocata ottiene una copia dei parametri passati
          \item passando un oggetto come parametro alla procedura, si passa il puntatore all'oggetto
        \end{itemize}
\end{itemize}

\bigskip
Per la valutazione della complessità di algoritmi scritti in pseudocodice si userà il \textbf{criterio di costo costante} \textit{(Sezione~\ref{sec:criterio-costo-logaritmico})}, poiché non verranno manipolati dati più grandi rispetto a quelli in ingresso.
Come conseguenza diretta, ogni istruzione \texttt{i} di pseudocodice verrà eseguita in un tempo costante \(c_\texttt{i}\)
Grazie a questa assunzione sarà possibile trascurare il modello di calcolo della macchina che esegue lo pseudocodice \textit{(normalmente la macchina \RAM)}.

La complessità temporale sarà studiata con più dovizia di quella spaziale perché più rilevante nella realtà.
Infatti, mentre la memoria ha un costo limitato, il tempo è una risorsa più significativa e costosa ai fini della risoluzione di un determinato problema.

\subsubsection{Divide et Impera}
\label{sec:divide-et-impera}

La tecnica chiamata \textbf{Divide et Impera} \textit{(dall'Inglese Divide and Conquer)} è una tecnica algoritmica molto comune.
Essa si applica a problemi grossi e difficili da risolvere e si articola nei seguenti passi:

\begin{enumerate}
  \item \textbf{Divide} - divisione del problema in problemi più piccoli da risolvere
  \item \textbf{Impera} - risoluzione dei problemi più piccoli
  \item \textbf{Combina} - le soluzioni vengono combinate al fine di ottenere il risultato finale
\end{enumerate}

Dividendo a sufficienza i problemi, inevitabilmente si ottengono sotto problemi più facili, risolubili senza che essi debbano essere divisi ulteriormente.
Questa è una tecnica di natura ricorsiva, in quanto i sotto problemi sono risolti tramite l'algoritmo stesso.

\bigskip
In generale, un algoritmo \textbf{divide et impera} presenta le seguenti caratteristiche:

\begin{itemize}
  \item Il problema viene \textbf{diviso} \(a\) in sotto problemi, ognuno di dimensione \(\sfrac{1}{b}\) di quello originale
  \item Se il sotto problema ha dimensione \(n\) sufficientemente piccola \textit{(tale per cui \(n < c\) con \(c\) costante)} allora esso può essere \textbf{risolto} in tempo costante \textit{(quindi in \(\Theta(1)\))}
  \item Indicando con \(D(n)\) e \(C(n)\) i costi di divisione e ricombinazione, è possibile esprimere il costo totale \(T(n)\) come:
        \[ T(n) = \begin{cases} \Theta(1)                                 & \quad \text{ se } n < c   \\
              D(n) + aT\left(\sfrac{n}{b}\right) + C(n) & \quad \text{ altrimenti }
          \end{cases}\]
\end{itemize}

\subsubsection{Ricorrenze}

Una \textbf{ricorrenza} è una caratteristica delle funzioni ricorsive, ossia quelle funzioni caratterizzate da:

\begin{itemize}
  \item uno o più \textbf{casi base}
  \item una o più \textbf{chiamate} a sé stessa, con eventuali parametri più piccoli
\end{itemize}

Esistono \(3\) tecniche principali per il risolvimento delle ricorrenze:

\begin{enumerate}
  \item \textbf{Sostituzione}
  \item \textbf{Albero di ricorsione}
  \item \textbf{Teorema dell'Esperto} (dall'Inglese \textit{Master Theorem})
\end{enumerate}

Tutte queste tecniche verranno analizzate più nel dettaglio nelle seguenti Sezioni.

\subsubsection{Metodo di sostituzione}

Il metodo di sostituzione si articola in \(2\) passi:

\begin{enumerate}
  \item \textbf{Ipotizzare} una soluzione
  \item \textbf{Trovare le costanti} e dimostrare che la soluzione è corretta tramite induzione
\end{enumerate}

Può essere usato per trovare il limite superiore o il limite inferiore di una ricorrenza.

Questo metodo porta ad un risultato esatto, ma può essere applicato solo laddove è inizialmente facile ipotizzare una soluzione.

\subsubsection{Metodo dell'albero di ricorsione}

Il metodo dell'albero di ricorsione fornisce un risultato approssimato la cui validità va provata con altre tecniche \textit{(normalmente per sostituzione)}.
Questo metodo, infatti, semplifica di molto il primo passo del metodo di sostituzione, ossia l'ipotesi della soluzione.

\begin{enumerate}
  \item L'albero di ricorsione viene disegnato
        \begin{itemize}[label=\(\rightarrow\)]
          \item ad ogni livello la somma della dimensione di ciascun nodo è pari alla dimensione della radice
        \end{itemize}
  \item La complessità di ciascun livello viene sommata per trovare la complessità totale
\end{enumerate}

\bigskip
Questo tecnica è molto facile da applicare nei problemi di tipo \textit{Divide et Impera}, come sarà mostrato nel prossimo Paragrafo.

\paragraph{Albero di ricorsione e tecnica \textit{Divide et Impera}}

Come annunciato precedentemente, questa tecnica si adatta particolarmente agli algoritmi che adottano la tecnica \textit{Divide et Impera}.

Infatti, se una ricorrenza è nella forma
\[ T(n) = T(a) + T(b),\quad a + b = 1,\ a \geq b \]
il corrispondente albero delle ricorrenze si esaurirà al livello \(k\), in corrispondenza del quale il parametro \(a\) avrà valore \textbf{unitario}.

Ciò avverrà quando \(n \cdot a^k = 1\), quindi per \(k = \log_{\sfrac{1}{a}}{(n)}\).
La complessità risultante andrà calcolata come la somma del costo del livello, che essendo composto da parametri unitari, è pari ad \(n\):
\[ T(n) = n \log_{\sfrac{1}{a}}{(n)} \]
Di conseguenza:
\[ T(n) = \bigO(n \log{(n)})  \]

Questo risultato va sempre verificato con il metodo di sostituzione.

\subsubsection{Teorema dell'Esperto}
\label{sec:teorema-dell-esperto}

Il \textbf{Teorema dell'Esperto} fornisce un metodo \inlinequote{da manuale} per risolvere le ricorrenze nella forma:
\[ T(n) = aT(\sfrac{n}{b}) + f(n)\]

Il teorema richiede di memorizzare tre casi, ma permette di trovare una soluzione alla ricorrenza con relativa semplicità.

\bigskip
\textit{Formalmente}, siano \(a \geq 1,\ b > 1\) due costanti e \(f(n)\) una funzione dal comportamento asintotico positivo.
A partire da essi è possibile definire \(T(n),\ n \in \mathbb{N}\) tramite la ricorrenza:
\[ T(n) = aT(\sfrac{n}{b}) + f(n) , \quad a \geq 1,\ b > 1 \]
Il valore di \(\sfrac{n}{b}\) è considerato arrotondato per difetto o per eccesso (quindi rispettivamente \(\lfloor\sfrac{n}{b}\rfloor\) o \(\lceil\sfrac{n}{b}\rceil\)).

\(T(n)\) può essere limitato asintoticamente come segue:

\begin{enumerate}[label=\arabic*), ref=(\arabic*)]
  \item\label{enum:teorema-esperto-1} Se \(f(n) = \bigO\left(n^{\log_b(a) - \epsilon}\right)\) per una costante \(\epsilon > 0\), allora \(T(n) = \Theta\left(n^{\log_b (a)}\right)\)
  \item\label{enum:teorema-esperto-2} Se \(f(n) = \Theta\left(n^{\log_b(a)}\right)\), allora \(T(n) = \Theta\left(n^{\log_b(a)} \log{(n)}\right)\)
  \item\label{enum:teorema-esperto-3} Se \(f(n) = \Omega\left(n^{\log_b(a) + \epsilon}\right)\) per una costante \(\epsilon > 0\) e se \(a \cdot f(\sfrac{n}{b}) \leq c \cdot f(n)\) per una costante \(c < 1\) \textit{(condizione di regolarità)} e dei valori di \(n\) sufficientemente grandi, allora \(T(n) = \Theta\left(f(n)\right)\)
\end{enumerate}

\bigskip
\textit{Informalmente}, il teorema porta a confrontare \(f(n)\) con la funzione \(n^{\log_b(a)}\).

Se, come nel caso \ref{enum:teorema-esperto-1}, la funzione \(n^{\log_b(a)}\) è la più grande delle due, allora la soluzione è \(T(n) = \Theta\left(n^{\log_b (a)}\right)\).
Se, invece, è \(f(n)\) ad essere la più grande, come nel caso \ref{enum:teorema-esperto-3}, allora la soluzione è \(T(n) = \Theta\left(f(n)\right)\).
Infine, se come nel caso \ref{enum:teorema-esperto-2}, le due funzioni sono della stessa dimensione, moltiplicando per un fattore logaritmico risulta che la soluzione è \(T(n) = \Theta\left(n^{\log_b(a)} \log{(n)}\right) = \Theta\left(f(n) \log{(n)}\right)\).

\bigskip
Prima di poter applicare il teorema, bisogna tenere conto di alcuni dettagli non \textit{ovvi}.

Nel primo caso, \ref{enum:teorema-esperto-1}, non solo \(f(n)\) deve essere più piccola di \(n^{\log_b(a)}\), ma deve essere \textbf{polinomialmente più piccola}.
Essa sarà, asintoticamente, più piccola di un fattore \(n^\epsilon,\ \epsilon>0\).

Nel terzo caso, \ref{enum:teorema-esperto-3}, non solo \(f(n)\) deve essere più grande di \(n^{\log_b(a)}\), ma deve essere \textbf{polinomialmente più grande} e deve soddisfare la condizione di \inlinequote{regolarità} che impone \(a \cdot f(\sfrac{n}{b}) \leq c \cdot f(n)\).
Questa condizione è soddisfatta dalla maggior parte delle funzioni polinomialmente limitate che possono essere incontrate.

\bigskip
E importante realizzare che i tre casi del teorema \textbf{non coprono tutti i possibili comportamenti} asintotici di \(f(n)\).
C'è infatti un \inlinequote{vuoto} tra i casi \ref{enum:teorema-esperto-1} e \ref{enum:teorema-esperto-2} quando \(f(n)\) è più piccola di \(n^{\log_b(a)}\) ma non \textbf{polinomialmente} più piccola.
In modo analogo, la stessa problematica si presenta tra i casi \ref{enum:teorema-esperto-2} e \ref{enum:teorema-esperto-3} quando \(f(n)\) è più grande di \(n^{\log_b(a)}\), ma non \textbf{polinomialmente} più grande.
Se la funzione \(f(n)\) cade in uno di questi \inlinequote{vuoti}, o se la condizione di regolarità del caso \ref{enum:teorema-esperto-3} non è valida, il teorema del maestro \textbf{non è applicabile}.

\paragraph{Confronto polinomiale}

Per cercare di chiarire il concetto di polinomialmente \inlinequote{più piccolo} e \inlinequote{più grande}, verranno ora esposti degli \textit{esempi}.

\begin{itemize}
  \item \(n\) è polinomialmente \textbf{più piccolo} di \(n^2\)
  \item \(n \log (n)\) è polinomialmente \textbf{più grande} di \(n^{1/2}\)
  \item \(n \log (n)\) è più grande di \(n\) ma \textbf{non polinomialmente}
\end{itemize}

\bigskip
In generale, una funzione \(f(n)\) è polinomialmente \textbf{più grande} di una funzione \(g(n)\) se vale la relazione:
\[\dfrac{f(n)}{g(n)} = \textit{ polinomio di grado } \geq 1 \]

\paragraph{Casi particolari}

Il Teorema dell'Esperto si semplifica notevolmente qualora \(f(n)\) è una funzione \(\Theta(n^k)\), con \(k\) costante:

\begin{enumerate}[label=\arabic*), ref=(\arabic*)]
  \item\label{enum:caso-particolare-teorema-esperto-1} Se \(k < \log_b(a)\), allora \(T(n) = \Theta\left(n^{\log_b(a)}\right)\)
  \item\label{enum:caso-particolare-teorema-esperto-2} Se \(k = \log_b(a)\), allora \(T(n) = \Theta\left(n^k \log{(n)}\right)\)
  \item\label{enum:caso-particolare-teorema-esperto-3} Se \(k > \log_b(a)\), allora \(T(n) = \Theta\left(n^k\right)\)
\end{enumerate}

Nel caso \ref{enum:caso-particolare-teorema-esperto-3} la condizione aggiuntiva di regolarità è automaticamente verificata.

\bigskip
Un'altro caso particolare è identificato nel caso in cui la ricorrenza abbia forma
\[T(n) = \begin{cases}
    \Theta(1) \quad                                                           & \text{se } n \leq m \leq h \\
    \displaystyle \sum_{1 \leq i \leq h} a_i \cdot T(n-i) + c \cdot n^k \quad & \text{altrimenti}
  \end{cases}\]
in cui i coefficienti \(a_i \in \mathbb{N} \, \forall \, i\).

\bigskip
Ponendo \(a = \displaystyle \sum_{i \leq i \leq h} a_i\), se:

\[\begin{cases}
    a = 1  \ \Rightarrow \      & T(n) = \bigO(n^{k+1}) \\
    a \geq 2  \  \Rightarrow \  & T(n) = \bigO(a^n n^k)
  \end{cases}\]

\subsection{Algoritmi di supporto}

\subsubsection{Operazioni sugli heap}

\begin{center}
  \begin{minipage}{0.31\textwidth}
    \centering
    \begin{lstlisting}[style=pseudocode, numbers=none]
PARENT(i):
  return floor(i / 2)
\end{lstlisting}
  \end{minipage}
  \begin{minipage}{0.3\textwidth}
    \begin{lstlisting}[style=pseudocode, xleftmargin=20pt, xrightmargin=20pt, numbers=none]
LEFT(i):
  return 2 * i
\end{lstlisting}
  \end{minipage}
  \begin{minipage}{0.3\textwidth}
    \begin{lstlisting}[style=pseudocode, numbers=none]
RIGHT(i):
  return 2 * i + 1
\end{lstlisting}
  \end{minipage}
\end{center}

Ogni array \texttt{A} che rappresenta uno heap ha \(2\) attributi:

\begin{itemize}
  \item \texttt{A.length}, rappresentante il numero totale di elementi dell'array
  \item \texttt{A.heap-size}, rappresentante il numero totale di elementi dell'heap
        \begin{itemize}
          \item \texttt{A.heap-size} \(\leq\) \texttt{A.length} \(\forall \, \texttt{A}\)
          \item solo gli elementi fino a \texttt{A.heap-size} hanno le proprietà dello heap
          \item l'array potrebbe però contenere elementi dopo l'indice \texttt{A.heap-size}
        \end{itemize}
\end{itemize}

\subsubsection{Algoritmo \texttt{MAX-HEAPIFY}}
\label{sec:algoritmo-max-heapify}

L'algoritmo di \texttt{MAX-HEAPIFY} è una funzione che ordina i nodi di un \textbf{heap}, trasformandolo in un \textbf{max-heap}.
La proprietà caratteristica dei \textit{max-heap} è che il valore di ogni nodo dell'albero è sempre maggiore o uguale di quello dei figli.

L'algoritmo è di tipo ricorsivo ed è illustrato nel Listato~\ref{lst:costruzione-max-heap}.

\begin{lstlisting}[style=pseudocode, caption={Costruzione di un \texttt{MAX-HEAP}}, label={lst:costruzione-max-heap}]
MAX-HEAPIFY(A, i)
  l := LEFT(i)
  r := RIGHT(i)
  if l < A.heap-size and A[l] > A[i]
    max := l
  else
    max := i
  if r < A.heap-size and A[r] > A[max]
    max := r
  if max != i
    swap A[i], A[max]
    MAX-HEAPIFY(A, max)
\end{lstlisting}

\paragraph{Analisi dell'algoritmo \texttt{MAX-HEAPIFY}}

La complessità temporale di \texttt{MAX-HEAPIFY} è pari a
\[ T = \bigO(h) = \bigO\left(\log{(n)}\right) \]
dove \(h\) corrisponde all'altezza dell'albero, che essendo quasi completo è inferiore a \(\log{(n)}\).

\bigskip
Sarebbe stato possibile giungere alla stessa soluzione usando il Teorema dell'Esperto per la ricorrenza
\[ T(n) = T\left(\dfrac{2n}{3}\right) + \Theta(1) \]
che rappresenta il tempo di esecuzione di \texttt{MAX-HEAPIFY} nel caso pessimo.

In questo caso, infatti, l'ultimo livello dell'albero è pieno esattamente a metà.
L'algoritmo viene applicato ricorsivamente sul sotto albero sinistro (contenente \(\leq \sfrac{2n}{3}\), con \(n\) nodi totali).

\subsubsection{Algoritmo \texttt{BUILD-MAX-HEAP}}
\label{sec:algoritmo-build-max-heap}

L'algoritmo \texttt{BUILD-MAX-HEAP} permette di costruire un \textbf{max-heap} partendo da un \textbf{array}.
La costruzione avviene \textit{bottom-up}, partendo dalle foglie fino alla radice.
Lo pseudocodice dell'algoritmo è mostrato nel Listato~\ref{lst:algoritmo-build-max-heap}.

Funziona grazie a due proprietà dello heap:

\begin{itemize}
  \item Tutti gli elementi oltre l'indice \texttt{A.length / 2} sono delle \textbf{foglie}, gli altri sono dei \textbf{nodi interni}
  \item I sotto alberi fatti di solo foglie sono max-heap in quanto costituiti da \textbf{un solo elemento}
\end{itemize}

\begin{lstlisting}[style=pseudocode, caption={Algoritmo \texttt{BUILD-MAX-HEAP}}, label={lst:algoritmo-build-max-heap}]
BUILD-MAX-HEAP(A)
  A.heap-size := A.length
  for i := A.length / 2 downto 1
    MAX-HEAPIFY(A, i)
\end{lstlisting}

\paragraph{Analisi dell'algoritmo \texttt{BUILD-MAX-HEAP}}

Il costo dell'algoritmo \texttt{BUILD-MAX-HEAP}, a priori, potrebbe risultare pari a \(\bigO\left(n \log{(n)}\right)\).
Tuttavia, questo limite non fornisce una precisione sufficiente.

Infatti, si osserva che:

\begin{itemize}
  \item L'altezza di un albero quasi completo di \(n\) nodi è \(\left\lfloor \log_2(n) \right\rfloor\)
  \item Definendo come \inlinequote{altezza di un nodo di uno heap} la lunghezza del cammino più lungo che porta ad una foglia, il costo dell'algoritmo invocato su un nodo di altezza \(h\) è \(\bigO(h)\)
  \item Il numero massimo di nodi di altezza \(h\) di uno heap è \(\left\lceil \dfrac{n}{2^{h+1}} \right\rceil\) volte ad ogni altezza \(h\)
\end{itemize}

Grazie a queste proprietà, il costo di computazione dell'algoritmo \texttt{BUILD-MAX-HEAP} è pari a:
\[ \displaystyle \sum_{n=0}^{\left\lfloor \log_2(n) \right\rfloor} \left[ \dfrac{n}{n^{h+1}} \right] \bigO(h) = \bigO \left(n \displaystyle \sum_{n=0}^{\left\lfloor \log_2(n) \right\rfloor} \dfrac{h}{2^h} \right) = \bigO (n)\]

Il passo finale della soluzione deriva dall'uguaglianza
\[ \sum_{h=0}^{\left\lfloor \log_2(n) \right\rfloor} \dfrac{1}{2^h} = \dfrac{1}{1 - \sfrac{1}{2}} \]
come sommatoria di una serie geometrica.

\subsection{Problemi di ordinamento}

L'\textbf{ordinamento} degli elementi di una sequenza è un problema molto comune e fornisce un classico esempio di problema risolvibile tramite algoritmi.
Esistono parecchi algoritmi di ordinamento \textit{(insertion sort, selection sort, bubble sort, \ldots)}, ognuno con la propria complessità.

\bigskip
Nelle prossime Sezioni alcuni di essi verranno analizzati in dettaglio.

\subsubsection{\texttt{INSERTION-SORT}}

L'algoritmo \texttt{INSERTION-SORT} è uno dei più semplici algoritmi di ordinamento.

\textit{Idea} dell'algoritmo:
per ogni elemento \(x\) di una sequenza \(A\) di elementi, partendo dalla prima posizione, inserisce l'elemento viene prelevato e l'array viene scorso fino a quando non viene trovata una posizione valida in cui inserire \(x\).

L'array viene quindi ordinato senza sul posto, senza bisogno di spazio di memoria ausiliario.

  \begin{minipage}{0.45\textwidth}
    \begin{lstlisting}[style=pseudocode, label={Pseudocodice dell'algoritmo di Insertion Sort}, label={lst:pseudocodice-insertion-sort}]
INSERTION-SORT(A)
  for j := 2 to A.length
    key := A[j]
    i := j - 1
    while i > 0 and A[i] > key
      A[i + 1] := A[i]
      i := i - 1
    A[i + 1] := key
    \end{lstlisting}
  \end{minipage}
  \begin{minipage}{0.45\textwidth}
    \centering
    \begin{tabular}{ccc}
      \textit{riga} & \textit{costo} & \textit{ripetizioni}     \\ \hline
      \(2\)         & \(c_1\)        & \(n\)                    \\
      \(3\)         & \(c_2\)        & \(n-1\)                  \\
      \(4\)         & \(c_3\)        & \(n-1\)                  \\
      \(5\)         & \(c_4\)        & \(\sum_{j=2}^n t_j\)     \\
      \(6\)         & \(c_5\)        & \(\sum_{j=2}^n (t_j-1)\) \\
      \(7\)         & \(c_6\)        & \(\sum_{j=2}^n (t_j-1)\) \\
      \(8\)         & \(c_7\)        & \(n-1\)                  \\
    \end{tabular}
  \end{minipage}


\paragraph{Analisi dell'algoritmo \texttt{INSERTION-SORT}}

Il tempo di esecuzione di \texttt{INSERTION-SORT(A)} è:

\[ T(n) = c_1 \cdot n + c_2 \cdot (n-1) + c_3 \cdot (n-1) + c_4 \cdot \displaystyle \sum_{j=2}^n t_j + c_5 \cdot \displaystyle \sum_{h=2}^n (t_j - 1) + c_6 \cdot \displaystyle \sum_{h=2}^n (t_j - 1) + c_7 \cdot (n-1) \]

È quindi possibile identificare i casi ottimi e pessimi per poter calcolare l'effettiva complessità temporale:

\begin{itemize}
  \item Caso \textbf{ottimo}:
        \begin{itemize}
          \item gli elementi sono già ordinati, \(t_2 = \ldots = t_n = 1\)
          \item \(T(n) \approx an + b \ \Rightarrow \  T(n) = \Theta(n)\)
          \item si può anche affermare che \(T(n) = \Omega(n)\) perché il limite inferiore è funzione di \(\Theta(n)\)
        \end{itemize}
  \item Caso \textbf{pessimo}:
        \begin{itemize}
          \item gli elementi sono già ordinati ma in ordine decrescente \(t_2 = 2,\ t_3 = 3,\ \ldots\)
          \item \(T(n) \approx an^2 + bn + c \ \Rightarrow \ T(n) = \Theta(n^2)\)
          \item si può anche affermare che \(T(n) = \bigO(n^2)\) perché il limite superiore è funzione di \(\Theta(n^2)\)
        \end{itemize}
\end{itemize}

\subsubsection{\texttt{MERGE-SORT}}

\textit{Idea} dell'algoritmo:
Se l'array da ordinare ha meno di \(2\) elementi, allora è ordinato per definizione.
Altrimenti:
\begin{itemize}
  \item l'array viene diviso in \(2\) sotto array, ognuno con la metà degli elementi di quello originario
  \item i \(2\) sotto array vengono ordinati applicando di nuovo l'algoritmo
  \item grazie all'algoritmo \texttt{MERGE} i \(2\) sotto array vengono combinati
  \item gli elementi dell'array ottenuto sono ordinati
\end{itemize}

Il \texttt{MERGE-SORT} è un algoritmo ricorsivo e adotta la tecnica \textbf{divide ed impera}.
Per completare l'algoritmo, infatti, è necessario definire un sotto algoritmo \texttt{MERGE} che combina le soluzioni dei problemi divisi.

\textit{Idea} dell'algoritmo \texttt{MERGE}:

\begin{enumerate}
  \item \label{enum:begin-merge-loop} Partendo dall'inizio dei \(2\) sotto array, viene cercato il minimo dei \(2\) elementi correnti
  \item Il minimo viene inserito all'inizio dell'array da restituire
  \item L'inizio dell'array da cui è stato prelevato il minimo viene avanzato di \(1\)
  \item L'esecuzione riprende dal passo \ref{enum:begin-merge-loop}
\end{enumerate}

\begin{lstlisting}[style=pseudocode, caption={Pseudocodice dell'algoritmo \texttt{MERGE-SORT}}, label={lst:pseudocodice-merge-sort}]
MERGE-SORT(A, p, R)
  if p < r
    q := floor((p + r) / 2)
    MERGE-SORT(A, p, q)
    MERGE-SORT(A, q + 1, r)
    MERGE(A, p, q, r)
\end{lstlisting}

\begin{minipage}[t]{0.495\textwidth}
  \begin{lstlisting}[style=pseudocode, caption={Pseudocodice dell'algoritmo \texttt{MERGE}}, label={lst:pseudocodice-merge}]
MERGE(A, p, q, r)
  $\texttt{n}_{\texttt{1}}$ := q - p + 1
  $\texttt{n}_{\texttt{2}}$ := r - q
  alloca L[1 .. $\texttt{n}_{\texttt{1}}$ + 1]
  alloca R[1 .. $\texttt{n}_{\texttt{2}}$ + 1]
  for i := 1 to $\texttt{n}_{\texttt{1}}$
    L[i] := A[p + i - 1]
  for j := 1 to $\texttt{n}_{\texttt{2}}$
    R[j] := A[q + j]
  L[$\texttt{n}_{\texttt{1}}$ + 1] := $\infty$
  R[$\texttt{n}_{\texttt{2}}$ + 1] := $\infty$
  i := 1
  j := 1
  for k := p to r
    if L[i] <= R[j]
      A[k] := L[i]
      i := i + 1
    else
      A[k] := R[j]
      j := j + 1
  \end{lstlisting}
\end{minipage}
\begin{minipage}[t]{0.495\textwidth}
  \centering
  \begin{tabular}[t]{ccc}
    \textit{riga} & \textit{costo}                      & \textit{ripetizioni}        \\ \hline
    \(2\)         & \(c_1\)                             & \(1\)                       \\
    \(3\)         & \(c_2\)                             & \(1\)                       \\
    \(4\)         & \(\Theta(\texttt{n})\)              & \(1\)                       \\
    \(5\)         & \(\Theta(\texttt{n}_{\texttt{1}})\) & \(\texttt{n}_{\texttt{1}}\) \\
    \(8\)         & \(\Theta(\texttt{n}_{\texttt{2}})\) & \(\texttt{n}_{\texttt{2}}\) \\
    \(10\)        & \(c_3\)                             & 1                           \\
    \(11\)        & \(c_4\)                             & 1                           \\
    \(12\)        & \(c_5\)                             & 1                           \\
    \(13\)        & \(c_7\)                             & 1                           \\
    \(14\)        & \(\Theta(\texttt{n})\)              & \(\texttt{r} - \texttt{p}\) \\
    \(15\)        & \(c_8\)                             & 1                           \\
    \(16\)        & \(c_9\)                             & 1                           \\
    \(17\)        & \(c_{10}\)                          & 1                           \\
    \(19\)        & \(c_{11}\)                          & 1                           \\
    \(20\)        & \(c_{12}\)                          & 1                           \\
  \end{tabular}
\end{minipage}

\begin{itemize}
  \item \texttt{A} è l'array di \textbf{input} di dimensione \(n\) \textit{(da ordinare)}
  \item \texttt{p} è la posizione iniziale del primo elemento
  \item \texttt{r} è la posizione dell'ultimo elemento
\end{itemize}

\paragraph{Analisi dell'algoritmo \texttt{MERGE}}

Nell'algoritmo \texttt{MERGE} prima vengono copiati gli elementi dei \(2\) sotto array \texttt{A[p .. q]} e \texttt{A[q+1 .. r]} in \(2\) array temporanei \(L\) e \(R\) e poi vengono fusi in \texttt{A[p .. r]}.
Per non dover controllare se \(L\) e \(R\) sono vuoti si usa un valore particolare \((\infty)\), più grande di ogni possibile valore, in ultima posizione negli array.

La dimensione dei dati in input è \(\texttt{n} = \texttt{r} - \texttt{p} + 1\) ed è composto da \(3\) cicli \textbf{\texttt{for}}:

\begin{itemize}
  \item \(2\) cicli di inizializzazione (linee \(10\) e \(11\)) per assegnare i valori a \(L\) e \(R\)
        \begin{enumerate}
          \item il primo è eseguito \(n_1\) volte, con \(\Theta(n_1) = \Theta(\texttt{q}-\texttt{p}+1) = \Theta(\sfrac{n}{2}) = \Theta(n)\)
          \item il secondo è eseguito \(n_2\) volte, con \(\Theta(n_2) = \Theta(\texttt{r}-\texttt{q}) = \Theta(\sfrac{n}{2}) = \Theta(n)\)
        \end{enumerate}
\end{itemize}
Quindi combinando i due passi, \(T(n) = \Theta(n)\)

\paragraph{Analisi delle ricorrenze dell'algoritmo \texttt{MERGE-SORT}}

Siano:

\begin{itemize}
  \item \(a = b = 2\) perché il problema viene diviso in due sotto problemi di dimensione \textit{quasi} analoga
  \item \(D(n) = \Theta(1)\) perché la divisione del problema ha costo costante
  \item \(C(n) = \Theta(n)\) perché è necessario scorrere tutti gli elementi dei sotto array per poterli ricombinare
\end{itemize}

La ricorrenza totale corrisponderà quindi a:

\[T(n) = \begin{cases}
    \Theta(1)                                           & \quad \text{ se } n < 2   \\
    \Theta(1) + 2T\left(\dfrac{n}{2}\right) + \Theta(n) & \quad \text{ altrimenti }
  \end{cases}\]

Per essere più precisi, la ricorrenza dovrebbe essere \(T(\lfloor \sfrac{n}{2} \rfloor) + T(\lfloor \sfrac{n}{2} \rfloor)\) e non \(2T(\sfrac{n}{2})\), ma l'approssimazione è sufficiente perché non influisce sul comportamento asintotico della funzione.

\bigskip

Riscrivendo la ricorrenza dell'algoritmo senza fare uso delle notazioni \textit{Theta}, risulta che:

\[
  T(n) = \begin{cases}
    c                                & \quad \text{ se } n < 2  \\
    2T\left(\dfrac{n}{2}\right) + cn & \quad \text{ altrimenti}
  \end{cases}
\]

Per un appropriata costante \(c\).

La ricorrenza può essere visualizzata tramite un \textbf{albero di ricorsione}, scegliendo per semplicità il caso in cui \(n\) è una potenza di \(2\) \textit{(e quindi ogni ramo viene diviso in due sotto rami senza problemi di arrotondamento)}.
Una rappresentazione dell'albero di ricorsione è mostrata in Figura~\ref{fig:albero-ricorsione-algoritmo-merge-sort}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-25.tikz}
  \caption{Albero di ricorsione per l'algoritmo \texttt{MERGE-SORT} su un array di dimensione \(n = 2^k\).}
  \label{fig:albero-ricorsione-algoritmo-merge-sort}
  \bigskip
\end{figure}

Sommando i costi dei vari livelli si ottiene:

\[ T(n) = cn \log{(n)} + cn \quad \Rightarrow \quad T(n) = \Theta(n \log{(n)}) \]

\bigskip
Analogamente sarebbe stato possibile applicare il Teorema dell'Esperto, giungendo alla stessa soluzione.
Infatti:

\[ T(n) = 2T(\sfrac{n}{2}) + \Theta(n)\ \]

dove \(a = b = 2,\ f(n) = n\), come nel caso~\ref{enum:teorema-esperto-2} del Teorema.

Di conseguenza, \(n^{\log_b(a)} = n^1 = n\) e la complessità temporale è quindi:
\[ T(n) = \Theta(n \log{(n)}) \]

\subsubsection{\texttt{HEAPSORT}}
\label{sec:heap-sort}

L'algoritmo \texttt{HEAPSORT}, \textit{come suggerisce il nome}, sfrutta le proprietà degli heap \textit{(Sezione~\ref{sec:heap})} per ordinare gli elementi di un array.
Questo algoritmo farà uso degli algoritmi \textit{di supporto} \texttt{BUILD-MAX-HEAP} \textit{(Sezione~\ref{sec:algoritmo-build-max-heap})} e \texttt{MAX-HEAPIFY} \textit{(Sezione~\ref{sec:algoritmo-max-heapify})}.

\bigskip
\textit{Idea} dell'algoritmo:
Ad ogni ciclo viene preso l'elemento più grande \textit{(primo dell'array, in quanto max-heap)} in fondo alla parte di array ancora da ordinare \textit{(corrispondente allo heap).}
La dimensione dell'heap viene decrementata di \(1\) e viene ricostruito il max-heap \textit{(tramite \texttt{MAX-HEAPIFY})} usando come radice l'ultima foglia a destra dell'ultimo livello \textit{(l'ultimo elemento dell'array)}.

\begin{lstlisting}[style=pseudocode, caption={Pseudocodice dell'algoritmo \texttt{HEAPSORT}}, label={sec:algoritmo-heapsort}]
HEAPSORT(A)
  BUILD-MAX-HEAP(A)
  for i := A.length downto 2
    swap A[1], A[i]
    A.heap-size := A.heap-size - 1
    MAX-HEAPIFY(A, 1)
\end{lstlisting}

Dove \texttt{A} è l'array di \textbf{input} di dimensione \(n\) \textit{(da ordinare)}

\paragraph{Analisi dell'algoritmo \texttt{HEAPSORT}}

La complessità dell'algoritmo \texttt{HEAPSORT} è data dalla somma delle seguenti ricorrenze:

\begin{itemize}
  \item \texttt{BUILD-MAX-HEAP} è invocato una volta, con costo \(\bigO(n)\)
  \item \texttt{MAX-HEAPIFY} è invocato \(n-1\) volte, con costo \(\bigO\left(\log{(n)}\right)\)
\end{itemize}

Quindi la complessità totale sarà:
\[ T(n) = \bigO (n \log{(n)}) \]

\subsubsection{\texttt{QUICKSORT}}

L'algoritmo \texttt{QUICKSORT} è un algoritmo in stile \textit{\nameref{sec:divide-et-impera}}, che ordina sul posto.
Nel caso pessimo ha complessità \(\Theta(n^2)\), superiore all'\nameref{sec:heap-sort}, ma nel caso medio ha complessità \(\Theta\left(n \log{(n)}\right)\).

\textit{Idea} dell'algoritmo, dato un array \texttt{A[p..r]} da ordinare:

\begin{itemize}
  \item \textbf{Divide}: \texttt{A} viene diviso in \(2\) sotto array \texttt{A[p..q-1]} e \texttt{A[q+1..r]}
        \begin{itemize}
          \item \texttt{q} è il minimo elemento del sotto array \texttt{A[p..q-1]} (\texttt{A[n]} \(\leq\) \texttt{A[q]} \(\forall\) \texttt{n} \(\in [\texttt{p}, \texttt{q-1}]\))
          \item \texttt{q} è il massimo elemento del sotto array \texttt{A[q+1..r]} (\texttt{A[n]} \(\geq\) \texttt{A[q]} \(\forall\) \texttt{n} \(\in [\texttt{q+1}, \texttt{r}]\))
        \end{itemize}
  \item \textbf{Impera}: i sotto array \texttt{A[p...q-1]} e \texttt{A[q+1...r]} vengono ordinati utilizzando \texttt{QUICKSORT}
  \item \textbf{Combina}: l'array \texttt{A[p..r]} è già ordinato
\end{itemize}

La parte \inlinequote{complicata} da implementare riguarda la partizione: essa viene effettuata tramite l'algoritmo \texttt{PARTITION}, che avrà l'incarico di scegliere un elemento attorno al quale gli elementi ruoteranno, detto \texttt{pivot}.

Proprietà dell'elemento \texttt{pivot}:

\begin{itemize}
  \item Tutti gli elementi \textbf{minori} di \texttt{pivot} vengono posti nel sotto array \textbf{sinistro}
  \item Tutti gli elementi \textbf{maggiori} di \texttt{pivot} vengono posti nel sotto array \textbf{destro}
\end{itemize}

\begin{minipage}[t]{0.495\textwidth}
  \begin{lstlisting}[style=pseudocode, caption={Pseudocodice di \texttt{QUICKSORT}}, label={sec:algoritmo-quicksort}]
QUICKSORT(A, p, r):
  if p < r:
    q := PARTITION(A, p, r)
    QUICKSORT(A, p, q - 1)
    QUICKSORT(A, q + 1, r)
  \end{lstlisting}
\end{minipage}
\begin{minipage}[t]{0.495\textwidth}
  \begin{lstlisting}[style=pseudocode, caption={Pseudocodice di \texttt{PARTITION}}, label={sec:algoritmo-partition}]
PARTITION(A, p, r):
  x := A[r]
  i := p - 1
  for j := p to r - 1
    if A[j] <= x
        i := i + 1
        swap A[i], A[j]
  swap A[i + 1], A[r]
  return i + 1
  \end{lstlisting}
\end{minipage}

\begin{itemize}
  \item \texttt{A} è l'array di \textbf{input} di dimensione \(n\) \textit{(da ordinare)}
  \item \texttt{p} è la posizione iniziale del primo elemento
  \item \texttt{r} è la posizione dell'ultimo elemento
\end{itemize}

\paragraph{Analisi di \texttt{QUICKSORT}}

La complessità di \texttt{PARTITION} è \(\Theta(n),\ n = r - p + 1\).

\bigskip
Il tempo di esecuzione di \texttt{QUICKSORT} dipende da come viene partizionato l'array:

\begin{itemize}
  \item Se ogni volta uno dei \(2\) sotto array è vuoto mentre l'altro contiene \(n-1\) elementi si ha il caso \textbf{pessimo}:
        \begin{itemize}
          \item la ricorrenza è \(T(n) = T(n - 1) + \Theta(n)\)
          \item risolvendo per sostituzione o tramite il Teorema dell'Esperto dimostra che la soluzione è \(\Theta(n^2)\)
          \item questo caso si può verificare quando l'array è già ordinato in senso decrescente
        \end{itemize}
  \item Se ogni volta i due sotto array hanno una dimensione pari a \(\sfrac{n}{2}\) si ha il caso \textbf{ottimo}:
        \begin{itemize}
          \item la ricorrenza è \(T(n) = 2T(\sfrac{n}{2}) + \Theta(n)\)
          \item risolvendo si dimostra che la soluzione è \(\Theta(n \log{(n)})\)
        \end{itemize}
\end{itemize}

Se la proporzione di divisione fosse diversa da \(\sfrac{1}{2}\) e \(\sfrac{1}{2}\), la complessità rimarrebbe la stessa.

\bigskip
Il caso \textbf{medio} va analizzato in modo diverso.
Qualora le partizioni fossero bilanciate (e quindi i sotto array fossero di pari lunghezze \(\sfrac{n}{2}\)), infatti, la complessità sarebbe più bassa.

Più nel dettaglio:

\begin{itemize}
  \item Il costo di una divisione \textbf{bilanciata}: \(\Theta(n)\)
  \item Il costo di una divisione \textbf{non bilanciata}: \(\Theta(n)\)
\end{itemize}

Quindi il costo di una catena di divisioni è la stessa e quindi l'algoritmo sarà \(\Theta(n \log{(n)})\).

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-28.tikz}
  \caption{Partizione bilanciata e sbilanciata}
  \label{fig:partizione-bilanciata-sbilanciata}
  \bigskip
\end{figure}

\bigskip
Lo stesso risultato sarebbe stato raggiunto anche applicando il Teorema dell'Esperto.

Nel caso ottimo, la ricorrenza è \(T(n) \leq 2T(\sfrac{n}{2}) + \Theta(n)\).
La soluzione sarà quindi pari a: \[ T(n) = \bigO\left(n \log{(n)}\right) \]

Nel caso pessimo, \textit{in cui l'algoritmo produce una divisione \(9\) a \(1\)}, la ricorrenza è \(T(n) \leq T(\sfrac{9n}{10}) + T(\sfrac{n}{10}) + c \cdot n\).
La soluzione sarà quindi pari a: \[ T(n) = \bigO\left(n^2\right) \]

\subsubsection{\texttt{COUNTING-SORT}}

L'algoritmo \texttt{COUNTING-SORT} è un algoritmo di ordinamento che richiede che tutti i valori in ingresso \(a_0,\, \ldots,\, a_k\) siano tutti i numeri compresi tra \(0\) ed una certa costante \(k\).

\textit{Idea} di dell'algoritmo:
se nell'array ci sono \(m_e\) valori più piccoli di un certo elemento \(e\) di valore \(v_e\), nell'array ordinato quest'ultimo sarà in posizioni \(m_e + 1\).
Quindi per ordinare l'array basta contare quante copie dello stesso valore \(v_e\) sono presenti all'interno dello stesso: questa informazione viene usata per determinare, per ogni elemento \(e, 0 \leq v_e \leq k\), quanti elementi sono più piccoli di \(e\).
Infine bisogna tenere conto degli elementi ripetuti.

\begin{lstlisting}[style=pseudocode, caption={Pseudocodice algorimo \texttt{COUNTING-SORT}}, label={lst:counting-sort}]
COUNTING-SORT(A, B, K):
  for i := 0 to k
    C[i] := 0
  for j := 1 to A.length
    C[A[j]] := C[A[j]] + 1
    // ora C[i] contiene il numero di elementi = i
  for i := 1 to k
    C[i] := C[i] + C[i - 1]
    // ora C[i] contiene il numero di elementi <= i
  for j := A.length downto 1
    B[C[A[j]]] := A[j]
    C[A[j]] := C[A[j]] - 1
\end{lstlisting}

In cui i parametri sono:

\begin{itemize}
  \item \texttt{A} è l'array di \textbf{input} di dimensione \(n\) \textit{(da ordinare)}
  \item \texttt{B} è l'array di \textbf{output} di dimensione \(n\) \textit{(ordinato)}
  \item \texttt{k} è il valore massimo tra i valori di \texttt{A}
\end{itemize}

\paragraph{Analisi dell'algoritmo \texttt{COUNTING SORT}}

La complessità dell'algoritmo \texttt{COUNTING-SORT} è data dai \(4\) cicli \textbf{\texttt{for}}:

\begin{itemize}
  \item Il \textbf{\texttt{for}} del ciclo che inizia alla riga \(2\) ha complessità \(\Theta(k)\).
  \item Il \textbf{\texttt{for}} del ciclo che inizia alla riga \(4\) ha complessità \(\Theta(n)\).
  \item Il \textbf{\texttt{for}} del ciclo che inizia alla riga \(7\) ha complessità \(\Theta(k)\).
  \item Il \textbf{\texttt{for}} del ciclo che inizia alla riga \(10\) ha complessità \(\Theta(n)\).
\end{itemize}

La complessità totale sarà quindi \(\Theta(n+k)\).

Se \(k = \bigO(n)\), allora l'algoritmo \texttt{COUNTING-SORT} ha complessità \(\bigO(n)\) \textit{(lineare)}.
\texttt{COUNTING-SORT} è più veloce di \texttt{MERGE-SORT} e \texttt{HEAPSORT} perché fa delle forti assunzioni sulla distribuzione dei valori da ordinare \textit{(assumendo che ogni valore sia \(\leq k\))}.

In caso contrario, la complessità dell'algoritmo \texttt{COUNTING-SORT} sarà superiore a quella degli altri algoritmi.

\bigskip
Si noti che è possibile ottenere una versione semplificata dell'algoritmo senza fare uso dell'array \texttt{B}, perdendo tuttavia stabilità.
Se nell'array da ordinare ci fossero più elementi con lo stesso valore, questi appariranno nell'array ordinato mantenendo il loro ordine iniziale \textit{(proprietà garantita dall'uso dell'array \texttt{B})}.

Questa non è una proprietà particolarmente interessante nell'ordinare i numeri, ma ordinando oggetti tramite un loro attributo chiave.

\subsubsection{Limite inferiore della complessità nei problemi di ordinamento}

Nelle Sezioni precedenti sono stati introdotti ed analizzati svariati algoritmi di ordinamento che sono in grado di ordinare \(n\) numeri in tempo \(\bigO(n \log{(n)})\).
Questi algoritmi condividono una importante proprietà: \textit{l'ordine da essi determinato dipende solo da confronti tra gli elementi di input}.
Essi prendono quindi il nome di \textbf{algoritmi di ordinamento comparativi}.

Questi algoritmi possono essere visti in modo astratto tramite un \textbf{albero di decisione}, un albero binario che rappresenta i confronti tra gli elementi fatti da un determinato algoritmo di ordinamento con un certo input.
L'esecuzione dell'algoritmo corrisponde al tracciamento di un percorso dalla radice alla foglia contenente gli elementi in ordine.

Poiché ogni algoritmo deve poter creare le \(n!\) permutazioni, incluse quelle che possono apparire più volte, l'albero avrà un numero di foglie superiore a \(n!\).
Un esempio di albero di decisione, insieme alla sua notazione, è mostrato nella Figura~\ref{fig:albero-di-decisione-esempio}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-29.tikz}
  \caption{Albero di decisione. Un nodo con la notazione \(i : j\) indica la permutazione tra gli elementi \(i\) e \(j\). Il percorso verde mostra la decisione dell'algoritmo di ordinamento quando avendo come input \(\langle a_1 = 9, a_2 = 6, a_3  = 2\rangle\). La permutazione \(\langle 3, 2, 1 \rangle\) nella foglia verde indica l'ordinamento corretto. Poiché ci sono \(3\) elementi, ci saranno \(3! = 6\) foglie.}
  \label{fig:albero-di-decisione-esempio}
  \bigskip
\end{figure}

Poiché è possibile costruire un albero di decisione per ogni dato \(n\), tenendo conto del \nameref{par:lemma-altezza-albero-binario}, \textit{(Paragrafo~\ref{par:lemma-altezza-albero-binario})}, si ottiene il seguente teorema:

\indentquote{Ogni albero di decisione di ordinamento di \(n\) elementi ha altezza \(\Omega(n \log n)\).}

\paragraph{Dimostrazione del limite inferiore della complessità}

Sia \(f\) il numero di foglie dell'albero di decisione di ordinamento di \(n\) elementi.
Come visto prima, \(f \geq n!\).

Per il Lemma sarà valida la relazione:

\[ n! \leq f \leq 2^h \ \Rightarrow \ 2^h \geq n! \]

Quindi il limite inferiore della complessità è:

\[ h \geq \log{(n!)} \geq \log\left(\left(\sfrac{n}{2}\right)^{\sfrac{n}{2}}\right) = \dfrac{n}{2} \log\left(\dfrac{n}{2}\right) = \Omega(n \log{(n)} )\]

come volevasi dimostrare.

\bigskip
Alternativamente sarebbe stato possibile ottenere lo stesso risultato tramite l'approssimazione di Stirling:
\[ n! > \left(\dfrac{n}{e}\right)^n \quad \Rightarrow \quad h \geq \log\left(\left(\sfrac{n}{e}\right)^n\right) = n \log\left(\sfrac{n}{e}\right) = n \log{(n)} - n \log{(e)} = \Omega(n \log{(n)})\]

\clearpage

\section{Strutture dati}

Le strutture dati sono usate per \textbf{contenere oggetti}.
Essi rappresentano \textit{collezioni (o insiemi dinamici)} di oggetti.

Normalmente, gli oggetti di una struttura dati presentano:

\begin{itemize}
  \item Una \textbf{chiave}, usata per indicizzare l'oggetto
  \item Dei \textbf{dati satellite}, ognuno associato ad una delle chiavi
\end{itemize}

Sono definiti \(2\) tipi di operazioni sulle strutture dati:

\begin{itemize}
  \item operazioni che \textbf{modificano} la collezione
  \item operazioni che \textbf{interrogano} la collezione
\end{itemize}

\bigskip
Le operazioni più usate, in pseudocodice, sono:

\begin{itemize}
  \item \texttt{SEARCH(S, k)} - \textit{interrogazione}
        \begin{itemize}
          \item restituisce il \textbf{riferimento all'oggetto} \texttt{x} corrispondente alla chiave \texttt{K} nella collezione \texttt{S}
          \item se nessun elemento corrisponde alla chiave, restituisce \texttt{NIL}
        \end{itemize}
  \item \texttt{INSERT(S, x)} - \textit{modifica}
        \begin{itemize}
          \item \textbf{inserisce} l'oggetto \texttt{x} nella collezione \texttt{S}
        \end{itemize}
  \item \texttt{DELETE(S, x)} - \textit{modifica}
        \begin{itemize}
          \item \textbf{elimina} l'oggetto \texttt{x} dalla collezione \texttt{S}
        \end{itemize}
  \item \texttt{MINIMUM(S)} - \textit{interrogazione}
        \begin{itemize}
          \item restituisce l'oggetto nella collezione \texttt{S} con la \textbf{chiave più piccola}
        \end{itemize}
  \item \texttt{MAXIMUM(S)} - \textit{interrogazione}
        \begin{itemize}
          \item restituisce l'oggetto nella collezione \texttt{S} con la \textbf{chiave più grande}
        \end{itemize}
  \item \texttt{SUCCESSOR(S, x)} - \textit{interrogazione}
        \begin{itemize}
          \item restituisce l'oggetto nella collezione \texttt{S} con la \textbf{chiave successiva} alla chiave dell'oggetto \texttt{x}
          \item necessita di una qualche forma di ordinamento all'interno di \texttt{S}, definita dalla struttura dati
        \end{itemize}
  \item \texttt{PREDECESSOR(S, x)} - \textit{interrogazione}
        \begin{itemize}
          \item restituisce l'oggetto nella collezione \texttt{S} con la \textbf{chiave precedente} alla chiave dell'oggetto \texttt{x}
          \item necessita di una qualche forma di ordinamento all'interno di \texttt{S}, definita dalla struttura dati
        \end{itemize}
\end{itemize}

\subsection{Pila - \textit{stack}}

Una \textbf{pila} è una collezione di oggetti sulla quale è possibile compiere le seguenti operazioni:

\begin{itemize}
  \item \textbf{Controllare} se è vuota
  \item \textbf{Inserire} un elemento nella collezione - operazione di \texttt{PUSH}
  \item \textbf{Cancellare} l'elemento \textbf{in cima} alla collezione, restituendolo - operazione di \texttt{POP}
\end{itemize}

Una pila è gestita quindi con una politica \LIFO \textit{(last-in, first-out)}: l'elemento che viene cancellato è l'\textbf{ultimo} ad essere stato inserito.
Eseguendo una operazione \texttt{PUSH} di un elemento \texttt{e} su una pila \texttt{S}, seguita immediatamente da una \texttt{POP} di nuovo su \texttt{S}, l'elemento restituito dall'ultima operazione sarà di nuovo \texttt{e}.

\bigskip
La pila può essere implementata come un \texttt{array}.
Infatti, se la pila può contenere al massimo \(n\) elementi, allora è possibile allocare un array di dimensione \(n\) e usarlo come pila.
Per tenere traccia dell'indice dell'elemento che è stato inserito per ultimo viene introdotto un attributo chiamato \texttt{top} che indica l'indice dell'ultimo elemento inserito.

Usando questa implementazione, considerando una pila \texttt{S}, se:

\begin{itemize}
  \item \texttt{S.top = t}, allora \texttt{S[1], S[2], ..., S[t]} contengono tutti gli elementi della pila
  \item \texttt{S.top = 0}, allora la pila è \textbf{vuota} e nessun elemento può essere \textbf{cancellato}
  \item \texttt{S.top = n}, allora la pila è \textbf{piena} e nessun elemento può essere \textbf{inserito}
\end{itemize}

\bigskip
Lo pseudocodice di queste funzioni è mostrato nei Listati~\ref{lst:push-pila}~e~\ref{lst:pop-pila}.

\begin{minipage}[t]{0.495\textwidth}
  \begin{lstlisting}[style=pseudocode, caption={\texttt{PUSH} in pila}, label={lst:push-pila}]
PUSH(S, x)
  if S.top = S.length
    error "overflow"
  else
    S.top := S.top + 1
    S[S.top] := x
  \end{lstlisting}
\end{minipage}
\begin{minipage}[t]{0.495\textwidth}
  \begin{lstlisting}[style=pseudocode, caption={\texttt{POP} in pila}, label={lst:pop-pila}]
POP(S)
  if S.top = 0
    error "underflow"
  else
    S.top := S.top - 1
  return S[S.top + 1]
  \end{lstlisting}
\end{minipage}

La complessità temporale delle operazioni è \(\bigO(1)\).

\subsection{Coda - \textit{queue}}

Una \textbf{coda} è una collezione di oggetti sulla quale è possibile compiere le seguenti operazioni:

\begin{itemize}
  \item \textbf{Controllare} se è vuota
  \item \textbf{Inserire} un elemento della collezione - operazione di \texttt{ENQUEUE}
  \item \textbf{Cancellare} un elemento \textbf{dal fondo} della collezione, restituendolo - operazione do \texttt{DEQUEUE}
\end{itemize}

Una coda è gestita quindi con una politica \FIFO \textit{(first-in, first-out)}: l'elemento che viene cancellato è il \textbf{primo} ad essere stato inserito.

\bigskip
La coda può essere implementata come un array.
Infatti, se la coda può contenere al massimo \(n\) elementi, allora è possibile allocare un array di dimensione \(n+1\) e usarlo come coda.
Per tenere traccia degli indici del primo e dell'ultimo elemento inserito \textit{(rispettivamente)}, vengono introdotti due attributi chiamati \texttt{head} e \texttt{tail}.

Usando questa implementazione, gli attributi della coda \texttt{Q} indicheranno:

\begin{itemize}
  \item \texttt{Q.head} l'indice dell'elemento da più da più tempo nell'array
  \item \texttt{Q.tail} l'indice in cui il prossimo elemento dovrà essere inserito
  \item \texttt{Q.tail - 1} l'indice dell'elemento da meno da meno tempo nell'array
\end{itemize}

La presenza di uno spazio aggiuntivo nell'array rispetto alla dimensione effettiva della coda è dovuto alla gestione dell'elemento aggiunto quando essa è piena.
In tal caso i puntatori \texttt{head} e \texttt{tail} combacerebbero, facendo collassare la struttura.
Questo non è l'unico stratagemma impiegato per risolvere il problema ma esistono strategie alternative.

\bigskip
Inoltre, per poter scrivere lo pseudocodice, è necessario analizzare ulteriormente il funzionamento della coda:

\begin{itemize}
  \item Gli elementi della coda \texttt{Q} avranno indici \(\texttt{Q.head},\ \texttt{Q.head + 1},\ \ldots,\ \texttt{Q.tail - 1},\ \texttt{Q.tail}\)
  \item La coda ha funzionamento circolare:
        \begin{itemize}
          \item e \(\texttt{Q.tail} = \texttt{Q.length}\) e un nuovo elemento viene inserito, il prossimo valore di \texttt{Q.tail} sarà \(1\)
          \item se \(\texttt{Q.head} = \texttt{Q.tail}\) allora la coda è vuota
          \item se \(\texttt{Q.head} = \texttt{Q.tail + 1}\) allora la coda è piena
        \end{itemize}
  \item Se la coda non è piena, c'è sempre almeno una cella libera tra \texttt{Q.tail} e \texttt{Q.head}
\end{itemize}

\bigskip
Lo pseudocodice di queste funzioni è mostrato nei Listati~\ref{lst:enqueue-coda}~e~\ref{lst:dequeue-coda}.

\begin{minipage}[t]{0.495\textwidth}
  \begin{lstlisting}[style=pseudocode, caption={\texttt{ENQUEUE} in coda}, label={lst:enqueue-coda}]
ENQUEUE(Q, x)
  if Q.head = Q.tail + 1:
    error "overflow"
  if Q.tail = Q.length
    Q.tail := 1
  else
    Q.tail := Q.tail + 1
  \end{lstlisting}
\end{minipage}
\begin{minipage}[t]{0.495\textwidth}
  \begin{lstlisting}[style=pseudocode, caption={\texttt{DEQUEUE} in coda}, label={lst:dequeue-coda}]
DEQUEUE(Q)
  if Q.head = Q.tail:
    error "underflow"
  x := Q[Q.head]
  if Q.head = Q.tail
    Q.head := 1
  else
    Q.head := Q.head + 1
  return x
  \end{lstlisting}
\end{minipage}

La complessità temporale delle operazioni è \(\bigO(1)\).

\subsection{Liste \textit{(doppiamente)} concatenate - \textit{deque}}
\label{sec:liste-concatenate}

Una lista concatenata è una struttura dati in cui gli elementi sono sistemati in un ordine lineare, in modo simile ad un array.
L'ordine non è dato dagli indici degli elementi \textit{(che possono essere ordinati in un qualsiasi modo nella memoria del calcolatore)}, ma da una \textit{catena} di puntatori.

Una \textbf{lista doppiamente concatenata} \textit{(deque)} è costituita da oggetti con \(3\) attributi:

\begin{itemize}
  \item \texttt{key}, rappresentante il \textbf{contenuto} dell'oggetto
  \item \texttt{next}, puntatore all'oggetto \textbf{seguente}
  \item \texttt{prev}, puntatore all'oggetto \textbf{precedente}
\end{itemize}

Sia \texttt{x} un oggetto della lista.
Allora:

\begin{itemize}
  \item Se \(\texttt{x.next} = \texttt{NIL}\), allora \(x\) non ha successore ed è l'\textbf{ultimo} elemento della lista
  \item Se \(\texttt{x.prev} = \texttt{NIL}\), allora \(x\) non ha predecessore ed è il \textbf{primo} elemento della lista
  \item Ogni lista \texttt{L} ha un attributo \texttt{L.head} che \textbf{punta al primo} elemento della lista
\end{itemize}

Una \textbf{lista \textit{(singolarmente)} concatenata} ha un funzionamento analogo, con le stesse funzioni, ma senza avere il riferimento all'oggetto precedente \textit{(ha solamente il riferimento all'elemento successivo, \texttt{next})}

\bigskip
Su una lista doppiamente concatenata è possibile eseguire \(3\) operazioni:

\begin{itemize}
  \item \textbf{Ricercare} un elemento, tramite la funzione \texttt{LIST-SEARCH}
  \item \textbf{Inserire} un elemento, tramite la funzione \texttt{LIST-INSERT}
  \item \textbf{Cancellare} un elemento, tramite la funzione \texttt{LIST-DELETE}
\end{itemize}

\bigskip
Un esempio di lista doppiamente concatenata è mostrata in Figura~\ref{fig:lista-doppiamente-concatenata}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-30.tikz}
  \caption{Lista doppiamente concatenata}
  \label{fig:lista-doppiamente-concatenata}
  \bigskip
\end{figure}

\subsubsection{Operazioni sulle liste}

Nei prossimi Paragrafi verranno illustrati gli pseudocodici delle operazioni sulle liste, insieme alle loro complessità temporali.

\paragraph{Ricerca di un elemento}

\textit{Input}: la lista \texttt{L} in cui cercare e la chiave \texttt{k} dell'elemento desiderato.

\textit{Output}: il puntatore all'elemento desiderato se esso è trovato nella lista, \texttt{NIL} in caso contrario.

\textit{Complessità temporale}: \(\Theta(n)\) nel caso pessimo \textit{(la chiave non è nella lista)}.

\bigskip
Lo pseudocodice è riportato nel Listato~\ref{lst:list-search}.

\begin{lstlisting}[style=pseudocode, caption={Pseudocodice dell'algoritmo \texttt{LIST-SEARCH}}, label={lst:list-search}]
LIST-SEARCH(L, k)
  x := L.head
  while x != NIL and x.key != k
    x := x.next
  return x
\end{lstlisting}

\paragraph{Inserimento di un elemento}

\textit{Input}: la lista \texttt{L} in cui cercare e l'oggetto \texttt{x} da aggiungere \textit{(inizializzato dalla chiave)}.

\textit{Output}: nessuno, la lista \texttt{L} viene alterata senza che ne venga creata una nuova.

\textit{Complessità temporale}: \(\bigO(1)\).

\bigskip
Lo pseudocodice è riportato nel Listato~\ref{lst:list-insert}.

\begin{lstlisting}[style=pseudocode, caption={Pseudocodice dell'algoritmo \texttt{LIST-INSERT}}, label={lst:list-insert}]
LIST-INSERT(L, x)
  x.next := L.head
  if L.head != NIL
    L.head.prev := x
  L.head := x
  x.prev := NIL
\end{lstlisting}

\paragraph{Cancellazione di un elemento}

\textit{Input}: la lista \texttt{L} in cui cercare e l'oggetto \texttt{x} da cancellare.
Non è sufficientemente passare solo la chiave dell'oggetto: \textbf{è necessario passare tutto l'oggetto}.

\textit{Output}: nessuno, la lista \texttt{L} viene alterata senza che ne venga creata una nuova.

\textit{Complessità temporale}: \(\Theta(1)\).

\bigskip
Lo pseudocodice è riportato nel Listato~\ref{lst:list-delete}.

\begin{lstlisting}[style=pseudocode, caption={Pseudocodice dell'algoritmo \texttt{LIST-DELETE}}, label={lst:list-delete}]
LIST-DELETE(L, x)
  if x.prev != NIL
    x.prev.next := x.next
  else
    L.head := x.next
  if x.next != NIL
    x.next.prev := x.prev
\end{lstlisting}

\paragraph{Cancellazione di un elemento data la chiave}

\textit{Input}: la lista \texttt{L} in cui cercare e l'oggetto \texttt{x} da cancellare.
Contrariamente alla funzione analizzata precedentemente, \textbf{è sufficiente passare solo la chiave dell'oggetto}.

\textit{Output}: nessuno, la lista \texttt{L} viene alterata senza che ne venga creata una nuova.

\textit{Complessità temporale}: \(\Theta(n)\) nel caso pessimo \textit{(l'oggetto ricercato non è nella lista)}.

\bigskip
Lo pseudocodice è riportato nel Listato~\ref{lst:list-delete-key}.

\begin{lstlisting}[style=pseudocode, caption={Pseudocodice dell'algoritmo \texttt{LIST-DELETE-KEY}}, label={lst:list-delete-key}]
  LIST-DELETE(L, k)
    x = LIST-SEARCH(L, k)
    if x = NIL:
      return
    LIST-DELETE(L, x)
\end{lstlisting}

\subsubsection{Altri tipi di liste}

Oltre alle liste doppiamente concatenate, delle liste comunemente usate sono:

\begin{itemize}
  \item \textbf{Ordinate}
        \begin{itemize}
          \item l'ordinamento degli elementi corrisponde a quello delle chiavi
          \item il primo elemento ha la chiave minima, l'ultimo la massima
        \end{itemize}
  \item \textbf{Non ordinate}
  \item \textbf{Circolari}
        \begin{itemize}
          \item il puntatore \texttt{prev} della testa \texttt{(\texttt{head})} punta alla coda (\texttt{tail})
          \item il puntatore \texttt{next} della coda \texttt{(\texttt{tail})} punta alla testa (\texttt{head})
        \end{itemize}
\end{itemize}

\subsection{Dizionari ed indirizzamento diretto}

Un \textbf{dizionario} è una struttura dati costituita da un insieme dinamico di oggetti che supporta solo le operazioni di:

\begin{itemize}
  \item \textbf{Ricerca} di un elemento della collezione
  \item \textbf{Inserimento} di un elemento nella collezione
  \item \textbf{Cancellazione} di un elemento dalla collezione
\end{itemize}

Gli oggetti all'interno di un dizionario sono indirizzati tramite le loro chiavi, che si assume essere sempre numeri naturali \textit{(se così non fosse, si potrebbe comunque usare la loro rappresentazione in memoria come stringhe di bit)}.

Se la cardinalità \(m\) dell'insieme delle possibili chiavi \(U\) \textit{(quindi \(m = |U|\))} è ragionevolmente piccola, il dizionario può essere implementato tramite un array di \(m\) elementi.
In questo caso l'array prende il nome di \textbf{tabella ad indirizzamento diretto}.
Ogni elemento \(\texttt{T}[\texttt{k}]\) dell'array contiene il riferimento all'oggetto con chiave \texttt{k} se un tale oggetto esiste nella tabella, altrimenti \texttt{NIL}.

\bigskip
Lo pseudocodice delle funzioni appena elencate è mostrato nei Listati~\ref{lst:search-dizionario},~\ref{lst:insert-dizionario}~e~\ref{lst:delete-dizionario}.

I parametri \texttt{T} e \texttt{k} rappresentano rispettivamente il \textbf{dizionario} e la \textbf{chiave} dell'elemento da cercare.

\begin{lstlisting}[numbers=none, style=pseudocode, caption={Ricerca}, label={lst:search-dizionario}]
DIRECT-ADDRESS-SEARCH(T, k)
  return T[k]
\end{lstlisting}

\begin{lstlisting}[style=pseudocode, caption={Inserzione}, label={lst:insert-dizionario}]
DIRECT-ADDRESS-INSERT(T, k)
  T[x.key] = x
\end{lstlisting}

\begin{lstlisting}[numbers=none, style=pseudocode, caption={Cancellazione}, label={lst:delete-dizionario}]
DIRECT-ADDRESS-DELETE(T, k)
  T[x.key] := NIL
\end{lstlisting}

La complessità delle operazioni è \(\bigO(1)\), quindi i dizionari sono le strutture dati più efficienti tra quelle analizzate fin'ora.
Tuttavia, se il numero di chiavi memorizzate è di molto inferiore alla cardinalità dell'insieme delle possibili chiavi, lo spazio occupato sarà inutilmente alto.

\bigskip
Un esempio di dizionario è mostrato nella Tabella~\ref{fig:dizionario}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-31.tikz}
  \caption{Dizionario}
  \label{fig:dizionario}
  \bigskip
\end{figure}

\subsection{Tabelle Hash}

Una tabella hash usa una memoria proporzionale al numero di chiavi effettivamente memorizzate nel dizionario, indipendentemente dalla cardinalità dell'insieme \(U\) di chiavi.

\textit{Idea}: un oggetto di chiave \texttt{k} è memorizzato in tabella in una cella di indice \(h(\texttt{k})\), \(h\) è una \textbf{funzione di hash}:

\begin{itemize}
  \item Se \(m\) è la dimensione della tabella, allora \(h\) è una funzione definita come \(h: \rightarrow \{0, \ldots, m-1\}\)
        \begin{itemize}
          \item il \textbf{dominio} corrisponde al possibile numero di elementi dell'insieme
          \item il \textbf{codominio} corrisponde all'indice degli elementi
          \item \(h(\texttt{k})\) è detto valore \textbf{hash} della chiave \texttt{k}
        \end{itemize}
  \item La tabella \texttt{T} ha \(m\) celle \(\texttt{T}[0], \texttt{T}[1], \ldots, \texttt{T}[m-1]\)
\end{itemize}

\subsubsection{Collisioni}

Ammettendo di avere \(|U|\) possibili chiavi, la funzione \(h\) deve mapparle su un numero \(m < |U|\) di righe della tabella.
Di conseguenza, chiavi diverse daranno origine ad hash uguali:
\[\exists \, k_1, k_2 \ \Rightarrow \ h(k_1) = h_(k_2) \]
Questo fenomeno prende il nome di \textbf{collisione}.
Una delle tecniche per risolvere le collisioni prende il nome di \textbf{concatenamento}.

\textit{Idea} della tecnica: gli oggetti che vengono mappati sullo stesso slot vengono posti in una lista concatenata.

\bigskip
La collisioni e le loro risoluzioni sono mostrate rispettivamente nelle Figure~\ref{fig:collisioni-dizionario}~e~\ref{fig:risoluzione-collisioni-dizionario}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.9]{image-32.tikz}
  \caption{Collisioni in un dizionario}
  \label{fig:collisioni-dizionario}
  \bigskip
\end{figure}

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.9]{image-33.tikz}
  \caption{Risoluzione delle collisioni tramite concatenamento}
  \label{fig:risoluzione-collisioni-dizionario}
  \bigskip
\end{figure}

\subsubsection{Operazioni sulle tabelle hash}

\begin{lstlisting}[style=pseudocode, numbers=none]
CHAINED-HASH-INSERT(T, x)
  inserisci X in testa alla lista T[h(x.key)]
\end{lstlisting}
\begin{lstlisting}[style=pseudocode, numbers=none]
CHAINED-HASH-SEARCH(T, k)
  cerca un elemento con chiave k nella lista T[h(k)]
\end{lstlisting}
\begin{lstlisting}[style=pseudocode, numbers=none]
CHAINED-HASH-DELETE(T, x)
  cancella x dalla lista T[h(x.key)]
\end{lstlisting}

Le complessità di queste operazioni è:

\begin{itemize}
  \item \(\bigO(1)\) per l'\textbf{inserzione} - \texttt{CHAINED-HASH-INSERT}
        \begin{itemize}
          \item per ipotesi, l'elemento non è già nella tabella
        \end{itemize}
  \item \(\bigO(\texttt{T}[h(k)])\) per la \textbf{ricerca} - \texttt{CHAINED-HASH-SEARCH}
  \begin{itemize}
    \item \(\bigO(n)\) con \(n = \text{numero di elementi nella lista \texttt{T}[h(k)]}\)
  \end{itemize}
  \item \(\bigO(1)\) per la \textbf{cancellazione} - \texttt{CHAINED-HASH-DELETE}
        \begin{itemize}
          \item se la lista è singolarmente concatenata, allora la complessità è \(\bigO(\texttt{T}[h(x.key)])\)
        \end{itemize}
\end{itemize}

\bigskip
Nel caso pessimo, in cui tutti gli \(n\) elementi memorizzati finiscono nello stesso slot, la complessità è quella di una ricerca in una lista di \(n\) elementi \textit{(quindi \(\bigO(n)\))}.
Nella realtà, tuttavia, si dimostra che la complessità è più bassa.

Data una tabella hash \texttt{T} di dimensione \(m\) contenente \(n\) elementi.
Allora si definisce il \textbf{fattore di carico} \(\alpha\) come il numero medio di elementi per ogni slot della tabella \textit{(la lunghezza della catena di una cella)}:
\(\alpha = \sfrac{n}{m}\)

Poiché il numero di elementi è compreso tra \(0\) e il numero massimo di chiavi \textit{(infatti \(0 \leq n \leq |U|\))}, per ogni tabella è verificata la relazione \( 0 \leq \alpha \leq \sfrac{|U|}{m} \)

\bigskip
Ipotizzando ora che ogni chiave abbia la stessa probabilità di finire in una delle qualsiasi \(m\) celle di \texttt{T} \textit{(la probabilità è quindi \(\sfrac{1}{m}\))}, la lunghezza della media di una lista è:
\[ E[n_j] = \displaystyle \dfrac{1}{m} \sum_{i=1}^{m} n_i = \dfrac{n}{m} = \alpha \]
Questa condizione è detta \textbf{ipotesi dell'hashing uniforme semplice}.

Il tempo medio necessario a cercare una chiave \(k\) non presente nella lista \textit{(il caso peggiore peggiore per la ricerca)} è \(\Theta(1 + \alpha)\).
Il termine \(1\) è dato dalla complessità del calcolo di \(h(k)\), che viene considerato costante.

\bigskip
Allo stesso tempo, il tempo medio per la ricerca di un elemento presente nella lista è \(\Theta(1+\alpha)\).
Questo valore richiede una dimostrazione aggiuntiva.

Infatti, se \(n = \bigO(m)\), allora il fattore di carico è \(\alpha = \sfrac{n}{m} = \sfrac{\bigO(m)}{m} = \bigO(1)\).
Di conseguenza, la complessità temporale è, in media, \textbf{costante per tutte le operazioni}.

\subsubsection{Funzioni hash}

Fin'ora non è stato discusso il modo in cui una funzione possa creare un hash ma solo che esistono funzioni che possono farlo.
La domanda che sorge spontanea è quindi:

\indentquote{Come si definisce una funzione hash? Quali devono essere le sue caratteristiche?}

In primo luogo, una funzione deve soddisfare l'ipotesi di hashing uniforme semplice.
A tale scopo, essa deve essere in grado di \inlinequote{separare} tra di loro chiavi \inlinequote{vicine}.
Se esse fossero distribuite \textbf{uniformemente} in un intervallo \([0, \ldots, K-1]\), sarebbe sufficiente prendere una funzione del tipo \(h(k) = \lfloor \sfrac{k}{L} \cdot m \rfloor\).
Tipicamente, si impiegano delle funzioni euristiche basate sul dominio delle chiavi.

Un'altra ipotesi delle funzioni hash è che la chiave \(k\) sia un \textbf{intero non negativo} \textit{(e quindi \(\forall \, k \in K, \ k \in \mathbb{N}\))}.
Questa ipotesi non crea un vincolo difficile da risolvere in quanto ogni valore può essere trattato come intero positivo interpretando correttamente la sua codifica \textit{(per esempio, facendo un cast da float ad intero senza segno)}.

\paragraph{Metodo della divisione}

Nel \textbf{metodo della divisione}, una chiave \(k\) viene mappata in uno degli \(m\) slot disponibili nella tabella considerando il resto di \(k\) diviso da \(m\).
La funzione di hash è quindi:
\[ h(k) = k \modop m \]

\bigskip
Questa tecnica fornisce funzioni di hashing molto facili da calcolare \textit{(necessitano di una sola operazione)}.

Per evitare risultati vicini tra di loro \textit{(per ridurre le collisioni)} è necessario non prendere valori di \(k\) nella forma \(k = 2^p\).
Se così non fosse, \(h(k)\) dipenderebbe solo dai \(p\) bit meno significativi di \(k\).
A tal scopo si scelgono valori di \(m\) \textbf{primi} e sufficientemente \textbf{lontani da una potenza di \(2\)}.

\paragraph{Metodo della moltiplicazione}

Nel \textbf{metodo della moltiplicazione}, il valore di \(k\) viene moltiplicato per una costante \(A\) reale compresa tra \(0\) ed \(1\):
\[ A \in \mathbb{R},\ 0 < A < 1 \]
Dopodiché si considera la parte frazionaria di \(kA\) moltiplicata per \(m\), prendendone la parte intera.

La funzione \(h(k)\) ha quindi forma:
\[h(k) = \left\lfloor m (kA \modop 1) \right\rfloor \]
In questa formula \(x \modop 1 = x - \lfloor x \rfloor\) corrisponde alla parte frazionaria di \(x\).

Tramite questa funzione il valore di \(m\) non è critico, perché le eventuali collisioni dipenderanno dal valore di \(A\).
Normalmente si scelgono valori di \(m\) pari a potenza di \(2\) \textit{(cioè \(m = 2^p\))} per rendere più semplici i calcoli.

\bigskip
È utile prendere come \(A\) un valore che sia nella forma \(\sfrac{s}{2^w}\), con \(w\) pari alla dimensione della parola di memoria del calcolatore e \(0 < s < 2^w\).

Se \(k\) è minore della lunghezza della parola (\(k < 2^w\)), allora \(k \cdot s = k \cdot A \cdot 2^w\) è il numero di \(2w\) bit della forma \(r_1 2^w + r_0\) ed i suoi \(w\) bit meno significativi \textit{(rappresentato da \(r_0\))} costituiscono \(kA \modop 1\).
Il valore dell'hash cercato \textit{(se \(m = 2^p\))} è costituito dai \(p\) bit più significativi di \(r_0\).

Un valore di \(A\) proposto da \textit{David Knuth} è pari all'inverso della sezione aurea:
\[ A = \dfrac{\sqrt{5} - 1}{2} \approx 0.6180339887 \ldots \]

Applicando il calcolo precedente, è necessario prendere come valore di \(A\) la frazione nella forma \(\sfrac{s}{2^w}\) più vicina a questo valore.

\subsubsection{Hashing universale}

Scegliendo accuratamente un insieme di chiavi, può accadere che la funzione di hash le assegni tutte allo stesso slot, portando il tempo medio di accesso ai dati a \(\Theta(n)\) \textit{(a causa della necessità di scorrere la lista associata allo slot)}.
Ogni funzione di hash è vulnerabile a questo tipo di problema.

Come soluzione, la funzione di hash viene scelta \textbf{casualmente} in una classe di funzioni all'inizio dell'esecuzione.
Grazie alla sua caratteristica aleatoria, l'algoritmo può comportarsi in modo diverso ad ogni esecuzione del programma, anche per input uguali.

La scelta di alcuni algoritmi potrebbe portare a prestazioni ridotte con un determinato insieme di chiavi, ma la probabilità di tale avvenimento è limitata ed uniforme per ogni funzione scelta.

\bigskip
Sia \(\mathscr{H}\) un \textbf{insieme finito di funzioni} di hash che mappano un dato universo \(U\) di chiavi nell'insieme \(\{0,\, 1, \, \ldots, \, m-1\}\).
Questo insieme è detto \textbf{universale} se per ogni paio di chiavi distinte \(k, l \in U\) il numero di funzioni di hash \(h \in \mathscr{H}\) tale per cui la probabilità che \(h(k) = h(l)\) è al massimo \(\dfrac{|\mathscr{H}|}{m}\).

\bigskip
In altre parole, con una funzione di hashing scelta casualmente in \(\mathscr{H}\), la probabilità di una collisione tra chiavi diverse \(k, l\) è non più della probabilità \(\sfrac{1}{m}\) di una collisione se \(h(k)\) ed \(h(l)\) fossero scelte casualmente ed indipendentemente dall'insieme \(\{0, \, 1, \, \ldots, \, m-1\}\).

\subsubsection{Indirizzamento aperto}
La tecnica dell'\textbf{indirizzamento aperto} serve a ridurre le collisioni.
In questo caso la tabella contiene tutte le chiavi possibili e non sarà necessaria memoria ulteriore per le liste concatenate.
Di conseguenza il fattore di carico \(\alpha\) non sarà mai maggiore di \(1\).

Il vantaggio sta nel minor uso di memoria: non dovendo impiegare puntatori, sarà possibile costruire tabelle di memoria più grandi che però occupano lo stesso spazio, potenzialmente riducendo le collisioni e velocizzando la ricerca degli elementi.

Per effettuare inserzioni usando l'indirizzamento aperto, le celle della tabella vengono successivamente analizzate finché non ne viene trovata una vuota, grazie una \textbf{sequenza di ispezione} calcolata dalla funzione di hash in base alla chiave dell'oggetto.
Le celle analizzate potrebbero \textit{(e in molti casi lo sono)} non essere in ordine numerico.

La funzione di hash avrà forma:
\[ h: U \times \left\{0, 1, \ldots, m-1\right\} \rightarrow \left\{0, 1, \ldots, m-1\right\} \]
Ogni sequenza di permutazione generata da \(h\), costruita come:
\[\langle h(k, 0),\, h(k, 1),\, \ldots\ ,\, h(k, m-1)  \rangle\]
è una permutazione di \(\langle 0, \ldots, m-1\rangle\)

\subsubsection{Operazioni in caso di indirizzamento aperto}

Nei prossimi Paragrafi verranno illustrati gli pseudocodici delle operazioni sulle tabelle hash con indirizzamento aperto, insieme alle loro complessità temporali.

\paragraph{Inserimento}

\begin{lstlisting}[style=pseudocode, caption={Pseudocodice dell'algoritmo \texttt{HASH-INSERT}}, label={sec:inserimento-ind-aperto}]
HASH-INSERT(T, k)
i := 0
repeat
  j := h(k, i)
  if T[j] = NULL
    T[j] := k
    return j
  else
    i := i + 1
until i = m
error "hash table overflow"
\end{lstlisting}

Per inserire un elemento di chiave \texttt{k} nella collezione \texttt{T} l'algoritmo procede secondo la \textbf{sequenza di ispezione}, controllando il contenuto di ogni corrispondente cella \textit{candidata}.

\paragraph{Ricerca}

\begin{lstlisting}[style=pseudocode, caption={Pseudocodice dell'algoritmo \texttt{HASH-SEARCH}}, label={sec:ricerca-ind-aperto}]
HASH-SEARCH(T, k)
  i := 0
  repeat
    j := h(k, i)
    if t[j] = k
      return j
    else:
      i := i + 1
  until t[j] = NIL or i = m
  return NIL
\end{lstlisting}

L'algoritmo per la ricerca della chiave \(k\) controlla \textbf{la stessa sequenza di slot} usata nell'algoritmo di inserzione.
La ricerca può quindi terminare con successo qualora venisse trovato uno slot vuoto, poiché \(k\) sarebbe stata inserita in quest ultimo e non in uno slot successivo nella sequenza.

L'algoritmo prende come input una tabella di hash \texttt{T} ed una chiave \texttt{k}, restituendo \texttt{j} se lo slot \texttt{j} contiene la chiave \texttt{k} oppure \texttt{NIL} se la chiave non è presente nella tabella.

\paragraph{Cancellazione}

La cancellazione è una operazione più complicata, in quanto impostare a \texttt{NIL} lo slot desiderato non è possibile.
Se così fosse, la ricerca di chiavi successive a quella cancellata non sarebbe permesso.

Una possibile soluzione sta nel rimpiazzare il valore \texttt{NIL} con un valore convenzionale \texttt{DELETED}.
Così facendo tuttavia le complessità non dipenderebbero più dal fattore di carico \textit{(come si vedrà più avanti)}.

\paragraph{Complessità temporale}

Il tempo impiegato per trovare lo slot desiderato \textit{(sia esso relativo ad un oggetto da eliminare o da inserire)} dipende dalla sequenza di ispezione restituita dalla funzione \(h\), quindi dipende dalla sua implementazione.

Ipotizzando che le chiavi e le sequenze di ispezione abbiano distribuzione uniforme \textit{(ipotesi di \textbf{hashing uniforme})}, ognuna delle \(m!\) permutazioni di \(\langle 0, \ldots, m-1 \rangle\) ha la stessa probabilità di essere scelta.
Questa è una estensione alla sequenza di ispezione della precedente condizione di hashing uniforme semplice.

L'analisi della complessità viene fatta in funzione del fattore di carico \(\alpha\), che grazie alla struttura della tabella rispetterà le condizioni \(0 \leq \alpha \leq 1,\ \alpha = \sfrac{n}{m} \Rightarrow n \leq m\).

Sotto ipotesi di hashing uniforme, il numero medio di ispezioni necessarie per effettuare l'inserimento di in nuovo oggetto nella tabella è \(m\) se \(\alpha = 1\) \textit{(la tabella è piena)} e non più di \(\sfrac{1}{1-\alpha}\) se \(\alpha<1\) \textit{(se la tabella ha spazio disponibile)}.
Il numero medio di ispezioni necessarie per trovare un elemento presente in tabella è:

\[  T(\alpha) \ \begin{cases}
    = \dfrac{m+1}{2}                                         & \quad \text{ se } \alpha = 1 \\[5pt]
    < \dfrac{1}{\alpha} \log\left(\dfrac{1}{1-\alpha}\right) & \quad \text{ se } \alpha < 1
  \end{cases}  \]

\subsubsection{Tecniche di ispezione}

Nella pratica, costruire funzioni di hash che soddisfino l'ipotesi di hashing fin'ora ricercate è molto difficili.
Per questo motivo si accettano delle approssimazioni che si rivelano soddisfacenti.
Esistono \(3\) tecniche:

\begin{enumerate}
  \item Ispezione lineare
  \item Ispezione quadratica
  \item Doppio hashing
\end{enumerate}

Seppur nessuna di queste tecniche produca le \(m!\) permutazioni che sarebbero necessarie per soddisfare l'ipotesi di hashing uniforme, nella pratica forniscono una distribuzione di chiavi sufficientemente uniforme.

Tutte le tecniche necessitano di una \textbf{funzione di hash ausiliaria} nella forma \(h^\prime : U \rightarrow \left\{0, 1, \ldots, m-1\right\}\).

\paragraph{Ispezione lineare}

Data una funzione di hash ordinaria \(h^\prime : U \mapsto \{0, 1, \ldots, m-1\}\) \textit{(detta funzione di hash ausiliaria)}, il metodo di \textbf{ispezione lineare} usa la funzione di hash definita come:
\[ h(k, i) = (h^\prime(k) + i) \modop m, \quad i = 0, 1, \ldots, m-1 \]

Data una chiave \(k\), la sequenza inizia dallo slot \(T\left([h^\prime (k)]\right)\) per proseguire con \(T\left([h^\prime (k) + 1]\right)\) fino a \(T[m-1]\).
Dopodiché l'algoritmo ricomincia da \(T[0]\) esplorando tutti gli slot di \(T\).

Poiché il valore iniziale determina l'intera sequenza di ispezione, solo \(m\) sequenze distinte possono essere generate.

\bigskip
Questo algoritmo, facile da implementare, soffre di un problema noto come \textbf{addensamento primario} \textit{(primary clustering)}.
La presenza di uno slot vuoto preceduto da \(i\) slot pieni viene riempito con probabilità pari a \(\sfrac{(i+1)}{m}\).
Serie lunghe di slot occupati tendono ad allungarsi ulteriormente, aumentando di conseguenza il tempo medio di ricerca.

\paragraph{Ispezione quadratica}

Il metodo di \textbf{ispezione quadratica} usa una funzione di hash nella forma:
\[ h(k, i) = \left(h^\prime(k) + c_1 i + c_2 i ^2\right), \quad c_2 \neq 0,\ i = 0, 1, \ldots, m-1 \]
in cui \(h^\prime( k)\) prende il nome di \textit{funzione di hash ausiliaria} e \(c_1, c_2\) sono chiamate \textit{costanti ausiliarie}.

La posizione iniziale della sequenza è \(T\left[h^\prime (k)\right]\), seguita da posizioni la cui distanza dipende in modo quadratico dal valore di \(i\).

I valori delle costanti \(c_1,\) e \(c_2\) non sono scelti in modo libero ma sono forzati dalla forma della tabella.

\bigskip
Se due chiavi hanno gli stessi elementi iniziali della sequenza di ispezione (se \(h(k_1, 0) = h(k_2, 0)\)) allora condivideranno la stessa sequenza (\(h(k_1, i) = h(k_2, i) \, \forall \, i\)).
Questa proprietà porta ad una forma di addensamento, detta \textbf{addensamento secondario} \textit{(secondary clustering)}.

Come nell'ispezione lineare, poiché lo slot di partenza determina tutta la sequenza, esisteranno solo \(m\) sequenze distinte.

\paragraph{Doppio hashing}

Il metodo di \textbf{doppio hashing} una funzione di hash nella forma:
\[ h(k, i) = \left(h_1(k) + i h_2(k)\right) \modop m, \quad i = 0, 1, \ldots, m-1 \]
dove \(h_1, h_2\) sono funzioni di hash ausiliarie.

La posizione iniziale della sequenza è \(T\left[h_1(k)\right]\) mentre le posizioni successive hanno distanza dalla precedente pari a \(h_2(k) \modop m\).

\bigskip
Contrariamente ai metodi visti fin'ora, la sequenza generata dal doppio hashing dipende in due modi dalla chiave \(k\), dato che la posizione iniziale e la distanza tra posizioni successive della sequenza possono variare.

Il valore di \(h_2(k)\) non deve avere divisori comuni \textit{(se non \(1\))} con la dimensione \(m\) della tabella di hash per permettere la ricerca dell'intera tabella stessa.

Un modo comodo per assicurare questa è scegliere \(m\) pari ad una potenza di \(2\) e scegliere \(h_2\) in modo che produca sempre numeri dispari.

Alternativamente si può scegliere \(m\) numero primo e scegliere \(h_2\) in modo che produca sempre un intero positivo minore di \(m\).
Allora \(h_1\) e \(h_2\) assumono la forma:

\begin{align*}
  h_1(k) & = k \modop n                         \\
  h_2(k) & = 1 + \left(k \modop n^\prime\right)
\end{align*}

\bigskip
Il numero di sequenze generate tramite questa tecnica è quindi \(\Theta(m^2)\) \textit{(e non più \(\Theta(m)\) come visto fin'ora)} visto che ogni possibile coppia \(\langle h_1(k), h_2(k) \rangle\) genera una sequenza differente.

Le performance del doppio hashing sono molto vicine alle performance dello schema ideale di hashing uniforme.
Di conseguenza, questo è considerato il metodo migliore per generare sequenze di ispezione.

\clearpage

\section{Grafi}
\label{sec:grafi}

\subsection{Alberi binari}

Come già introdotto nella Sezione~\ref{sec:alberi}, un \textbf{albero binario} è composto da \(3\) elementi:

\begin{itemize}
  \item un nodo \textbf{radice}
  \item un albero binario, suo \textbf{sotto albero sinistro}
  \item un albero binario, suo \textbf{sotto albero destro}
\end{itemize}

Normalmente gli alberi binari sono rappresentati nella memoria del calcolatore usando delle strutture di dati concatenate, come le liste concatenate \textit{(viste nella Sezione~\ref{sec:liste-concatenate})}.
Alternativamente, come per gli \textit{heap} \textit{(Sezione~\ref{sec:heap})}, possono essere rappresentati tramite array \textit{(enumerando i nodi)}.

Ogni nodo dell'albero è rappresentato da un oggetto che ha i seguenti attributi:

\begin{itemize}
  \item \texttt{key}, la \textbf{chiave} del nodo
        \begin{itemize}
          \item può rappresentare il contenuto del nodo
          \item può avere anche dati satelliti
        \end{itemize}
  \item \texttt{p}, il \textbf{puntatore} al nodo \textbf{padre}
        \begin{itemize}
          \item a volte viene chiamato \texttt{parent}
          \item se \texttt{x.p = NIL}, allora \texttt{x} è la radice
        \end{itemize}
  \item \texttt{left}, il \textbf{puntatore} alla radice del \textbf{sotto albero sinistro}
        \begin{itemize}[label=\(\rightarrow\)]
          \item se \texttt{left = NIL}, allora il sotto albero sinistro è \textbf{vuoto}
        \end{itemize}
  \item \texttt{right}, il \textbf{puntatore} alla radice del \textbf{sotto albero destro}
  \item   \begin{itemize}[label=\(\rightarrow\)]
          \item se \texttt{right = NIL}, allora il sotto albero destro è \textbf{vuoto}
        \end{itemize}
\end{itemize}

Inoltre ogni albero \texttt{T} ha un attributo \texttt{root}, il \textbf{puntatore} alla \textbf{radice} dell'albero stesso.

\bigskip
Un esempio di albero binario è mostrato nella Figura~\ref{fig:esempio-albero-binario}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-34.tikz}
  \caption{Esempio di albero binario}
  \label{fig:esempio-albero-binario}
  \bigskip
\end{figure}

\subsubsection{Operazioni sugli alberi binari}

Nei Paragrafi seguenti \textit{(\ref{attraversamento-albero-binario}~-~\ref{par:cancellazione-albero-binario})} verranno analizzate delle operazioni comunemente applicate agli alberi binari, insieme a delle immagini esplicative.

\paragraph{Attraversamento}
\label{attraversamento-albero-binario}

L'attraversamento di un albero binario può avvenire esplorando prima in larghezza o prima in profondità.
Nel primo caso si userà l'algoritmo \texttt{BFS} \textit{(Breadth First Search)}, mentre nel secondo si userà l'algoritmo \DFS \textit{(Depth First Search)}.

\begin{itemize}
  \item Nel caso del \texttt{BFS}, si cerca di visitare il nodo più lontano dalla radice, a patto che sia figlio di un nodo già visitato
        \begin{itemize}
          \item non è necessario ricordare i nodi che sono stati visitati
        \end{itemize}
  \item Nel caso del \texttt{DFS}, si cerca di visitare il nodo più vicino alla radice, a patto che non sia già stato visitato
        \begin{itemize}
          \item è necessario tenere traccia di quali nodi sono già stati visitati
        \end{itemize}
\end{itemize}

\bigskip
Un confronto grafico tra i due algoritmi e l'ordine di attraversamento è mostrato nelle Figure~\ref{fig:attraversamento-albero-bfs}~e~\ref{fig:attraversamento-albero-dfs}.

\begin{figure}
  \bigskip
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \tikzfig[0.8]{image-36.tikz}
    \caption{Attraversamento tramite \BFS}
    \label{fig:attraversamento-albero-bfs}
  \end{subfigure}
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \tikzfig[0.8]{image-37.tikz}
    \caption{Attraversamento tramite \DFS}
    \label{fig:attraversamento-albero-dfs}
  \end{subfigure}
  \caption{Confronto negli attraversamenti di un albero binario tramite \BFS e \DFS}
  \label{fig:confronto-attraversamento-bfs-dfs}
  \bigskip
\end{figure}

\bigskip
Entrambi gli algoritmi verranno analizzati più nel dettaglio, applicati ai grafi, nei Paragrafi~\ref{par:grafi-bfs}~e~\ref{par:grafi-dfs}.

\paragraph{Inserzione}
\label{par:inserzione-albero-binario}

L'inserzione di un nodo in un albero binario è leggermente diversa se essa vuole operare su un nodo \textbf{foglia} \textit{(esterno)} o un nodo \textbf{interno}:

\begin{itemize}
  \item nel primo caso basta segnare il nodo come sotto albero
  \item nel secondo caso bisogna cambiare uno dei sotto alberi, assegnandoli al nuovo nodo \texttt{n}
\end{itemize}

\bigskip
Il processo di inserzione è illustrato nella Figura~\ref{fig:inserzione-albero-binario}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.8]{image-38.tikz}
  \caption{Inserzione di un nodo in un albero binario}
  \label{fig:inserzione-albero-binario}
  \bigskip
\end{figure}

\paragraph{cancellazione}
\label{par:cancellazione-albero-binario}

La cancellazione di un elemento da un nodo non è sempre possibile senza creare ambiguità: se un nodo con due sotto alberi viene eliminato, non sarà possibile costruire un nuovo albero binario.

Per eliminare un nodo

\begin{itemize}
  \item \textbf{con un solo sotto albero}, è sufficiente assegnare al padre di \texttt{n} il sotto albero di \texttt{n} nell'attributo corretto \textit{(sia esso \texttt{left} o \texttt{right})}
  \item \textbf{senza sotto alberi}, è sufficiente assegnare al padre di \texttt{n} il valore di \texttt{NIL} nell'attributo corretto \textit{(sia esso \texttt{left} o \texttt{right})}
\end{itemize}

\bigskip
Il processo di cancellazione è illustrato nella Figura~\ref{fig:cancellazione-albero-binario}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.8]{image-39.tikz}
  \caption{cancellazione di un nodo in un albero binario}
  \label{fig:cancellazione-albero-binario}
  \bigskip
\end{figure}

\subsection{Alberi binari di ricerca - \textit{Binary Search Trees}, \BST}

Un \textbf{albero binario di ricerca} \textit{(in Inglese Binary Search Tree, \BST)} è un albero binario che soddisfa la seguente proprietà:

Sia \texttt{x} un nodo di un \BST.
Se \texttt{y} è parte del:

\begin{itemize}
  \item sotto albero \textbf{sinistro} di \texttt{x}, allora \(\texttt{y.key} \leq \texttt{x.key}\)
  \item sotto albero \textbf{destro} di \texttt{x}, allora \(\texttt{y.key} \geq \texttt{x.key}\)
\end{itemize}

Più semplicemente, il figlio sinistro di un nodo sarà sempre \textit{minore o uguale} del nodo stesso.
Allo stesso modo, il figlio destro sarà \textit{maggiore o uguale}.

\bigskip
Un esempio di \BST è mostrato nella Figura~\ref{fig:esempio-bst}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-35.tikz}
  \caption{Esempio di binary search tree - \BST}
  \label{fig:esempio-bst}
  \bigskip
\end{figure}

\bigskip
Grazie alla loro costruzione, è possible attraversare un \BST visitando le chiavi in ordine crescente tramite un semplice algoritmo ricorsivo, detto attraversamento simmetrico o \texttt{INORDER-TREE-WALK}.
Il suo nome deriva dalla sua caratteristica di funzionamento:

\begin{itemize}
  \item il sotto albero sinistro viene visitato ed i suoi nodi vengono restituiti
  \item la radice viene restituita
  \item il sotto albero destro viene visitato ed i suoi nodi vengono restituiti
\end{itemize}.

Sono definiti altri algoritmi che funzionano in modo analogo:

\begin{itemize}
  \item \textbf{anticipato}, nel \texttt{PREORDER-TREE-WALK}, in cui la radice è restituita dopo i sotto alberi
  \item \textbf{posticipato}, nel \texttt{POSTORDER-TREE-WALK}, in cui la radice è restituita dopo i sotto alberi
\end{itemize}

\subsubsection{Altezza di un \BST}

In generale, l'altezza di un \BST è pari a:
\[ h \leq \log{\left(\dfrac{n+1}{2}\right)}, \quad \forall \, n \geq 1\]
Ragionando in termini asintotici l'altezza è \(\Theta(\log{(n)})\) per un albero bilanciato mentre è \(\Theta(n)\) nel caso pessimo, in cui tutti i nodi sono in linea.
Il confronto tra i due casi avviene nella Figura~\ref{fig:altezza-albero-binario}

\bigskip
É poi possibile dimostrare che l'altezza attesa di un albero costruito inserendo le chiavi in ordine casuale con distribuzione uniforme è \(\bigO\left(\log{(n)}\right)\).

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{subfigure}[b]{0.495\textwidth}
    \centering
    \tikzfig{image-44.tikz}
    \caption{albero completo}
  \end{subfigure}
  \begin{subfigure}[b]{0.495\textwidth}
    \centering
    \tikzfig{image-45.tikz}
    \caption{albero in linea}
  \end{subfigure}
  \caption{Altezza di un albero binario}
  \label{fig:altezza-albero-binario}
  \bigskip
\end{figure}

\subsubsection{Attraversamento di \BST}

\paragraph{\texttt{INORDER-TREE-WALK} }

\begin{lstlisting}[style=pseudocode, caption={Attraversamento simmetrico di \BST}, label={lst:attraversamento-bst-inorder}]
INORDER-TREE-WALK(x)
  if x != NIL
    INORDER-TREE-WALK(x.left)
    print(x.key)
    INORDER-TREE-WALK(x.right)
\end{lstlisting}

dove \texttt{x} è la radice del \BST che si vuole visitare.

\bigskip
Chiamando questo algoritmo su un \BST \texttt{T}, tutti i suoi elementi verranno stampati in ordine crescente.

Con \(n\) nodi nel sotto albero, il tempo di esecuzione è \(\Theta(n)\).
Questo valore può essere dimostrato per induzione:

\begin{enumerate}
  \item se l'albero è \textbf{vuoto}, l'algoritmo è eseguito in tempo costante \(c\)
  \item se l'albero ha \(2\) sotto alberi di dimensioni \(k\) `\(n - k - 1\), \(T(n)\) è dato dalla ricorrenza
        \[ T(n) = T(k) + T(n - k - 1) + d \]
\end{enumerate}

\paragraph{\texttt{PREORDER-TREE-WALK} e \texttt{POSTORDER-TREE-WALK}}

\begin{minipage}[t]{0.495\textwidth}
  \begin{lstlisting}[style=pseudocode, caption={Attraversamento anticipato di \BST}, label={lst:attraversamento-bst-preorder}]
PREORDER-TREE-WALK(x)
  if x != NIL
    print(x.key)
    PREORDER-TREE-WALK(x.left)
    PREORDER-TREE-WALK(x.right)
  \end{lstlisting}
\end{minipage}
\begin{minipage}[t]{0.495\textwidth}
  \begin{lstlisting}[style=pseudocode, caption={Attraversamento posticipato di un \BST}, label={lst:attraversamento-bst-postorder}]
POSTORDER-TREE-WALK(x)
  if x != NIL
    POSTORDER-TREE-WALK(x.left)
    POSTORDER-TREE-WALK(x.right)
    print(x.key)
    \end{lstlisting}
\end{minipage}

Il funzionamento di queste due funzioni è analogo a quello di \texttt{INORDER-TREE-WALK}, ma le chiavi vengono stampate in ordine diverso.
La complessità sarà la stessa.

\subsubsection{Operazioni sui \BST}
\label{sec:operazioni-bst}

Nei Paragrafi seguenti \textit{(\ref{par:ricerca-bst}~\ref{par:cancellazione-bst})} verranno analizzate delle operazioni comunemente applicate ai \BST, insieme alle loro relative complessità.

\paragraph{Ricerca}
\label{par:ricerca-bst}

Per effettuare la ricerca di un nodo con una data chiave in un \BST, dato un puntatore alla radice dell'albero e la chiave \texttt{k}, si usa la procedura riportata nel Listato~\ref{lst:tree-search}.
Essa ritornerà il puntatore al nodo \texttt{x} se presente nell'albero, altrimenti ritornerà \texttt{NIL}.

La procedura inizia dal nodo radice e traccia un percorso verso le foglie dell'albero.
Per ogni nodo \texttt{x} che incontra, confronta il valore \texttt{x.key} con \texttt{k}:

\begin{itemize}
  \item Se sono uguali o se \texttt{x} non ha un sotto albero, allora la ricerca termina
  \item Se \texttt{k} è minore di \texttt{x.key}, allora la ricerca continua nel sotto albero sinistro
  \item Se \texttt{k} è maggiore di \texttt{x.key}, allora la ricerca continua nel sotto albero destro
\end{itemize}

\begin{lstlisting}[style=pseudocode, caption={Ricerca di un nodo in un \BST}, label={lst:tree-search}]
TREE-SEARCH(x, k)
  if x = NIL or k = x.key
    return x
  if k < key[x]
    return TREE-SEARCH(x.left, k)
  else
    return TREE-SEARCH(x.right, k)
\end{lstlisting}

\bigskip
Lo stesso algoritmo può essere implementato in modo \textit{ricorsivo} tramite un ciclo while.
Un esempio di questo codice è mostrato nel Listato~\ref{lst:iterative-tree-search}.

\begin{lstlisting}[style=pseudocode, caption={Ricerca iterativa di un nodo in un BST}, label={lst:iterative-tree-search}]
ITERATIVE-TREE-SEARCH(x, k)
  while x != NIL and k != x.key:
    if k < x.key
      x = x.left
    else
      x = x.right
\end{lstlisting}

\bigskip
La complessità temporale dell'algoritmo è \(\bigO(h)\), con \(h = \text{altezza dell'albero}\).

\paragraph{Massimo e minimo}
\label{par:massimo-minimo-bst}

Dato un \BST, può essere importante trovare gli elementi le cui chiavi rappresentano i valori massimi o minimi.

Un elemento dell'albero binario il cui valore è il minimo è sempre trovato seguendo il sotto albero sinistro \textit{(tramite i puntatori \texttt{left})} partendo dalla radice finché non si incontra un nodo senza sotto albero (il cui puntatore ha valore \texttt{NIL}).
Allo stesso modo, il valore massimo è trovato seguendo sempre il sotto albero destro \textit{(tramite i puntatori \texttt{right})}.

I gli pseudocodici delle due funzioni sono illustrati nei Listati~\ref{lst:min-bst}~e~\ref{lst:max-bst}.
Essi sono reciprocamente simmetrici.

\begin{minipage}[t]{0.495\textwidth}
  \begin{lstlisting}[style=pseudocode, caption={Minimo di un BST}, label={lst:min-bst}]
TREE-MINIMUM(x)
  while x.left != NIL
    x = x.left
  return x
  \end{lstlisting}
\end{minipage}
\begin{minipage}[t]{0.495\textwidth}
  \begin{lstlisting}[style=pseudocode, caption={Massimo di un BST}, label={lst:max-bst}]
TREE-MAXIMUM(x)
  while x.right != NIL
    x = x.right
  return x
  \end{lstlisting}
\end{minipage}

dove \texttt{x} è il puntatore alla radice dell'albero e il valore restituito sarà il puntatore al valore massimo o minimo, a seconda del caso.

\bigskip
La complessità temporale di entrambi gli algoritmi è \(\bigO(h)\), con \(h = \text{altezza dell'albero}\).

\paragraph{Successore e predecessore}
\label{par:successore-predecessore-bst}

Dato un nodo di un \BST, può essere importante trovare il suo successore o predecessore.

Se tutte le chiavi sono diverse, il successore di n nodo \texttt{x} è definito come il nodo con la chiave più piccola che è maggiore di \texttt{x.key}.
Allo stesso modo il predecessore di \texttt{x} è il nodo con la chiave più grande che è minore di \texttt{x.key}.
La struttura del \BST, tuttavia, permette di di eseguire queste operazioni senza dovere effettivamente confrontare le chiavi.

Gli pseudocodici delle due funzioni sono illustrati nei Listati~\ref{lst:predecessore-bst}~e~\ref{lst:successore-bst}.

\begin{minipage}[t]{0.495\textwidth}
  \begin{lstlisting}[style=pseudocode, caption={Successore di di un nodo}, label={lst:successore-bst}]
TREE-SUCCESSOR(x)
  if x.right != NIL
    return TREE-MINIMUM(x.right)
  y := x.parent
  while y != NIL and x = y.right
    x := y
    y := y.parent
  return y
  \end{lstlisting}
\end{minipage}
\begin{minipage}[t]{0.495\textwidth}
  \begin{lstlisting}[style=pseudocode, caption={Successore di di un nodo}, label={lst:predecessore-bst}]
TREE-PREDECESSOR(x)
  if x.left != NIL
    return TREE-MAXIMUM(x.left)
  y := x.parent
  while y != NIL and x = y.left
    x := y
    y := y.parent
  return y
      \end{lstlisting}
\end{minipage}

Gli algoritmi restituiscono rispettivamente il successore e il predecessore di un nodo \texttt{x}.

\bigskip
Se il sotto albero destro di \texttt{x} è vuoto, il successore di \texttt{x} è il primo elemento \texttt{y} che si incontra risalendo nell'albero da \texttt{x} tale che \texttt{x} è nel sotto albero sinistro di \texttt{y}.
Allo stesso modo se il sotto albero sinistro di \texttt{x} è vuoto, il predecessore di \texttt{x} è il primo elemento \texttt{y} che si incontra risalendo nell'albero da \texttt{x} tale che \texttt{x} è nel sotto albero destro di \texttt{y}.

\bigskip
La complessità temporale di entrambi gli algoritmi è \(\bigO(h)\), con \(h = \text{altezza dell'albero}\).

\paragraph{Inserimento}
\label{par:inserimento-bst}

L'operazione di inserimento causa modifiche nella rappresentazione del \BST.
La struttura dati deve essere modificata in modo da rispettare la proprietà dei \BST.

L'inserzione di un nuovo valore è una operazione relativamente immediata.

Per inserire un nuovo valore \texttt{v} all'interno del \BST \texttt{T}, viene usato l'algoritmo \texttt{TREE-INSERT}.
Esso prende come parametro un puntatore al nodo \texttt{z} tale per cui \(\texttt{z.key } = \texttt{ v}\), \(\texttt{z.left } = \texttt{ NIL}\), \(\texttt{z.right } = \texttt{ NIL}\).
Modifica \texttt{T} e alcuni dei campi di \texttt{z} in modo che il nuovo nodo sia inserito nella posizione appropriata dell'albero.

Lo pseudocodice dell'algoritmo è mostrato nel Listato~\ref{lst:inserimento-bst}.

\begin{lstlisting}[style=pseudocode, caption={Inserimento di un nuovo nodo}, label={lst:inserimento-bst}]
TREE-INSERT(T, z)
  y := NIL
  x := T.root
  while x != NIL
    y := x
    if z.key < x.key
      x := x.left
    else
      x := x.right
  z.parent := y
  if y = NIL
    T.root := z
  else if z.key < y.key
    y.left := z
  else
    y.right := z
\end{lstlisting}

\bigskip
La complessità temporale di entrambi gli algoritmi è \(\bigO(h)\), con \(h = \text{altezza dell'albero}\).

\paragraph{cancellazione}
\label{par:cancellazione-bst}

Analogamente a quanto già descritto nell'inserimento, l'operazione di cancellazione causa modifiche nella struttura del \BST.

Infatti, per eliminare un nodo \texttt{z} da un \BST \texttt{T}:

\begin{itemize}
  \item Se \texttt{z} non ha sotto alberi, allora è sufficiente modificare il padre di \texttt{z}, rimuovendo i suoi figli
  \item Se \texttt{z} ha un solo sotto albero, allora è sufficiente modificare il padre di \texttt{z}, aggiungendo come sotto albero \texttt{z}
  \item Se \texttt{z} ha due sotto alberi, è necessario:
        \begin{itemize}
          \item trovare il successore \texttt{s} di \texttt{z}
          \item copiare \texttt{s.key}, \texttt{s.left}, \texttt{s.right} in \texttt{z} e cancellare \texttt{s}
        \end{itemize}
\end{itemize}

Lo pseudocodice dell'algoritmo è mostrato nel Listato~\ref{lst:cancellazione-bst}, mentre una raffigurazione dei tre casi è mostrata nelle Figure \ref{fig:cancellazione-nodo-bst-senza-sotto-alberi}, \ref{fig:cancellazione-nodo-bst-con-un-solo-sotto-albero} e \ref{fig:cancellazione-nodo-bst-con-due-sotto-alberi}.

\begin{lstlisting}[style=pseudocode, caption={Cancellazione di un nodo}, label={lst:cancellazione-bst}]
TREE-DELETE(T, z):
  if z.left = NIL or z.right = NIL
    y := z
  else
    y := TREE-SUCCESSOR(z)
  if y.left != NIL
    x := y.left
  else
    x := y.right
  if x != NIL
    x.parent := y.parent
  if y.parent = NIL
    T.root := x
  else if y = y.parent.left
    y.parent.left := x
  else
    y.parent.right := x
  if y != z
    z.key := y.key
  return y
\end{lstlisting}

Dove \texttt{T} è la radice dell'albero e \texttt{z} è il nodo da eliminare.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.7]{image-41.tikz}
  \caption{Cancellazione di un nodo senza sotto alberi}
  \label{fig:cancellazione-nodo-bst-senza-sotto-alberi}
  \bigskip
\end{figure}

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.7]{image-42.tikz}
  \caption{Cancellazione di un nodo con un solo sotto albero}
  \label{fig:cancellazione-nodo-bst-con-un-solo-sotto-albero}
  \bigskip
\end{figure}

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[0.7]{image-43.tikz}
  \caption{Cancellazione di un nodo con due sotto alberi}
  \label{fig:cancellazione-nodo-bst-con-due-sotto-alberi}
  \bigskip
\end{figure}

Funzionamento dell'algoritmo:

\begin{itemize}
  \item Il nodo \texttt{y} è quello effettivamente da eliminare
  \item Se \texttt{z} non ha più di un sotto albero, allora \texttt{y} coincide con \texttt{z}. In caso contrario, \texttt{y} è il nodo successore di \texttt{z} \textit{(linee \(2-4\))}
  \item Nelle linee \(5-7\) viene assegnato a \texttt{x} la radice del sotto albero di \texttt{y} se presente o \texttt{NIL} altrimenti
  \item Le linee \(8-14\) sostituiscono \texttt{y} con il suo sotto albero, avente \texttt{x} come radice
  \item Nelle linee \(15-16\), se \texttt{z} ha \(2\) sotto alberi, la chiave di \texttt{z} è sostituita con quella del suo successore \texttt{y}
\end{itemize}

\bigskip
La complessità temporale di entrambi gli algoritmi è \(\bigO(h)\), con \(h = \text{altezza dell'albero}\).

\subsection{Alberi bilanciati}

Esistono svariate definizioni di albero bilanciato.

\textit{Informalmente}, un albero è detto \textbf{bilanciato} se e solo se non esistono due foglie nell'albero tali che la distanza dalla radice di una sia molto maggiore dell'altra.

Una possibile definizione formale \textit{(definita da Adelson-Velskii e Landis)}, è la seguente:

\indentquote{Un albero è bilanciato se e solo se, per ogni nodo \texttt{x} dell'albero, le altezze dei due sotto alberi di \texttt{x} differiscono al massimo di \(1\).}

Esistono varie tecniche per mantenere un albero bilanciato.
Nella sezione seguente verranno esplorati gli alberi \textbf{rosso-neri}.

\subsection{Alberi rosso-neri - \RB}
\label{sec:alberi-rosso-neri}

Gli \textbf{alberi rosso-neri} \textit{(o dall'Inglese red-black, \RB)} sono alberi bilanciati.
Le operazioni fondamentali mostrate nella Sezione precedente avranno complessità temporale pari a \(\bigO(\log{(n)})\), con \(h = \text{altezza dell'albero}\), minore di quanto riscontrato negli \BST.

I nodi degli alberi \RB hanno un attributo aggiuntivo, relativo al colore, che può indicare se esso è rosso o nero.
Distribuendo i colori nel modo corretto, viene garantito che nessun percorso dalla radice alle foglie sia più lungo di un altro.

Ogni nodo dell'albero contiene gli attributi \texttt{color}, \texttt{key}, \texttt{left}, \texttt{right} e \texttt{parent}.
Se un nodo non ha un padre o dei sotto alberi, allora il suo valore è \texttt{NIL}.

Un albero binario è da considerarsi rosso-nero se ha le seguenti \(5\) \textit{proprietà}:

\begin{enumerate}[label=\arabic*., ref=(\arabic*)]
  \item \label{enum:proprieta-1-nodi-rb}Ogni nodo è \textbf{rosso} o \textbf{nero}
  \item \label{enum:proprieta-2-nodi-rb}La \textbf{radice} è \textbf{nera}
  \item \label{enum:proprieta-3-nodi-rb}Ogni \textbf{foglia} è \textbf{nera}
  \item \label{enum:proprieta-4-nodi-rb}Se un nodo è \textbf{rosso}, entrambi i suoi figli sono \textbf{neri}
  \item \label{enum:proprieta-5-nodi-rb} Per ogni \textbf{nodo}, ogni percorso dal nodo alle foglie discendenti contiene lo stesso numero di \textbf{nodi neri}
        \begin{itemize}
          \item il numero di nodi neri nel percorso che part dal nodo \texttt{x} è detto \(bh(\texttt{x})\)
          \item il nodo stesso non è conteggiato in \(bh(\texttt{x})\) anche se nero
        \end{itemize}
\end{enumerate}

Per comodità, spesso tutte le foglie vengono rappresentate da un solo valore \texttt{NIL} anziché avere puntatori diversi.
Questo nodo sarà accessibile come attributo dell'albero \texttt{T} e ogni riferimento a \texttt{NIL} sarà rimpiazzato da un puntatore a \texttt{T.nil}.

\bigskip
Il confronto tra le due rappresentazioni è mostrato nella Figura~\ref{fig:confronto-tra-le-rappresentazioni-degli-alberi-RB}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \begin{subfigure}[b]{.495\textwidth}
    \centering
    \tikzfig[.9]{image-46.tikz}
    \caption{Ogni foglia è rappresentata come \texttt{NIL}}
    \bigskip
    \label{fig:foglia-rappresentata-come-NIL}
  \end{subfigure}
  \begin{subfigure}[b]{.495\textwidth}
    \centering
    \tikzfig[.9]{image-47.tikz}
    \caption{Tutte le foglie sono rappresentate da un solo nodo}
    \bigskip
    \label{fig:foglie-rappresentate-da-un-solo-nodo}
  \end{subfigure}
  \caption{Confronto tra le rappresentazioni degli alberi \RB}
  \label{fig:confronto-tra-le-rappresentazioni-degli-alberi-RB}
  \bigskip
\end{figure}

\bigskip
Grazie al modo in cui è costruito, un albero \RB con \(n\) nodi interni (quindi \(n\) nodi con chiave) ha altezza:
\[h \leq 2 \log_2{(n+1)}\]

Il numero di nodi interni di un albero \textit{(o di un sotto albero)} con radice \texttt{x} è sempre \(\geq 2^{bh(\texttt{x})-1}\).
Questa proprietà può essere dimostrata per induzione sull'altezza di \texttt{x}.

Per la proprietà \ref{enum:proprieta-5-nodi-rb} degli alberi, almeno metà dei nodi dalla radice \texttt{x} \textit{(esclusa)} ad una foglia sono neri.
Di conseguenza \(bh(\texttt{x}) \geq \sfrac{x}{2}\), \(n \geq 2^{\sfrac{h}{2}}\) e quindi:
\[ h \leq 2 \log_2{(n+1)} \]

\subsubsection{Operazioni sugli alberi \RB}

Nei Paragrafi seguenti \textit{(\ref{par:operazioni-rb-analoghe-a-bst}-\ref{par:cancellazione-rb})} verranno analizzate delle operazioni comunemente applicate ai \BST, insieme alle loro relative complessità.

\paragraph{Ricerca, massimo, minimo, successore e predecessore}
\label{par:operazioni-rb-analoghe-a-bst}

Lo pseudocodice delle operazioni di:

\begin{itemize}
  \item \textbf{Ricerca}
  \item \textbf{Massimo} e \textbf{Minimo}
  \item \textbf{Successore} e \textbf{Predecessore}
\end{itemize}

è analogo a quello mostrato nelle operazioni dei \BST, nella Sezione~\ref{sec:operazioni-bst}.

Sfruttando ;a formula relativa all'altezza di un albero, ricavata nella Sezione precedente, è possibile ricalcolare la complessità temporale di tutte le operazioni fin'ora.
Essa sarà pari a:

\[ \bigO\left(h\right) = \bigO\left(\log{(n)}\right)\]

Dove \(n\) indica il numero di nodi dell'albero.

\paragraph{Rotazioni}
\label{par:rotazioni-rb}

Le operazioni di Inserimento e Cancellazione (Sezioni \ref{par:inserimento-rb} e \ref{par:cancellazione-rb}), impiegate su un albero \RB, hanno complessità temporale \(\bigO\left(\log{(n)}\right)\).
Siccome essi modificano l'albero, la struttura risultante da queste operazioni potrebbe violare le proprietà elencate nella Sezione~\ref{sec:alberi-rosso-neri}.

Per ripristinare queste proprietà, il colore di alcuni nodi nell'albero e la struttura dei puntatori devono essere modificati.
Quest'ultima viene alterata tramite una operazione detta \textbf{rotazione}, che può essere \textbf{sinistra} o \textbf{destra}.

Una rotazione \textbf{destra} su un nodo di un albero \RB richiede che il suo figlio \textbf{destro} non sia \texttt{T.nil}, mentre il suo figlio sinistro può essere un nodo qualsiasi.
Allo stesso modo, una rotazione \textbf{sinistra} richiede che il figlio \textbf{sinistro} non sia \texttt{T.nil}.

Una rotazione sinistra sul nodo \texttt{x} con figlio \texttt{y} \inlinequote{ruota} attorno all'arco tra i due, trasformando \texttt{y} nella nuova radice con \texttt{x} come figlio sinistro e il figlio sinistro di \texttt{y} come figlio destro di \texttt{x}.
La rotazione sinistra avrà comportamento speculare.

\bigskip
Gli pseudocodici delle funzione \texttt{LEFT-ROTATE} e \texttt{RIGHT-ROTATE} sono mostrati nei Listati~\ref{lst:rotazione-sinistra-nodo-rt}~e~\ref{lst:rotazione-destra-nodo-rt}.

\begin{minipage}[t]{0.495\linewidth}
  \begin{lstlisting}[style=pseudocode, caption={Rotazione sinistra di un nodo in un RB}, label={lst:rotazione-sinistra-nodo-rt}]
LEFT-ROTATE(T, x)
  y := x.right
  x.right := y.left
  if y.left != T.nil
    y.left.parent := x
  y.parent := x.parent
  if x.parent = T.nil
    T.root := y
  else if x.parent.left = x
    x.parent.left := y
  else
    x.parent.right := y
  y.left := x
  x.parent := y
  \end{lstlisting}
\end{minipage}
\begin{minipage}[t]{0.495\linewidth}
  \begin{lstlisting}[style=pseudocode, caption={Rotazione destra di un nodo in un RB}, label={lst:rotazione-destra-nodo-rt}]
RIGHT-ROTATE(T, x)
  y := x.left
  x.left := y.right
  if y.right != T.nil
    y.right.parent := x
  y.parent := x.parent
  if x.parent = T.nil
    T.root := y
  else if x.parent.right = x
    x.parent.right := y
  else
    x.parent.left := y
  y.right := x
  x.parent := y
  \end{lstlisting}
\end{minipage}

\bigskip
Entrambe le procedure hanno complessità temporale \(\bigO(1)\).
Inoltre l'operazione di rotazione su un \RB produce sempre un \RB.

\bigskip
Una rappresentazione grafica dell'operazione è mostrata nell'immagine~\ref{fig:rotazione-sinistra-destra-rb}.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig[1.5]{image-48.tikz}
  \caption{Rotazione sinistra e destra di un albero \RB}
  \label{fig:rotazione-sinistra-destra-rb}
  \bigskip
\end{figure}

\paragraph{Inserimento}
\label{par:inserimento-rb}

L'inserimento è analogo a quello dell'\BST, illustrato nel Paragrafo~\ref{par:inserimento-bst}.
Tuttavia a fine procedura è necessario ristabilire le proprietà degli alberi \RB, se esse sono state violate, tramite l'algoritmo \texttt{RB-INSERT-FIXUP}.

\bigskip
Gli pseudocodici dei due algoritmi sono mostrati nei Listati~\ref{lst:insermento-nodo-rb}~e~\ref{lst:rb-insert-fixup}.

\begin{minipage}[t]{0.445\textwidth}
  \begin{lstlisting}[style=pseudocode, caption={Inserimento in un RB}, label={lst:insermento-nodo-rb}]
RB-INSERT(T, z)
  y := T.nil
  x := T.root
  while x != T.nil
    y := x
    if z.key < x.key
      x := x.left
    else
      x := x.right
  z.parent := y
  if y = T.nil
    T.root := z
  else if z.key < y.key
    y.left := z
  else
    y.right := z
  z.left := T.nil
  z.right := T.nil
  z.color := RED
  RB-INSERT-FIXUP(T, z)
\end{lstlisting}
\end{minipage}
\begin{minipage}[t]{0.55\textwidth}
  \begin{lstlisting}[style=pseudocode, caption={Fixup dopo inserimento}, label={lst:rb-insert-fixup}]
RB-INSERT-FIXUP(T, z)
  while z.parent.color = RED
    if z.parent = z.parent.parent.left
      y := z.parent.parent.right
      if y.color = RED
        // caso 1
        z.parent.color := BLACK
        y.color := BLACK
        z.parent.parent.color := RED
        z = z.parent.parent
      else if z = z.parent.right
        // caso 2
        z := z.parent
        LEFT-ROTATE(T, z)
      // caso 3
      z.parent.color := BLACK
      z.parent.parent.color := RED
      RIGHT-ROTATE(T, z.parent.parent)
    else
      come le linee 3-18 con
        left and right invertiti
  T.root.color := BLACK
  \end{lstlisting}
\end{minipage}

Per comprendere il funzionamento della procedura \texttt{RB-INSERT-FIXUP} è necessario dividere il codice in \(3\) casi principali, per poi analizzare l'obiettivo del ciclo \textbf{\texttt{while}} delle linee \(2-21\).

Le proprietà \ref{enum:proprieta-1-nodi-rb} e \ref{enum:proprieta-3-nodi-rb} continuano ad essere valide poiché i figli di entrambi i nodi rossi inseriti sono \texttt{T.nil}.
Allo stesso modo la proprietà~\ref{enum:proprieta-5-nodi-rb}, che regola il numero di nodi neri tra su ogni percorso, è ancora soddisfatta.

Le uniche proprietà escluse sono:

\begin{itemize}
  \item la \ref{enum:proprieta-2-nodi-rb}, che richiede che la radice sia nera
        \begin{itemize}
          \item violata solo se \texttt{z} è la radice
        \end{itemize}
  \item la \ref{enum:proprieta-4-nodi-rb}, che richiede che entrambi i figli di un nodo rosso siano neri
        \begin{itemize}
          \item violata solo se il padre di \texttt{z} è rosso
        \end{itemize}
\end{itemize}

\bigskip
Il ciclo \textbf{\texttt{while}} del codice nel Listato~\ref{lst:rb-insert-fixup} mantiene valide le seguenti \(3\) proprietà:

\begin{enumerate}
  \item Il nodo \texttt{z} è rosso
  \item Se \texttt{z.parent} è la radice, allora \texttt{z.parent} è nero
  \item Se si dovesse presentare una violazione delle proprietà dell'albero \RB, allora è sicuramente una violazione della proprietà \ref{enum:proprieta-2-nodi-rb} o \ref{enum:proprieta-4-nodi-rb}:
        \begin{itemize}
          \item se la proprietà \ref{enum:proprieta-2-nodi-rb} è violata, allora \texttt{z} è la radice ed è rossa
          \item se la proprietà \ref{enum:proprieta-4-nodi-rb} è violata, allora sia \texttt{z} che \texttt{z.parent} sono rossi
        \end{itemize}
\end{enumerate}

Quindi, come evidenziato nel Listato~\ref{lst:rb-insert-fixup}, sono evidenziati \(3\) casi:

\begin{itemize}
  \item[Caso \(1\):] \texttt{y} è rosso, \texttt{x} è rosso, \texttt{y} è rosso \textit{(Figura~\ref{fig:caso-1-algoritmo-rb-insert-fixup})}.
    \begin{itemize}
      \item la proprietà \ref{enum:proprieta-4-nodi-rb} è violata
      \item bisogna ripetere la procedura su \texttt{x.parent} perché suo padre potrebbe essere rosso
      \item la situazione è evoluta nel caso \(2\)
    \end{itemize}
  \item[Caso \(2\):] \texttt{y} è nero, \texttt{z} è il figlio destro di \texttt{x} \textit{(Figura~\ref{fig:caso-2-algoritmo-rb-insert-fixup})}.
    \begin{itemize}
      \item la proprietà \ref{enum:proprieta-2-nodi-rb} è violata
      \item la situazione è evoluta nel caso \(3\)
    \end{itemize}
  \item[Caso \(3\):] \texttt{y} è nero, \texttt{z} è il figlio sinistro di \texttt{x} \textit{(Figura~\ref{fig:caso-3-algoritmo-rb-insert-fixup})}.
\end{itemize}

\begin{figure}[htbp]
  \begin{subfigure}{\textwidth}
    \centering
    \bigskip
    \tikzfig[0.7]{image-49.tikz}
    \caption{caso \(1\) dell'algoritmo \texttt{RB-INSERT-FIXUP}}
    \label{fig:caso-1-algoritmo-rb-insert-fixup}
    \bigskip
  \end{subfigure}
  \begin{subfigure}{\textwidth}
    \centering
    \bigskip
    \tikzfig[0.8]{image-50.tikz}
    \caption{caso \(2\) dell'algoritmo \texttt{RB-INSERT-FIXUP}}
    \label{fig:caso-2-algoritmo-rb-insert-fixup}
    \bigskip
  \end{subfigure}
  \begin{subfigure}{\textwidth}
    \centering
    \bigskip
    \tikzfig[0.8]{image-51.tikz}
    \caption{caso \(3\) dell'algoritmo \texttt{RB-INSERT-FIXUP}}
    \label{fig:caso-3-algoritmo-rb-insert-fixup}
    \bigskip
  \end{subfigure}
  \caption{Casi di \texttt{RB-INSERT-FIXUP}}
  \label{fig:casi-algoritmo-rb-insert-fixup}
\end{figure}

\bigskip
Ogni volta che \texttt{RB-INSERT-FIXUP} viene eseguito, esso può terminare \textit{(come nei casi \(2\) e \(3\))}, oppure può venire applicato ricorsivamente risalendo \(2\) livelli nell'albero \textit{(come nel caso \(1\))}.
In quest'ultimo caso, vengono effettuate \(2\) rotazioni \textit{(durante l'ultima chiamata)}.

Quindi, nel caso peggiore, esso viene invocato al massimo \(h\) volte, dove \(h\) è l'altezza dell'albero.
Di conseguenza la complessità temporale sarà \(\bigO(h) = \bigO(\log{(n)})\).

\paragraph{Cancellazione}
\label{par:cancellazione-rb}

La procedura \texttt{RB-DELETE} è una versione adattata dell'analogo \texttt{TREE-DELETE} \textit{(Paragrafo~\ref{par:cancellazione-albero-binario})}.
Dopo l'eliminazione del nodo, una procedura secondaria chiamata \texttt{RB-DELETE-FIXUP} viene chiamata per assicurarsi che le proprietà dell'albero \RB siano mantenute.

Il codice delle procedure è mostrato nel Listati~\ref{lst:cancellazione-nodo-rb}~e~\ref{lst:rb-delete-fixup}.

\begin{lstlisting}[style=pseudocode, caption={Cancellazione in un RB}, label={lst:cancellazione-nodo-rb}]
RB-DELETE(T, z)
  if z.left = T.nil or z.right = T.nil
    y := z
  else
    y := TREE-SUCCESSOR(z)
  if y.left != T.nil
    x := y.left
  else
    x := y.right
  x.parent := y.parent
  if y.parent = T.nil
    T.root := x
  else if y.parent.left = y
    y.parent.left := x
  else
    y.parent.right := x
  if y != z
    z.key := y.key
  if y.color = BLACK
    RB-DELETE-FIXUP(T, x)
\end{lstlisting}
\begin{lstlisting}[style=pseudocode, caption={Fixup dopo cancellazione}, label={lst:rb-delete-fixup}]
RB-DELETE-FIXUP(T, x)
  if x.color = RED or x.parent = T.nil
    // caso 0
    x.color := BLACK
  else if x = x.parent.left
    w := x.parent.right
    if w.color = RED
      // caso 1
      w.color := BLACK
      x.parent.color := RED
      LEFT-ROTATE(T, x.parent)
      w := x.parent.right
    if w.left.color = BLACK and w.right.color = BLACK
      // caso 2
      w.color := RED
      RB-DELETE-FIXUP(T, x.parent)
    else if w.right.color = BLACK
      // caso 3
      w.left.color := BLACK
      w.color := RED
      RIGHT-ROTATE(T, w)
      w := x.parent.right
    w.color := x.parent.color
    x.parent.color := BLACK
    w.right.color := BLACK
    LEFT-ROTATE(T, x.parent)
  else
  come le linee 2-26 con
    left and right invertiti
  \end{lstlisting}

\bigskip
Il funzionamento di \texttt{RB-DELETE}, come già introdotto, molto simile alla cancellazione di un nodo in un albero binario.
Possono essere evidenziate le seguenti differenze:

\begin{itemize}
  \item Viene usato \texttt{T.NIL} al posto del valore \texttt{NIL}
  \item Se un nodo rosso viene cancellato, non è necessario modificare il valore degli altri nodi
  \item Per la struttura di dell'algoritmo, se viene cancellato un nodo \texttt{y} con al massimo un figlio, allora \texttt{y} è nero
\end{itemize}

Se \texttt{y} è nero, le proprietà dell'albero \RB potrebbero non essere mantenute:

\begin{itemize}
  \item il numero di nodi neri incontrati in un cammino potrebbe essere diversa \textit{(Proprietà~\ref{enum:proprieta-5-nodi-rb} degli alberi \RB)}
  \item due nodi neri potrebbero essere adiacenti (\textit{(Proprietà~\ref{enum:proprieta-5-nodi-rb} degli alberi \RB)})
  \item \texttt{y} sarebbe potuta essere la radice, quindi essa potrebbe non essere più nera (\textit{(Proprietà~\ref{enum:proprieta-2-nodi-rb} degli alberi \RB)})
\end{itemize}

\bigskip
Il nodo passato alla funzione \texttt{RB-DELETE-FIXUP} è un tra:

\begin{itemize}
  \item Il nodo figlio (unico) di \texttt{y} prima che esso venisse eliminato
  \item Il nodo sentinella \texttt{T.nil} se \texttt{y} non aveva figli
\end{itemize}

Nel secondo caso, l'assegnamento nella linea \(10\) assicura che il padre di \texttt{x} e ora il nodo precedentemente padre di \texttt{y}, sia esso un nodo con valore o \texttt{T.nil}.

All'interno del codice si delineano \(4\) casi:

\begin{itemize}
  \item[Caso \(0\):] \texttt{x} è un nodo rosso, oppure è la radice.
  \item[Caso \(1\):] \texttt{x} è un nodo nero, suo fratello destro \texttt{w} è rosso \textit{(Figura~\ref{fig:caso-1-algoritmo-rb-delete-fixup})}.
    \begin{itemize}
      \item il padre di \texttt{x} è nero
      \item la struttura evolve nei casi \(2\), \(3\) o \(4\)
    \end{itemize}
  \item[Caso \(2\):] \texttt{x} è un nodo nero, suo fratello destro \texttt{w} è nero con i figli entrambi neri \textit{(Figura~\ref{fig:caso-2-algoritmo-rb-delete-fixup})}.
    \begin{itemize}
      \item se si proviene dal caso \(1\), allora \texttt{x.parent} è rosso. L'invocazione di \texttt{RB-DELETE-FIXUP} termina subito
    \end{itemize}
  \item[Caso \(3\):] \texttt{x} è nero, suo fratello destro \texttt{w} è nero con figlio sinistro rosso e figlio destro nero \textit{(Figura~\ref{fig:caso-3-algoritmo-rb-delete-fixup})}.
    \begin{itemize}
      \item la struttura evolve nel caso \(4\)
    \end{itemize}
  \item[Caso \(4\):] \texttt{x} è nero, suo fratello destro \texttt{w} è nero con figlio destro rosso \textit{(Figura~\ref{fig:caso-4-algoritmo-rb-delete-fixup})}.
\end{itemize}

Ogni volta che \texttt{RB-DELETE-FIXUP} viene invocata, essa può terminare \textit{(casi \(0,\ 1,\ 3,\ 4\))} o venire applicato ricorsivamente risalendo un livello \textit{(caso \(2\) non proveniente da \(1\))}.
Una catena di invocazioni esegue al massimo \(3\) rotazioni \textit{(evoluzione \(1 \rightarrow 3 \rightarrow 4\))}.

\texttt{RB-DELETE-FIXUP} può essere quindi invocata al massimo \(h\) volte, dove \(h\) è l'altezza dell'albero.
Di conseguenza la complessità temporale sarà \(\bigO(h) = \bigO(\log{(n)})\).

\begin{figure}[htbp]
  \begin{subfigure}{\textwidth}
    \centering
    \bigskip
    \tikzfig[0.8]{image-52.tikz}
    \caption{caso \(1\) dell'algoritmo \texttt{RB-DELETE-FIXUP}}
    \label{fig:caso-1-algoritmo-rb-delete-fixup}
    \bigskip
  \end{subfigure}
  \begin{subfigure}{\textwidth}
    \centering
    \bigskip
    \tikzfig[0.8]{image-53.tikz}
    \caption{caso \(2\) dell'algoritmo \texttt{RB-DELETE-FIXUP}}
    \label{fig:caso-2-algoritmo-rb-delete-fixup}
    \bigskip
  \end{subfigure}
  \begin{subfigure}{\textwidth}
    \centering
    \bigskip
    \tikzfig[0.8]{image-54.tikz}
    \caption{caso \(3\) dell'algoritmo \texttt{RB-DELETE-FIXUP}}
    \label{fig:caso-3-algoritmo-rb-delete-fixup}
    \bigskip
  \end{subfigure}
  \begin{subfigure}{0.99\textwidth}
    \centering
    \bigskip
    \tikzfig[0.8]{image-55.tikz}
    \caption{caso \(4\) dell'algoritmo \texttt{RB-DELETE-FIXUP}}
    \label{fig:caso-4-algoritmo-rb-delete-fixup}
    \bigskip
  \end{subfigure}

  \caption{Casi di \texttt{RB-DELETE-FIXUP}}
  \label{fig:casi-algoritmo-rb-delete-fixup}
\end{figure}

\subsection{Rappresentazione dei grafi in memoria}

Esistono \(2\) tecniche principali per rappresentare i grafi in memoria:

\begin{itemize}
  \item Tramite \textbf{liste di adiacenza}
        \begin{itemize}
          \item implementato tramite un array di liste
          \item per ogni vertice \(v\), la corrispondente lista contiene i vertici adiacenti a \(v\)
        \end{itemize}
  \item Tramite \textbf{matrici di adiacenza}
        \begin{itemize}
          \item in una matrice di adiacenza \(M\) l'elemento \(m_{ij}\) è \(1\) se c'è un arco da \(i\) a \(j\), altrimenti è \(0\)
        \end{itemize}
\end{itemize}

In entrambe le rappresentazioni, dato un nodo \texttt{u} appartenente ad un grafo \(G\), l'attributo \texttt{u.adj} rappresenta l'insieme di vertici adiacenti a \texttt{u} all'interno di \(G\).

\bigskip
La dimensione della rappresentazione è:

\begin{itemize}
  \item matrici di adiacenza: \(\bigO(|V|^2)\)
        \begin{itemize}
          \item la trasposta di una matrice di adiacenza relativa ad un grafo non orientato è uguale alla matrice stessa (\(A^T = A\))
        \end{itemize}
  \item liste di adiacenza: \(\bigO(|V|+|E|)\)
        \begin{itemize}
          \item il numero totale di elementi nelle liste è \(|E|\)
          \item il numero di elementi nell'array è \(|V|\)
        \end{itemize}
\end{itemize}

Quindi le liste di adiacenza sono in generale migliori quando il grafo è \textbf{sparso} (il numero di lati è elevato) mentre le matrici sono più adatte ai grafi \textbf{densi}.

\subsubsection{Operazioni sui grafi}

\paragraph{Visita in ampiezza - \BFS}
\label{par:grafi-bfs}

L'algoritmo \textit{Breadth First Search (\BFS)} è caratterizzato da:

\begin{itemize}
  \item \textit{Input}: un grafo \(G\) \textit{(sia diretto che non)}, un nodo \(s\) \textit{(sorgente)} di \(G\)
  \item \textit{Output}: visitare tutti i ndi di \(G\) che sono raggiungibili da \(S\)
        \begin{itemize}
          \item un nodo \(u\) è raggiungibile da \(s\) se c'è un cammino nel grafo che collega \(s\) a \(u\)
          \item il cammino tra \(s\) e un nodo \(u\) attraversato da \BFS è il più breve possibile
        \end{itemize}
\end{itemize}

L'algoritmo \BFS esplora sistematicamente i lati di \(G\) espandendo la frontiera tra vertici visitati e da visitare: esso scopre tutti i vertici di distanza \(k\) da \(s\) prima di scoprire i vertici di distanza \(k+1\).

Per tenere traccia dei nodi già visitati, l'algoritmo colora di \textit{bianco, grigio} e di \textit{nero} ogni nodo presente nel grafo:

\begin{itemize}
  \item Tutti i vertici sono inizialmente colorati di \textbf{bianco}
  \item Un nodo tutti i cui vicini sono stati scoperti viene colorato di \textbf{nero}
  \item Un nodo scoperto per la prima volta viene colorato di \textbf{grigio}
        \begin{itemize}
          \item i nodi grigi formano la frontiera da espandere
        \end{itemize}
\end{itemize}

Inoltre, ad ogni nodo viene aggiunto un attributo \texttt{dist} che indica la distanza di \(u\) dal nodo \(s\).

La lista di grafi da visitare è implementata tramite una coda \textit{(gestita con politica \FIFO)}.
Ad ogni iterazione, un nodo viene scelto ed i nodi bianchi adiacenti vengono esplorati.

\bigskip
Il Listato~\ref{lst:codice-bfs} contiene il codice dell'algoritmo appena descritto.

\begin{lstlisting}[style=pseudocode, caption={Algoritmo \texttt{BFS}}, label={lst:codice-bfs}]
BFS(G, s)
  for each u $\in$ G.V - {s}
    u.colour := WHITE
    u.dist := $\infty$
  s.colour := GREY
  s.dist := 0
  Q := $\emptyset$
  ENQUEUE(Q, s)
  while Q is not empty:
    u := DEQUEUE(Q)
    for each v $\in$ u.adj:
      if v.colour = WHITE:
        v.colour := GREY
        v.dist := u.dist + 1
        ENQUEUE(Q, v)
    u.colour := BLACK
\end{lstlisting}

Per calcolare la complessità temporale dell'algoritmo, è necessario analizzarlo in tutte le sue componenti.
Infatti esso è composto da:

\begin{itemize}
  \item Fase di \textbf{inizializzazione} \textit{(linee \(2-8\))} con complessità \(\bigO(|V|)\)
  \item Fase di visita dei nodi \textit{(linee \(9-15\))} con complessità \(\bigO(|E|)\)
\end{itemize}

La complessità totale sarà quindi \(\bigO(|V|+|E|)\).

\paragraph{Visita in profondità - \DFS}
\label{par:grafi-dfs}

L'algoritmo \textit{Depth First Search (\DFS)}, al contrario dell'algoritmo \BFS, si basa sull'idea di visitare i nodi con una politica \LIFO \textit{(il \BFS preferisce una politica \FIFO)}.
L'idea alla base è che i vicini di ogni nodo vengono visitati non appena questo è aggiunto in cima alla stack, senza dover aspettare gli altri nodi.
Questa scelta porta a visitare prima i nodi più lontani.

La procedura ha le stesse caratteristiche e funzionamento dell'algoritmo \BFS appena analizzato, con cui condivide anche la complessità.
Lo pseudocodice può essere ottenuto partendo dal \DFS e cambiando \texttt{ENQUEUE} con \texttt{PUSH} e \texttt{DEQUEUE} con \texttt{POP}.
Al suo interno i nodi vengono ulteriormente manipolati, aggiungendo gli attributi:

\begin{itemize}
  \item \texttt{d} che indica il tempo relativo all'inizio della scoperta di un nodo \textit{(discovery time)}
  \item \texttt{f} che indica il tempo relativo alla fine della scoperta di un nodo \textit{(finishing time)}
\end{itemize}

\bigskip
Lo pseudocodice è mostrato nei Listati~\ref{lst:codice-dfs}~e~\ref{lst:codice-dfs-visit}.

\begin{minipage}[t]{.495\textwidth}
  \begin{lstlisting}[style=pseudocode, caption={Algoritmo \texttt{DFS}}, label={lst:codice-dfs}]
DFS(G, s)
  for each u $\in$ G.V - {s}
    u.colour := WHITE
  time := 0
  for each u $\in$ G.V
    if u.colour = WHITE
      DFS-VISIT(u)
\end{lstlisting}
\end{minipage}
\begin{minipage}[t]{.495\textwidth}
  \begin{lstlisting}[style=pseudocode, caption={Funzione di supporto a \texttt{DFS}}, label={lst:codice-dfs-visit}]
DFS-VISIT(G, s)
  u.colour := GREY
  time := time + 1
  u.dist := time
  for each v $\in$ u.adj
    if v.colour = WHITE
      DFS-VISIT(v)
  u.colour := BLACK
  time := time + 1
\end{lstlisting}
\end{minipage}

La complessità temporale dell'algoritmo è data da:

\begin{itemize}
  \item Inizializzazione dei nodi e delle variabili \textit{(linee \(2-3\) dell'algoritmo \texttt{DFS})} con complessità \(\Theta(|V|)\)
  \item L'algoritmo \texttt{DFS-VISIT} è ripetuto \textit{(linee \(5-7\) dell'algoritmo \texttt{DFS})} una volta durante l'esecuzione del ciclo \textbf{\texttt{for}} \textit{(linee \(5-7\) dell'algoritmo \texttt{DFS-VISIT})} per ogni lato con complessità \(\Theta(|E|)\)
\end{itemize}

La complessità totale sarà quindi \(\bigO(|V|+|E|)\) \textit{(analoga al \BFS)}.

\paragraph{Ordinamento topologico}

Un ordinamento topologico di un \texttt{DAG} è un ordinamento lineare dei nodi di tutti i vertici di un grafo \(G\) tale per cui se \(G\) contiene un lato \((u, v)\), allora \(u\) appare prima di \(g\) nell'ordinamento.
I nodi saranno quindi disposti in una linea orizzontale.

È importante notare che gli ordinamenti topologici non sono univoci e che un dato \textit{DAG} può ammettere più ordinamenti diversi tra di loro.

L'ordinamento rispetterà quindi le precedenze tra eventi.
Esso sarà caratterizzato da:

\begin{itemize}
  \item \textit{input}: un \textit{DAG} \(G\)
  \item \textit{output}: una lista, ordinamento topologico di \(G\)
\end{itemize}

\bigskip
\textit{Idea} dell'algoritmo:

\begin{itemize}
  \item il \textit{DAG} viene vistato con un algoritmo \DFS
  \item quando un nodo viene colorato di nero, esso viene inserito in testa alla lista
  \item una volta che tutti i nodi sono stati visitati, la lista costruita è un ordinamento topologico di \(G\)
\end{itemize}

\bigskip
I Listati~\ref{lst:codice-topological-sort}~e~\ref{lst:codice-topsort-visit} contengono il codice dell'algoritmo appena descritto.

\begin{minipage}[t]{0.495\textwidth}
  \begin{lstlisting}[style=pseudocode, caption={Algoritmo \texttt{TOPOLOGICAL-SORT}}, label={lst:codice-topological-sort}]
TOPOLOGICAL-SORT(G)
  L := $\emptyset$
  for each u $\in$ G.V
    u.colour := WHITE
  for each u $\in$ G.V
    if u.colour = WHITE
      TOPSORT-VISIT(L, u)
  return L
  \end{lstlisting}
\end{minipage}
\begin{minipage}[t]{0.495\textwidth}
  \begin{lstlisting}[style=pseudocode, caption={Algoritmo \texttt{TOPSORT-VISIT}}, label={lst:codice-topsort-visit}]
TOPSORT-VISIT
  u.colour := grey
  for each v $\in$ u.adj
    TOPSORT-VISIT(L, v)
  x := elemento di lista x
  x.key = u
  LIST-INSERT(L, x)
  u.colour = BLACK
  \end{lstlisting}
\end{minipage}

Il tempo di esecuzione di \texttt{TOPSORT} è lo stesso di \DFS, quindi la sua complessità temporale è \(\Theta(|V|+|E|)\).
Infatti le linee \(6-8\) di \texttt{TOPSORT-VISIT} hanno complessità \(\Theta(1)\) ed il resto dell'algoritmo è uguale a \texttt{DFS}, con l'assenza della variabile \texttt{TIME}.

\clearpage

\section{Argomenti avanzati}

\subsection{Cammini minimi}

Il \textbf{cammino minimo} tra due vertici \textit{(o nodi)} di un grafo è il percorso che collega i vertici minimizzando la somma dei costi associati all'attraversamento di ciascun lato.

Il costo di attraversamento di un lato è dato da una funzione detta di \textbf{peso} definita come \(w: E \rightarrow \mathbb{R}\).

Il problema di ricerca del cammino minimo si mira a trovare un cammino \(P = (e_{v_1 v_2},e_{v_2 v_3},\ldots, e_{v_n v_n^\prime} )\), \textit{dal nodo \(v\) al nodo \(v^\prime\)}, che tra tutti i cammini possibili abbia il peso minore.
Normalmente i cammini minimi sono calcolati partendo da un nodo fissato \(s\) detto \textbf{sorgente}, dato del problema.
Affinché il cammino minimo resti ben definito non è possibile percorrere cicli negativi, poiché uno di essi verrebbe percorso infinite volte portando il costo a \(-\infty\).

\bigskip
Il peso di un cammino è definito come:
\[ W(p) = \displaystyle \sum_{i=1}^{k} w (v_{i-1}, v_i) \]

Il peso del cammino minimo dal nodo \(v\) al nodo \(v^\prime\) è quindi:

\[ \delta(v, v^\prime) = \begin{cases}
    \min\left\{w(p): v \xrightarrow{P}v^\prime\right\} \quad & \text{ se } \exists \text{ cammino da } v \text{ a } v^\prime \\[10pt]
    \infty                                                   & \text{altrimenti}
  \end{cases}\]

L'uscita dell'algoritmo fornisce, per ogni vertice, il costo del cammino dal nodo di partenza fino ad esso.
Per un nodo \(v\), il costo viene rappresentato come \(d[v] = \delta(s, v)\).
Il predecessore di un nodo \(v\) nel cammino minimo è indicato con \(Pi[v]\).

Inizialmente il costo viene impostato come \(d[v] = \infty\) ed esso viene progressivamente ridotto tramite rilassamenti.
Allo stesso modo il predecessore di ogni nodo viene inizializzato a \texttt{NIL}.

Operativamente, il rilassamento di un lato tra i nodi \(u\) e \(v\) è definito come:

\[\text{se } d[v] > d[u] + w(u, v) \quad \Rightarrow \quad d[v] \coloneqq d[u] + w(u, v), \ Pi[v] = u \]

\textit{Informalmente}, \inlinequote{costa di meno} passare per il lato \((u, v)\).

\bigskip
Lo stesso algoritmo in pseudocodice è mostrato nel Listato~\ref{lst:algoritmo-rilassamento-lato}.

\begin{lstlisting}[style=pseudocode, caption={Algoritmo per il rilassamento di un lato}, label={lst:algoritmo-rilassamento-lato}]
RELAX(u, v, adj, d, Pi)
  if d[v] > d[u] + adj[u][v]
    d[v] := d[u] + adj[u][v]
    Pi[v] := u
\end{lstlisting}

\subsubsection{Algoritmo di Bellman-Ford}

L'\textbf{algoritmo di Bellman-Ford} trova il cammino minimo tra una sorgente \texttt{s} e tutti i nodi di un grafo, a patto che i costi siano tutti positivi,

\textit{Idea} dell'algoritmo:
rilassare, uno alla volta e partendo da \(s\), ogni cammino.
Ad ogni iterazione si aumenta di un passo ogni cammino e dopo \(|V|-1\) iterazioni ogni nodo raggiungibile sarà stato visitato.

\bigskip
Lo pseudocodice dell'algoritmo è mostrato nel Listato~\ref{lst:algoritmo-bellman-ford}.

\begin{lstlisting}[style=pseudocode, caption={Algoritmo di Bellman-Ford}, label={lst:algoritmo-bellman-ford}]
BELLMAN-FORD(adj, s)
  V := vettore dei nodi di adj
  alloca d, array di dimensione adj
  alloca Pi, array di dimensione adj
  for i := 0 to adj.length
    d[i] := $\infty$
    Pi[i] := NIL
  d[s] := 0
  for i := 0 to adj.length - 1
    for each u $\in$ V
      for each v $\in$ adj[u]
        RELAX(u, v, adj, d, Pi)
  return d, Pi
\end{lstlisting}

\subsection{Programmazione dinamica}

La programmazione dinamica, in analogia con la tecnica Divide et Impera, si basa sull'idea di scomporre il problema da risolvere in sotto problemi finché non sono abbastanza semplici da essere risolti con facilità.
Questa tecnica è applicabile quando i problemi non sono indipendenti, cioè condividono dei sotto problemi in comune.
Una volto risolto uno di questi, la soluzione viene salvata in una tabella per permetterne il riuso in seguito.
Il termine programmazione non deriva dalla codifica in linguaggi di programmazione ma dal fatto che è una tecnica tabulare.

La programmazione dinamica è spesso usata per problemi di ottimizzazione.
La soluzione cercata è uno dei multipli sotto problemi ammessi.

\bigskip
I passi normalmente usati per applicare questa tecnica sono:

\begin{enumerate}
  \item Caratterizzazione della struttura delle soluzioni ottimali
  \item Definizione ricorsiva dei valori di una soluzione ottimale del problema
  \item Calcolo di una soluzione ottimale tramite algoritmi bottom-up \textit{(partendo dai problemi più semplici)}
  \item Costruzione della soluzione ottimale del problema richiesto
\end{enumerate}

\subsubsection{Algoritmo di programmazione dinamica}

Applicando un algoritmo ricorsivo per risolvere un certo problema tramite programmazione dinamica, può risultare che la complessità sia eccessivamente elevata \textit{(anche \(\Omega(2^n)\))} poiché certe soluzioni vengono calcolate più volte.

Facendo uso di memoria aggiuntiva \textit{(dove permesso)} è possibile ridurre il tempo di esecuzione, fino ad avere complessità polinomiale.
Questa scelta introduce un \textit{trade-off} spazio temporale: aumentando la complessità spaziale, quella temporale si riduce.

\bigskip
\textit{Idea} della tecnica:
il risultato dei sotto problemi già calcolati viene memorizzato.
Così facendo non è necessario calcolarli più di una volta, perché se essi servissero sarebbero disponibili nella tabella.
Se il costo dei singoli problemi da distinguere è polinomiale, allora la soluzione dell'intero problema richiederebbe tempo polinomiale.

Questa tecnica prende il nome di \textbf{memoization}.

\subsubsection{Approcci top-down e bottom-up}

All'interno della programmazione dinamica si identificano due tipi principali di approccio: \textbf{top-down} e \textbf{bottom-up}.

L'approccio top-down consiste nel cercare di risolvere dapprima il problema di dimensione \(n\) \textit{(il più grande)} per poi concentrarsi su problemi sempre più piccoli.
La dimensione della tabella dei risultati e dei parametri passati aumenta man mano che la ricerca della soluzione procede.

L'approccio bottom-up, al contrario, si concentra dai problemi più piccoli procedendo fino ai problemi più grandi.
Una volta risolto il problema di dimensione \(n\), tutti i problemi di dimensione \(< n\) sono già stati tutti risolti.

\subsection{Algoritmi golosi}

Gli algoritmi golosi (dall'Inglese greedy algorithm) rappresentano una classe particolare di algoritmi di ricerca.
Essi infatti si concentrano sul trovare la soluzione ottimale per ogni passo di risoluzione.

In generale, gli algoritmi golosi cercano gli ottimi locali valutando ad ogni passo la scelta migliore su cui continuare.
Questi algoritmi potrebbero portare a soluzioni non ottimali a livello globale ma, in alcuni casi, si dimostra che l'ottimo viene raggiunto.

Sono spesso impiegati in problemi difficili in cui l'ottimo locale fornisce una \inlinequote{buona approssimazione} dell'ottimo globale.

\subsection{Complessità e non determinismo}

L'obiettivo di questa Sezione sarà cercare di costruire una sorta di classe universale di complessità. cioè una funzione di complessità \(T(n)\) tale che ogni problema al suo interno ha soluzione che impiega al più \(T(n)\).
Inoltre verrà analizzato il non determinismo in relazione alla sua influenza sulla complessità di soluzione dei problemi.

Prima di poter cercare una risposta, è necessario dare alcune definizioni:

\begin{itemize}
  \item Data una funzione \(T(n)\), prende il nome \textbf{\(DTIME(T)\)} la classe di problemi tale per cui esiste un algoritmo che li risolve in tempo \(T(n)\)
  \item Allo stesso modo, si indica con \textbf{\(DSPACE(T)\)} l'insieme riconoscibili in spazio \(T(n)\) mediante \TM deterministiche
  \item \(NTIME(T)\) e \(NSPACE(T)\) rappresentano i loro analoghi con soluzioni non deterministiche
\end{itemize}

Un problema può essere identificato nel riconoscimento di un linguaggio (per semplicità, ricorsivo) e come algoritmo si sceglie una \TM.

\bigskip
Si dimostra che data una funzione totale e computabile \(T(n)\), esiste un linguaggio ricorsivo che non è in \(DTIME(T)\).
È quindi possibile costruire una gerarchia di linguaggi (e allo stesso modo di problemi) organizzata in base alla complessità temporale deterministica.
Allo stesso modo, si possono costruire delle gerarchie per \(DSPACE(T),\ NTIME(T)\) e \(NSPACE(T)\).

\bigskip
Ci si dedicherà ora alle computazioni non deterministiche.
Data una \TM non deterministica \(\mathcal{M}\), la sua complessità temporale \(T_{\mathcal{M}}(x)\) relativa al riconoscimento di una stringa \(x\) è definita come la lunghezza della computazione più breve tra tutte quelle che accettano \(x\).
\(T_{\mathcal{M}}(n)\) sarà poi il caso pessimo tra tutti i \(T_{\mathcal{M}}(x)\) con \(|X| = n\).

\(NTIME(T)\) sarà la classe dei linguaggi ricorsivi riconoscibili in tempo \(T\) mediante \TM non deterministiche.

Tuttavia, la soluzione di un problema attraverso algoritmi non deterministici avviene tramite meccanismi di calcolo effettivamente deterministici.
Se fosse possibile costruire una tecnica \inlinequote{poco costosa} per passare da una formulazione non deterministica ad una deterministica sarebbe possibile risolvere problemi \inlinequote{interessanti} in modo efficiente.

Questo metodo, ovviamente, non esiste.
Come mostrato nel passaggio da \FSA a \NFA (avvenuto nella Sezione~\ref{sec:conversione-nfa-dfa}), infatti, il meccanismo si complica in modo esponenziale.
Lo stesso concetto può essere mostrato per qualsiasi altro meccanismo di calcolo.

\clearpage

\section{Conclusioni sparse delle precedenti Sezioni}

L'obiettivo di questa Sezione sarà ricapitolare delle conclusioni tratte nelle precedenti sezioni, con particolare riguardo agli esercizi ed alle applicazioni pratiche.

\subsection{Ripasso di matematica}

\begin{itemize}
  \item Formula di Gauss: \(\displaystyle \sum_{i=1}^k i = \dfrac{k(k+1)}{2}\)
  \item Sommatoria di una serie geometrica: \(\displaystyle \sum_{n=0}^{+\infty} q^n = \dfrac{1}{1-q}\) con \(|q| < 1\)
\end{itemize}

\subsubsection{Proprietà dei logaritmi}

\begin{itemize}
  \item Definizione di logaritmo:  \(a^{\log_a{(b)}} = b,\ a, b > 0,\ a \neq 1\)
  \item Logaritmo del prodotto: \(\log_a{(b \cdot c)} = \log_a{(b) \cdot \log_a{(c)}}\)
  \item Logaritmo del rapporto: \(\log_a{\left(\sfrac{b}{c}\right)} = \log_a{(b)} - \log_a{(c)}\)
  \item Regola dell'esponente: \(\log_a{\left(b^c\right)} = c \cdot \log_a{(b)}\)
  \item Formula di cambio di base per i logaritmi: \(\log_a{(b)} = \dfrac{\log_c{(b)}}{\log_c{(a)}}\) con \(a, b, c > 0,\ c \neq 1\)
\end{itemize}

\subsection{Scala di potenza delle classi di automi}

Le classi di automi possono essere rappresentati in una scala, in ordine dal \textbf{più} al \textbf{meno} potente.
Ciò avviene nella Figura~\ref{fig:scala-potenza-automi}.

Si noti che \NFA e \FSA hanno potenza equivalente in quanto esiste un algoritmo per convertire i primi nei secondi.

\begin{figure}[htbp]
  \bigskip
  \centering
  \tikzfig{image-19.tikz}
  \caption{Scala di potenza degli automi}
  \label{fig:scala-potenza-automi}
  \bigskip
\end{figure}

\subsection{Chiusura degli automi rispetto alle operazioni}

La chiusura degli automi rispetto alle operazioni è mostrata all'interno della Tabella~\ref{tab:chisura-automi-rispetto-operazioni}.

\begin{table}[htbp]
  \bigskip
  \centering
  \scalebox{0.8}{
    \begin{tabular}{c|ccccccc}
      \textit{classe di automi} & \textit{unione \(\cup\)} & \textit{intersezione \(\cap\)} & \textit{complemento \(A^c\)} & \textit{differenza \(\backslash\)} & \textit{stella di Kleene \(A^\ast\)} & \textit{concatenazione \(\cdot\)} \\ \hline
      \FSA, \NFA                & \colorcmark              & \colorcmark                    & \colorcmark                  & \colorcmark                        & \colorcmark                          & \colorcmark                       \\
      \PDA                      & \colorxmark              & \colorxmark                    & \colorcmark                  & \colorxmark                        & \colorxmark                          & \colorxmark                       \\
      \NPDA                     & \colorcmark              & \colorxmark                    & \colorxmark                  & \colorxmark                        & \colorcmark                          & \colorcmark                       \\
      \TM, \NTM                 & \colorcmark              & \colorcmark                    & \colorxmark                  & \colorxmark                        & \colorcmark                          & \colorcmark                       \\
    \end{tabular}}
  \bigskip
  \caption{Chiusura degli automi rispetto alle operazioni}
  \label{tab:chisura-automi-rispetto-operazioni}
\end{table}

\subsection{Automi e grammatiche di Chomsky}

All'interno della Tabella~\ref{tab:automi-grammatiche-chomsky} è mostrata la relazione tra grammatiche secondo la caratterizzazione di Chomsky e corrispondente formalismo a potenza minima che la riconosce.

\begin{table}[htbp]
  \bigskip
  \centering
  \begin{tabular}{c|c|c}
    \textit{grammatica} & \textit{formalismo}               & \textit{note}                                                             \\ \hline
    \(0\)               & \TM                               & \textit{grammatiche \textbf{generali} e \textbf{dipendenti} dal contesto} \\
    \(1\)               & \textit{linear bounded automaton} & \textit{non trattati nel corso}                                           \\
    \(2\)               & \NPDA                             & \textit{grammatiche \textbf{non dipendenti} dal contesto}                 \\
    \(3\)               & \FSA, \NFA                        & \textit{grammatiche \textbf{regolari}}                                    \\
  \end{tabular}
  \bigskip
  \caption{Relazione tra grammatiche e formalismi}
  \label{tab:automi-grammatiche-chomsky}
\end{table}

\subsection{Pattern tipici delle grammatiche}

Si riconoscono pattern tipici per generare delle strutture note all'interno di grammatiche.
Essi sono mostrati in Tabella~\ref{tab:pattern-tipici}.

\begin{table}[htbp]
  \bigskip
  \centering
  \begin{tabular}{c|c|c}
    \textit{linguaggio}                           & \textit{pattern}                                                                               & \textit{note}                \\ \hline
    \(a^n b^n, n \geq 0\)                         & \(S \rightarrow a S b \, | \, \epsilon\)                                                       & \textit{stella di Kleene}    \\
    \(a^n b^n, n \geq 1\)                         & \(S \rightarrow a S b \, | \, ab\)                                                             & \textit{più di Kleene}       \\
    \(w w^r,\ w = \left(a \, | \, b\right)^\ast\) & \(S \rightarrow a S a \, | \, a B b \, | \, a \, | \, b \, | \, \epsilon \)                    & \textit{stringhe palindrome} \\
    \((a b^+)^\ast\)                              & \(\begin{cases} S \rightarrow ah \, | \, \epsilon \\ h \rightarrow bh \, | \, bS \end{cases}\) &                              \\
  \end{tabular}
  \bigskip
  \caption{Pattern tipici}
  \label{tab:pattern-tipici}
\end{table}

\subsubsection[Sintetizzazione di grammatiche di tipo 0]{Sintetizzazione di grammatiche di tipo \(0\)}

\begin{minipage}{0.99\textwidth}
  \bigskip
  \textit{Idea generale}: simulare i nastri di una \TM tramite le regole della grammatica.
  In dettaglio:
  \begin{itemize}
    \item I nastri memorizzano i caratteri non terminali
    \item Altri caratteri non terminali simulano le testine
    \item I simboli nei nastri non mossi attraverso regole di \inlinequote{swap}
  \end{itemize}
  \bigskip
\end{minipage}

\subsection{Linee guida sulla decidibilità}

\subsubsection{Casi immediati}

\begin{itemize}
  \item C'è una domanda di tipo \textbf{booleano} la cui risposta non dipende da alcun parametro esterno?
        \begin{itemize}[label=\(\Rightarrow\)]
          \item La domanda è \textbf{chiusa} e il problema è \textbf{decidibile}
        \end{itemize}
  \item La funzione in questione consiste di un \textbf{numero finito di casi}, tutti singolarmente decidibili e calcolabili?
        \begin{itemize}[label=\(\Rightarrow\)]
          \item La funzione è \textbf{calcolabile} e il problema è \textbf{decidibile}
        \end{itemize}
\end{itemize}

\subsubsection{Caso del programmatore}

È possibile scrivere un programma in un generico linguaggio (sia esso \texttt{C}, \texttt{Java}, \texttt{\ldots}) che risolve il problema dato?

\begin{itemize}[label=\(\Rightarrow\)]
  \item Se \textbf{sì}, la funzione è \textbf{calcolabile} e il problema è \textbf{decidibile}
\end{itemize}

Ciò implica che sia possibile scrivere un programma che per ogni possibile ingresso sia in grado di calcolare il valore corretto dell'uscita.
Se per qualche valore dell'ingresso l'uscita non è definita, è sufficiente far entrare il programma in un loop infinito.

\subsubsection{Riduzioni}

\begin{itemize}
  \item Esiste un problema \textbf{indecidibile} che è un caso particolare del problema in analisi
        \begin{itemize}[label=\(\Rightarrow\)]
          \item Il problema in analisi è \textbf{indecidibile}
        \end{itemize}
  \item Esiste un problema \textbf{decidibile} che è un caso particolare del problema in analisi
        \begin{itemize}[label=\(\Rightarrow\)]
          \item Il problema in analisi è \textbf{indecidibile}
        \end{itemize}
\end{itemize}

\subsubsection{Applicazione del teorema di Rice}

Il teorema di Rice può essere applicato nei casi in cui si deve verificare se un programma \textit{(o analogamente una \TM o un algoritmo)}:

\begin{itemize}
  \item Ha una data \textbf{proprietà} relativa alla funzione da esso calcolata
  \item Calcola una \textbf{funzione} tra quelle di un insieme dato
\end{itemize}

Infatti, se:

\begin{itemize}
  \item L'insieme di funzioni identificato \textbf{non è banale}
        \begin{itemize}[label=\(\Rightarrow\)]
          \item Il problema in analisi è \textbf{indecidibile}
        \end{itemize}
  \item L'insieme di funzioni identificato \textbf{è banale}
        \begin{itemize}[label=\(\Rightarrow\)]
          \item Il problema in analisi è \textbf{decidibile}
        \end{itemize}
\end{itemize}

Si ricordi che un insieme di funzioni è banale se è \textbf{vuoto} o se è l'insieme di \textbf{tutte le funzioni computabili}.

\subsubsection[Ricorsività di un insieme S di numeri naturali]{Ricorsività di un insieme \(S\) di numeri naturali}

\begin{itemize}
  \item \(S\) è \textbf{finito}
        \begin{itemize}[label=\(\Rightarrow\)]
          \item \(S\) è \textbf{ricorsivo}
        \end{itemize}
  \item \(S\) è \textbf{infinito}
        \begin{itemize}
          \item La funzione caratteristica di \(S\) è \textbf{computabile}
                \begin{itemize}[label=\(\Rightarrow\)]
                  \item \(S\) è \textbf{ricorsivo}
                \end{itemize}
          \item \(S\) può essere espresso come insieme di indici di \TM con una \textbf{proprietà comune relativa alla funzione che calcolano}
                \begin{itemize}[label=\(\Rightarrow\)]
                  \item \textbf{Si può} usare il teorema di Rice
                \end{itemize}
          \item \(S\) può essere espresso come insieme di indici di \TM con una \textbf{proprietà comune non relativa alla funzione che calcolano}
                \begin{itemize}[label=\(\Rightarrow\)]
                  \item \textbf{Non si può} usare il teorema di Rice
                \end{itemize}
        \end{itemize}
\end{itemize}

\subsection{Complessità}

\begin{itemize}
  \item Tutti i logaritmi sono nello stesso \(\Theta\) si può non omettere la \textbf{base}
        \begin{itemize}
          \item la formula di cambio base comporta un fattore moltiplicativo
          \item è possibile non indicare la base del logaritmo quando si valuta l'andamento asintotico
        \end{itemize}
  \item Contrariamente a quanto avviene nei logaritmi, negli esponenziali è importante specificare la \textbf{base}
  \item A meno che non sia esplicitamente indicato, negli esercizi con pseudocodice si usa il criterio di costo costante
\end{itemize}

\subsubsection{Complessità dei cicli}

Si supponga che un ciclo viene eseguito \(n\) volte.
Se, ad ogni ciclo, il contatore:

\begin{itemize}
  \item Viene incrementato di un valore costante \texttt{k}: la complessità è \(T(n) = \Theta(n)\)
  \item Viene moltiplicato per un valore costante \texttt{k}: la complessità è \(T(n) = \Theta\left(\log_\texttt{k}{(n)}\right) = \Theta\left(\log{(n)}\right)\)
  \item Viene elevato ad un valore costante \texttt{k}: la complessità è \(T(n) = \Theta\left(\log{\left(\log{(n)}\right)}\right)\)
\end{itemize}

\clearpage

\section{Codice relativo agli algoritmi}

In questa sezione vengono presentati gli algoritmi mostrati durante il corso.
Il linguaggio di implementazione è il C.

\subsection{Algoritmi di ordinamento}

\lstinputlisting[style=customc, firstline=3]{codes/sort/src/sort.c}

\end{document}